{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1bcc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ebf5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.14.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc8c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total images loaded: 47565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 4757/4757 [07:35<00:00, 10.43it/s, loss=0.1295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 finished | Avg Loss: 0.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 4757/4757 [07:35<00:00, 10.44it/s, loss=0.1482]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 finished | Avg Loss: 0.2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 4757/4757 [07:12<00:00, 10.99it/s, loss=0.1264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 finished | Avg Loss: 0.2431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 4757/4757 [07:50<00:00, 10.12it/s, loss=0.5252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 finished | Avg Loss: 0.1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 4757/4757 [08:23<00:00,  9.45it/s, loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 finished | Avg Loss: 0.1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 4757/4757 [08:00<00:00,  9.90it/s, loss=0.1254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 finished | Avg Loss: 0.1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 4757/4757 [09:06<00:00,  8.71it/s, loss=0.2404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 finished | Avg Loss: 0.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 4757/4757 [09:05<00:00,  8.72it/s, loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 finished | Avg Loss: 0.0650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 4757/4757 [08:37<00:00,  9.19it/s, loss=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 finished | Avg Loss: 0.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 4757/4757 [08:31<00:00,  9.31it/s, loss=0.0013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 finished | Avg Loss: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1190/1190 [01:12<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 0.901292967518133\n",
      "✅ Validation AUC: 0.9230383972221654\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Frequency-Aware DeepFake Detection (FFT + CNN)\n",
    "# Dataset: Extracted Faces (Real / Fake folders)\n",
    "# Limit images per video/folder\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "# 1. PATH CONFIG\n",
    "# =======================\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\"\n",
    "\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"extracted_frames_Cross_vit\")\n",
    "REAL_DIR = os.path.join(DATASET_DIR, \"real\")\n",
    "FAKE_DIR = os.path.join(DATASET_DIR, \"fake\")\n",
    "\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"results_frequency\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "LABELS = {\"real\": 0, \"fake\": 1}\n",
    "\n",
    "# Max faces per video/subfolder\n",
    "MAX_FACES_PER_VIDEO = 50\n",
    "\n",
    "# =======================\n",
    "# 2. FFT TRANSFORMATION\n",
    "# =======================\n",
    "\n",
    "def fft_transform(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    fft = np.fft.fft2(gray)\n",
    "    fft_shift = np.fft.fftshift(fft)\n",
    "    magnitude = np.log(np.abs(fft_shift) + 1)\n",
    "    magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    magnitude = magnitude.astype(np.uint8)\n",
    "    magnitude = cv2.cvtColor(magnitude, cv2.COLOR_GRAY2RGB)\n",
    "    return magnitude\n",
    "\n",
    "# =======================\n",
    "# 3. DATASET CLASS\n",
    "# =======================\n",
    "\n",
    "class FrequencyDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dir, transform=None, max_faces_per_video=50):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.max_faces_per_video = max_faces_per_video\n",
    "\n",
    "        def collect_images(base_dir, label):\n",
    "            for root, dirs, _ in os.walk(base_dir):\n",
    "                for subfolder in dirs:\n",
    "                    subfolder_path = os.path.join(root, subfolder)\n",
    "                    files = [f for f in os.listdir(subfolder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "                    files = files[:self.max_faces_per_video]  # Limit number of faces per video\n",
    "                    for f in files:\n",
    "                        self.samples.append((os.path.join(subfolder_path, f), label))\n",
    "\n",
    "        collect_images(real_dir, LABELS[\"real\"])\n",
    "        collect_images(fake_dir, LABELS[\"fake\"])\n",
    "\n",
    "        print(f\"✅ Total images loaded: {len(self.samples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        freq_img = fft_transform(img)\n",
    "\n",
    "        if self.transform:\n",
    "            freq_img = self.transform(image=freq_img)[\"image\"]\n",
    "\n",
    "        return freq_img, torch.tensor(label)\n",
    "\n",
    "# =======================\n",
    "# 4. TRANSFORMS\n",
    "# =======================\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# =======================\n",
    "# 5. DATASET & SPLIT\n",
    "# =======================\n",
    "\n",
    "dataset = FrequencyDataset(\n",
    "    real_dir=REAL_DIR,\n",
    "    fake_dir=FAKE_DIR,\n",
    "    transform=transform,\n",
    "    max_faces_per_video=MAX_FACES_PER_VIDEO\n",
    ")\n",
    "\n",
    "labels_list = [dataset[i][1].item() for i in range(len(dataset))]\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_list\n",
    ")\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set   = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=8, shuffle=False)\n",
    "\n",
    "# =======================\n",
    "# 6. MODEL\n",
    "# =======================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = timm.create_model(\n",
    "    \"resnet18\",\n",
    "    pretrained=True,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "# =======================\n",
    "# 7. TRAINING SETUP\n",
    "# =======================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "EPOCHS = 10\n",
    "\n",
    "# =======================\n",
    "# 8. TRAINING LOOP\n",
    "# =======================\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1} finished | Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# =======================\n",
    "# 9. EVALUATION\n",
    "# =======================\n",
    "\n",
    "model.eval()\n",
    "preds, labels, probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(val_loader, desc=\"Validation\"):\n",
    "        x = x.to(device)\n",
    "        out = model(x)\n",
    "\n",
    "        prob = torch.softmax(out, dim=1)[:, 1]\n",
    "        pred = out.argmax(1).cpu().numpy()\n",
    "\n",
    "        preds.extend(pred)\n",
    "        labels.extend(y.numpy())\n",
    "        probs.extend(prob.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "auc = roc_auc_score(labels, probs)\n",
    "\n",
    "print(\"✅ Validation Accuracy:\", acc)\n",
    "print(\"✅ Validation AUC:\", auc)\n",
    "\n",
    "# =======================\n",
    "# 10. SAVE RESULTS\n",
    "# =======================\n",
    "\n",
    "with open(os.path.join(RESULT_DIR, \"results.txt\"), \"w\") as f:\n",
    "    f.write(f\"Accuracy: {acc}\\n\")\n",
    "    f.write(f\"AUC: {auc}\\n\")\n",
    "\n",
    "# =======================\n",
    "# 11. INFERENCE FUNCTION\n",
    "# =======================\n",
    "\n",
    "def predict_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    freq = fft_transform(img)\n",
    "    freq = transform(image=freq)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(freq)\n",
    "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
    "\n",
    "    return \"FAKE\" if prob > 0.5 else \"REAL\", prob\n",
    "\n",
    "# Example usage:\n",
    "# label, conf = predict_image(\"frame_0000.jpg\")\n",
    "# print(label, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ebdfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total images loaded: 75568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 7557/7557 [12:24<00:00, 10.15it/s, loss=0.3117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 finished | Avg Loss: 0.3668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 7557/7557 [11:57<00:00, 10.54it/s, loss=0.1446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 finished | Avg Loss: 0.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 7557/7557 [12:41<00:00,  9.92it/s, loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 finished | Avg Loss: 0.2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 7557/7557 [13:13<00:00,  9.52it/s, loss=0.0309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 finished | Avg Loss: 0.1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 7557/7557 [14:59<00:00,  8.40it/s, loss=0.7479]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 finished | Avg Loss: 0.1404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 7557/7557 [14:22<00:00,  8.76it/s, loss=0.0875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 finished | Avg Loss: 0.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 7557/7557 [14:14<00:00,  8.84it/s, loss=0.0403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 finished | Avg Loss: 0.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 7557/7557 [14:36<00:00,  8.62it/s, loss=0.0432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 finished | Avg Loss: 0.0595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 7557/7557 [13:17<00:00,  9.48it/s, loss=0.0021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 finished | Avg Loss: 0.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 7557/7557 [15:58<00:00,  7.88it/s, loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 finished | Avg Loss: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 1890/1890 [02:42<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation Accuracy: 0.9241762604208019\n",
      "✅ Validation AUC: 0.9482802277049276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Frequency-Aware DeepFake Detection (FFT + CNN)\n",
    "# Dataset: Extracted Faces (Real / Fake folders)\n",
    "# Limit images per video/folder\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =======================\n",
    "# 1. PATH CONFIG\n",
    "# =======================\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\"\n",
    "\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"extracted_frames_Cross_vit\")\n",
    "REAL_DIR = os.path.join(DATASET_DIR, \"real\")\n",
    "FAKE_DIR = os.path.join(DATASET_DIR, \"fake\")\n",
    "\n",
    "RESULT_DIR = os.path.join(BASE_DIR, \"results_frequency\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "LABELS = {\"real\": 0, \"fake\": 1}\n",
    "\n",
    "# Max faces per video/subfolder\n",
    "MAX_FACES_PER_VIDEO = 80\n",
    "\n",
    "# =======================\n",
    "# 2. FFT TRANSFORMATION\n",
    "# =======================\n",
    "\n",
    "def fft_transform(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    fft = np.fft.fft2(gray)\n",
    "    fft_shift = np.fft.fftshift(fft)\n",
    "    magnitude = np.log(np.abs(fft_shift) + 1)\n",
    "    magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    magnitude = magnitude.astype(np.uint8)\n",
    "    magnitude = cv2.cvtColor(magnitude, cv2.COLOR_GRAY2RGB)\n",
    "    return magnitude\n",
    "\n",
    "# =======================\n",
    "# 3. DATASET CLASS\n",
    "# =======================\n",
    "\n",
    "class FrequencyDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dir, transform=None, max_faces_per_video=80):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        self.max_faces_per_video = max_faces_per_video\n",
    "\n",
    "        def collect_images(base_dir, label):\n",
    "            for root, dirs, _ in os.walk(base_dir):\n",
    "                for subfolder in dirs:\n",
    "                    subfolder_path = os.path.join(root, subfolder)\n",
    "                    files = [f for f in os.listdir(subfolder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "                    files = files[:self.max_faces_per_video]  # Limit number of faces per video\n",
    "                    for f in files:\n",
    "                        self.samples.append((os.path.join(subfolder_path, f), label))\n",
    "\n",
    "        collect_images(real_dir, LABELS[\"real\"])\n",
    "        collect_images(fake_dir, LABELS[\"fake\"])\n",
    "\n",
    "        print(f\"✅ Total images loaded: {len(self.samples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        freq_img = fft_transform(img)\n",
    "\n",
    "        if self.transform:\n",
    "            freq_img = self.transform(image=freq_img)[\"image\"]\n",
    "\n",
    "        return freq_img, torch.tensor(label)\n",
    "\n",
    "# =======================\n",
    "# 4. TRANSFORMS\n",
    "# =======================\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# =======================\n",
    "# 5. DATASET & SPLIT\n",
    "# =======================\n",
    "\n",
    "dataset = FrequencyDataset(\n",
    "    real_dir=REAL_DIR,\n",
    "    fake_dir=FAKE_DIR,\n",
    "    transform=transform,\n",
    "    max_faces_per_video=MAX_FACES_PER_VIDEO\n",
    ")\n",
    "\n",
    "labels_list = [dataset[i][1].item() for i in range(len(dataset))]\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    range(len(dataset)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels_list\n",
    ")\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set   = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=8, shuffle=False)\n",
    "\n",
    "# =======================\n",
    "# 6. MODEL\n",
    "# =======================\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = timm.create_model(\n",
    "    \"resnet18\",\n",
    "    pretrained=True,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "# =======================\n",
    "# 7. TRAINING SETUP\n",
    "# =======================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "EPOCHS = 10\n",
    "\n",
    "# =======================\n",
    "# 8. TRAINING LOOP\n",
    "# =======================\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1} finished | Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# =======================\n",
    "# 9. EVALUATION\n",
    "# =======================\n",
    "\n",
    "model.eval()\n",
    "preds, labels, probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(val_loader, desc=\"Validation\"):\n",
    "        x = x.to(device)\n",
    "        out = model(x)\n",
    "\n",
    "        prob = torch.softmax(out, dim=1)[:, 1]\n",
    "        pred = out.argmax(1).cpu().numpy()\n",
    "\n",
    "        preds.extend(pred)\n",
    "        labels.extend(y.numpy())\n",
    "        probs.extend(prob.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "auc = roc_auc_score(labels, probs)\n",
    "\n",
    "print(\"✅ Validation Accuracy:\", acc)\n",
    "print(\"✅ Validation AUC:\", auc)\n",
    "\n",
    "# =======================\n",
    "# 10. SAVE RESULTS\n",
    "# =======================\n",
    "\n",
    "with open(os.path.join(RESULT_DIR, \"results.txt\"), \"w\") as f:\n",
    "    f.write(f\"Accuracy: {acc}\\n\")\n",
    "    f.write(f\"AUC: {auc}\\n\")\n",
    "\n",
    "# =======================\n",
    "# 11. INFERENCE FUNCTION\n",
    "# =======================\n",
    "\n",
    "def predict_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    freq = fft_transform(img)\n",
    "    freq = transform(image=freq)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(freq)\n",
    "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
    "\n",
    "    return \"FAKE\" if prob > 0.5 else \"REAL\", prob\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
