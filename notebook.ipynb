{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1478f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164a3578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from encoder4editing.models.psp import pSp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e15817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: face_recognition in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from face_recognition) (0.3.0)\n",
      "Requirement already satisfied: Click>=6.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from face_recognition) (8.1.8)\n",
      "Requirement already satisfied: dlib>=19.7 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from face_recognition) (19.24.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from face_recognition) (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from face_recognition) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from Click>=6.0->face_recognition) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install face_recognition \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12000e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Collecting numpy (from torchvision)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\pip-uninstall-7hezru6x'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\Lib\\site-packages\\~%mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "facenet-pytorch 2.6.0 requires numpy<2.0.0,>=1.24.0, but you have numpy 2.0.2 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.5.1 which is incompatible.\n",
      "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.20.1 which is incompatible.\n",
      "fatal: destination path 'encoder4editing' already exists and is not an empty directory.\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'encoder4editing/requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision opencv-python scikit-learn matplotlib\n",
    "!git clone https://github.com/omertov/encoder4editing.git\n",
    "!pip install -r encoder4editing/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729b7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\elitelaptop\\appdata\\roaming\\python\\python39\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\elitelaptop\\appdata\\roaming\\python\\python39\\site-packages (from gdown) (4.14.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from gdown) (3.17.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from gdown) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\elitelaptop\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4->gdown) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from beautifulsoup4->gdown) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests[socks]->gdown) (2025.10.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617f3c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: torch in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from opencv-python) (2.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf02762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "CUDA dispo : True\n",
      "Nom GPU : NVIDIA GeForce GTX 1650 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA dispo :\", torch.cuda.is_available())\n",
    "print(\"Nom GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU d√©tect√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f48df4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "True\n",
      "NVIDIA GeForce GTX 1650 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29ac0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder4editing.models.psp import pSp\n",
    "from encoder4editing.utils.common import tensor2im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53feb60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# üìÅ 0. Imports\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from encoder4editing.models.psp import pSp\n",
    "from encoder4editing.utils.common import tensor2im\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# üß† Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd013b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_17584\\4242676774.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing/pretrained_models/e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "ckpt_path = 'encoder4editing/pretrained_models/e4e_ffhq_encode.pt'\n",
    "\n",
    "def load_model():\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = SimpleNamespace(**opts)\n",
    "    model = pSp(opts)       # ‚úÖ Corrig√© ici\n",
    "    model = model.to(device)  # ‚úÖ Envoyer sur le bon device\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77a01fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.4\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebe7147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Chargement du mod√®le pSp/e4e...\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üìå Mod√®le charg√© sur : cuda\n",
      "üîÑ Traitement de 790 images pour 'real' dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [09:31<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Traitement de 3975 images pour 'fake' dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [46:51<00:00,  2.83s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total features: 4765 (Real: 790, Fake: 3975)\n",
      "üìä Taille dataset avant √©quilibrage : [ 790 3975]\n",
      "‚úÖ Taille apr√®s √©quilibrage : [3975 3975]\n",
      "üéØ Accuracy (train): 0.9995\n",
      "üéØ Accuracy (test) : 0.9673\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.94      0.99      0.97       795\n",
      "        Fake       0.99      0.94      0.97       795\n",
      "\n",
      "    accuracy                           0.97      1590\n",
      "   macro avg       0.97      0.97      0.97      1590\n",
      "weighted avg       0.97      0.97      0.97      1590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_1052\\2413290988.py:145: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_1052\\2413290988.py:146: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_1052\\2413290988.py:156: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_1052\\2413290988.py:157: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"accuracy_train_test.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Calcul de la validation crois√©e l√©g√®re...\n",
      "üìä Moyenne validation crois√©e (√©chantillon r√©duit): 0.5975\n",
      "‚úÖ Graphiques sauvegard√©s dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_1052\\2413290988.py:172: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_1052\\2413290988.py:173: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"cross_validation_curve.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le et scaler sauvegard√©s dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements sklearn/pytorch inutiles\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Extraction des features √† partir des frames\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, label, device='cuda', batch_size=4):\n",
    "    features_list, labels_list = [], []\n",
    "\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Traitement de {len(frame_paths)} images pour '{label}' dans {folder_path}...\")\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        print(f\"‚ö†Ô∏è Aucune image trouv√©e dans {folder_path}\")\n",
    "        return features_list, labels_list\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                img_tensor = transform(img)\n",
    "                batch_images.append(img_tensor)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur chargement {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch_tensor)\n",
    "\n",
    "        for feat in batch_features:\n",
    "            feature_vector = feat.detach().cpu().numpy().flatten()\n",
    "            features_list.append(feature_vector)\n",
    "            labels_list.append(1 if label == 'fake' else 0)\n",
    "\n",
    "        del batch_tensor, batch_features\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return features_list, labels_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Chargement du mod√®le e4e/pSp\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "\n",
    "    print('üîç Chargement du mod√®le pSp/e4e...')\n",
    "    assert os.path.exists(ckpt_path), f\"‚ö†Ô∏è Checkpoint introuvable : {ckpt_path}\"\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"üìå Mod√®le charg√© sur : {device}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Entra√Ænement du classificateur + visualisation\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    print(\"üìä Taille dataset avant √©quilibrage :\", np.bincount(y))\n",
    "\n",
    "    # √âtape 1 : Oversampling\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print(\"‚úÖ Taille apr√®s √©quilibrage :\", np.bincount(y_res))\n",
    "\n",
    "    # √âtape 2 : Normalisation (float32 pour √©conomiser la m√©moire)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "    del X_res  # lib√®re m√©moire\n",
    "\n",
    "    # √âtape 3 : Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    "    )\n",
    "    del X_scaled, y_res  # lib√®re m√©moire\n",
    "\n",
    "    # √âtape 4 : RandomForest\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        class_weight='balanced_subsample',\n",
    "        random_state=42,\n",
    "        max_depth=18,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # √âtape 5 : √âvaluation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"üéØ Accuracy (train): {acc_train:.4f}\")\n",
    "    print(f\"üéØ Accuracy (test) : {acc_test:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "    # √âtape 6 : Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('üß© Matrice de confusion')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # √âtape 7 : Courbe d‚Äôaccuracy train/test\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Train', 'Test'], [acc_train, acc_test], color=['#4CAF50', '#2196F3'])\n",
    "    plt.title('üìä Accuracy du mod√®le (train vs test)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"accuracy_train_test.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # √âtape 8 : Validation crois√©e l√©g√®re (√©chantillon)\n",
    "    print(\"üìà Calcul de la validation crois√©e l√©g√®re...\")\n",
    "    scores = cross_val_score(clf, X_train[:800], y_train[:800], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    print(f\"üìä Moyenne validation crois√©e (√©chantillon r√©duit): {scores.mean():.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(1, len(scores)+1), scores, marker='o', linestyle='--', color='#FF9800')\n",
    "    plt.title('üìà Validation crois√©e l√©g√®re')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"cross_validation_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(\"‚úÖ Graphiques sauvegard√©s dans :\", output_dir)\n",
    "    return clf, scaler\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN PIPELINE\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results'\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    features_real, labels_real = process_frames_from_folder(path_real, model, 'real', device=device, batch_size=4)\n",
    "    features_fake, labels_fake = process_frames_from_folder(path_fake, model, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    X = np.array(features_real + features_fake)\n",
    "    y = np.array(labels_real + labels_fake)\n",
    "    print(f\"üìä Total features: {len(X)} (Real: {len(features_real)}, Fake: {len(features_fake)})\")\n",
    "\n",
    "    if len(X) == 0:\n",
    "        raise ValueError('‚ùå Aucune feature extraite ‚Äî v√©rifie tes dossiers de frames.')\n",
    "\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Mod√®le et scaler sauvegard√©s dans : {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c47e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Chargement du mod√®le e4e...\n",
      "Loading e4e over the pSp framework from checkpoint: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üéûÔ∏è 33 frames extraites dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\temp_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Extraction features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [06:06<00:00, 28.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Moyenne de probabilit√© Fake : 0.516\n",
      "üö® La vid√©o est probablement FAKE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Charger le mod√®le e4e/pSp ---\n",
    "from encoder4editing.models.psp import pSp\n",
    "from argparse import Namespace\n",
    "\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    print('üîç Chargement du mod√®le e4e...')\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "    model = pSp(opts).to(device).eval()\n",
    "    return model\n",
    "\n",
    "# --- Extraire les frames d‚Äôune vid√©o ---\n",
    "def extract_frames(video_path, output_dir, frame_interval=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_interval == 0:\n",
    "            frame_path = os.path.join(output_dir, f\"frame_{saved:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved += 1\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    print(f\"üéûÔ∏è {saved} frames extraites dans {output_dir}\")\n",
    "    return sorted(glob(os.path.join(output_dir, \"*.jpg\")))\n",
    "\n",
    "# --- Extraire les features avec e4e ---\n",
    "def extract_features(frame_paths, model, device='cuda', batch_size=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    features = []\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=\"üîç Extraction features\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        for p in batch_paths:\n",
    "            img = Image.open(p).convert('RGB')\n",
    "            batch_images.append(transform(img))\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch_tensor)\n",
    "        for feat in batch_features:\n",
    "            features.append(feat.detach().cpu().numpy().flatten())\n",
    "    return np.array(features)\n",
    "\n",
    "# --- Pr√©diction vid√©o ---\n",
    "def predict_video(video_path, model_path, scaler_path, classifier_path, tmp_frames_dir, device='cuda'):\n",
    "    model = load_model(model_path, device)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    clf = joblib.load(classifier_path)\n",
    "\n",
    "    frame_paths = extract_frames(video_path, tmp_frames_dir, frame_interval=10)\n",
    "    if len(frame_paths) == 0:\n",
    "        raise ValueError(\"‚ö†Ô∏è Aucune frame extraite de la vid√©o !\")\n",
    "\n",
    "    X = extract_features(frame_paths, model, device)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    preds = clf.predict_proba(X_scaled)[:, 1]  # probabilit√© \"fake\"\n",
    "    mean_pred = preds.mean()\n",
    "\n",
    "    print(f\"üìà Moyenne de probabilit√© Fake : {mean_pred:.3f}\")\n",
    "    if mean_pred > 0.5:\n",
    "        print(\"üö® La vid√©o est probablement FAKE\")\n",
    "    else:\n",
    "        print(\"‚úÖ La vid√©o semble REAL\")\n",
    "    return mean_pred\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\fake\\id0_id4_0004.mp4\"\n",
    "    model_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\"\n",
    "    scaler_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results\\scaler.pkl\"\n",
    "    classifier_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results\\deepfake_classifier.pkl\"\n",
    "    tmp_frames_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\temp_frames\"\n",
    "\n",
    "    predict_video(video_path, model_path, scaler_path, classifier_path, tmp_frames_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af621701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Chargement du mod√®le e4e...\n",
      "Loading e4e over the pSp framework from checkpoint: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üéûÔ∏è 53 frames extraites dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\temp_frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Extraction features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [06:07<00:00, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Moyenne de probabilit√© Fake : 0.697\n",
      "üö® La vid√©o est probablement FAKE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Charger le mod√®le e4e/pSp ---\n",
    "from encoder4editing.models.psp import pSp\n",
    "from argparse import Namespace\n",
    "\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    print('üîç Chargement du mod√®le e4e...')\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "    model = pSp(opts).to(device).eval()\n",
    "    return model\n",
    "\n",
    "# --- Extraire les frames d‚Äôune vid√©o ---\n",
    "def extract_frames(video_path, output_dir, frame_interval=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_interval == 0:\n",
    "            frame_path = os.path.join(output_dir, f\"frame_{saved:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved += 1\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    print(f\"üéûÔ∏è {saved} frames extraites dans {output_dir}\")\n",
    "    return sorted(glob(os.path.join(output_dir, \"*.jpg\")))\n",
    "\n",
    "# --- Extraire les features avec e4e ---\n",
    "def extract_features(frame_paths, model, device='cuda', batch_size=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    features = []\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=\"üîç Extraction features\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        for p in batch_paths:\n",
    "            img = Image.open(p).convert('RGB')\n",
    "            batch_images.append(transform(img))\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch_tensor)\n",
    "        for feat in batch_features:\n",
    "            features.append(feat.detach().cpu().numpy().flatten())\n",
    "    return np.array(features)\n",
    "\n",
    "# --- Pr√©diction vid√©o ---\n",
    "def predict_video(video_path, model_path, scaler_path, classifier_path, tmp_frames_dir, device='cuda'):\n",
    "    model = load_model(model_path, device)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    clf = joblib.load(classifier_path)\n",
    "\n",
    "    frame_paths = extract_frames(video_path, tmp_frames_dir, frame_interval=10)\n",
    "    if len(frame_paths) == 0:\n",
    "        raise ValueError(\"‚ö†Ô∏è Aucune frame extraite de la vid√©o !\")\n",
    "\n",
    "    X = extract_features(frame_paths, model, device)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    preds = clf.predict_proba(X_scaled)[:, 1]  # probabilit√© \"fake\"\n",
    "    mean_pred = preds.mean()\n",
    "\n",
    "    print(f\"üìà Moyenne de probabilit√© Fake : {mean_pred:.3f}\")\n",
    "    if mean_pred > 0.5:\n",
    "        print(\"üö® La vid√©o est probablement FAKE\")\n",
    "    else:\n",
    "        print(\"‚úÖ La vid√©o semble REAL\")\n",
    "    return mean_pred\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\real\\id0_0003.mp4\"\n",
    "    model_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\"\n",
    "    scaler_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results\\scaler.pkl\"\n",
    "    classifier_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results\\deepfake_classifier.pkl\"\n",
    "    tmp_frames_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\temp_frames\"\n",
    "\n",
    "    predict_video(video_path, model_path, scaler_path, classifier_path, tmp_frames_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30819ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lightgbm) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.5 MB/s  0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5286ed8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Chargement du mod√®le pSp/e4e...\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üìå Mod√®le charg√© sur : cuda\n",
      "üîÑ Traitement de 790 images pour 'real' dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [04:43<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Traitement de 3975 images pour 'fake' dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [23:07<00:00,  1.40s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total features: 4765 (Real: 790, Fake: 3975)\n",
      "üìä Taille dataset avant √©quilibrage : [ 790 3975]\n",
      "‚úÖ Taille apr√®s √©quilibrage : [3975 3975]\n",
      "[LightGBM] [Info] Number of positive: 3180, number of negative: 3180\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 6360, number of used features: 3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12412\\3995857957.py:146: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12412\\3995857957.py:147: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Accuracy (train): 0.8615\n",
      "üéØ Accuracy (test) : 0.7811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.77      0.80      0.78       795\n",
      "        Fake       0.79      0.76      0.78       795\n",
      "\n",
      "    accuracy                           0.78      1590\n",
      "   macro avg       0.78      0.78      0.78      1590\n",
      "weighted avg       0.78      0.78      0.78      1590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12412\\3995857957.py:157: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12412\\3995857957.py:158: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"accuracy_train_test.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Validation crois√©e rapide...\n",
      "üìä Moyenne validation crois√©e (sample): 0.5860\n",
      "‚úÖ Graphiques sauvegard√©s dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_fastss\n",
      "‚úÖ Mod√®le et scaler sauvegard√©s dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_fastss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12412\\3995857957.py:173: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12412\\3995857957.py:174: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"cross_validation_curve.png\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Extraction rapide des features\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, label, device='cuda', batch_size=16):\n",
    "    features_list, labels_list = [], []\n",
    "\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Traitement de {len(frame_paths)} images pour '{label}' dans {folder_path}...\")\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        print(f\"‚ö†Ô∏è Aucune image trouv√©e dans {folder_path}\")\n",
    "        return features_list, labels_list\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                batch_images.append(transform(img))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur chargement {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch_tensor)\n",
    "\n",
    "        # ‚ö°Ô∏è R√©duction spatiale (moyenne globale pour r√©duire dimension)\n",
    "        # Passe de 196,608 features ‚Üí ~512-1024 selon le mod√®le\n",
    "        batch_features = torch.mean(batch_features, dim=[2, 3])\n",
    "\n",
    "        features_list.extend(batch_features.cpu().numpy())\n",
    "        labels_list.extend([1 if label == 'fake' else 0] * len(batch_features))\n",
    "\n",
    "        del batch_tensor, batch_features\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return features_list, labels_list\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Chargement du mod√®le e4e/pSp\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "\n",
    "    print('üîç Chargement du mod√®le pSp/e4e...')\n",
    "    assert os.path.exists(ckpt_path), f\"‚ö†Ô∏è Checkpoint introuvable : {ckpt_path}\"\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"üìå Mod√®le charg√© sur : {device}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Entra√Ænement LightGBM optimis√©\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    print(\"üìä Taille dataset avant √©quilibrage :\", np.bincount(y))\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print(\"‚úÖ Taille apr√®s √©quilibrage :\", np.bincount(y_res))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "    del X_res\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    "    )\n",
    "    del X_scaled, y_res\n",
    "\n",
    "    # ‚ö°Ô∏è LightGBM optimis√©\n",
    "    clf = LGBMClassifier(\n",
    "        n_estimators=500,          # ‚Üì Moins d‚Äôarbres = plus rapide\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        force_col_wise=True,       # ‚ö°Ô∏è supprime l‚Äôoverhead CPU\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # ‚úÖ √âvaluation\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"üéØ Accuracy (train): {acc_train:.4f}\")\n",
    "    print(f\"üéØ Accuracy (test) : {acc_test:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "    # üìä Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('üß© Matrice de confusion')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # üìà Accuracy bar\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(['Train', 'Test'], [acc_train, acc_test], color=['#4CAF50', '#2196F3'])\n",
    "    plt.title('üìä Accuracy du mod√®le (train vs test)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"accuracy_train_test.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Validation crois√©e rapide\n",
    "    print(\"üìà Validation crois√©e rapide...\")\n",
    "    scores = cross_val_score(clf, X_train[:500], y_train[:500], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    print(f\"üìä Moyenne validation crois√©e (sample): {scores.mean():.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(range(1, len(scores)+1), scores, marker='o', linestyle='--', color='#FF9800')\n",
    "    plt.title('üìà Validation crois√©e rapide')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"cross_validation_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(\"‚úÖ Graphiques sauvegard√©s dans :\", output_dir)\n",
    "    return clf, scaler\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN PIPELINE\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_fastss'\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    features_real, labels_real = process_frames_from_folder(path_real, model, 'real', device=device, batch_size=4)\n",
    "    features_fake, labels_fake = process_frames_from_folder(path_fake, model, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    X = np.array(features_real + features_fake)\n",
    "    y = np.array(labels_real + labels_fake)\n",
    "    print(f\"üìä Total features: {len(X)} (Real: {len(features_real)}, Fake: {len(features_fake)})\")\n",
    "\n",
    "    if len(X) == 0:\n",
    "        raise ValueError('‚ùå Aucune feature extraite ‚Äî v√©rifie tes dossiers de frames.')\n",
    "\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Mod√®le et scaler sauvegard√©s dans : {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef538c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Chargement du mod√®le pSp/e4e...\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üìå Mod√®le charg√© sur : cuda\n",
      "üîÑ Traitement de 790 images pour 'real' dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [04:53<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Traitement de 3975 images pour 'fake' dans C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [24:08<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total samples: 4765 (Real: 790, Fake: 3975)\n",
      "üìä Taille dataset avant √©quilibrage : [ 790 3975]\n",
      "‚úÖ Taille apr√®s √©quilibrage : [3975 3975]\n",
      "üéØ Accuracy (train): 0.7975\n",
      "üéØ Accuracy (test) : 0.7195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.82      0.56      0.67       795\n",
      "        Fake       0.67      0.87      0.76       795\n",
      "\n",
      "    accuracy                           0.72      1590\n",
      "   macro avg       0.74      0.72      0.71      1590\n",
      "weighted avg       0.74      0.72      0.71      1590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_19292\\1633671959.py:231: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_19292\\1633671959.py:232: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le et scaler sauvegard√©s dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Helper : convert tensor image to numpy [0,1]\n",
    "# ---------------------------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    \"\"\"\n",
    "    Convert a torch tensor image (C,H,W) or (N,C,H,W) to numpy in range [0,1].\n",
    "    Handles tensors in ranges [0,1] or [-1,1].\n",
    "    \"\"\"\n",
    "    t = tensor.detach().cpu()\n",
    "    # single image or batch\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "\n",
    "    # If values look like [-1,1], convert to [0,1]\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        # clamp then map [-1,1] -> [0,1]\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Compute reconstruction errors (robust)\n",
    "# ---------------------------------------------\n",
    "def compute_reconstruction_errors(model, image_tensor, device='cuda'):\n",
    "    \"\"\"\n",
    "    Encode -> Reconstruct (via model forward) -> Compute MSE and (1-SSIM) per image.\n",
    "    Uses model(image_tensor, randomize_noise=False, resize=False) to reconstruct.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            # Try to get reconstructed images using the model's forward\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            # Some older wrappers may have different signature - try with only image\n",
    "            out = model(image_tensor)\n",
    "\n",
    "        # out might be a tensor, or tuple/list where first element is images\n",
    "        if isinstance(out, (tuple, list)):\n",
    "            reconstructed = out[0]\n",
    "        else:\n",
    "            reconstructed = out\n",
    "\n",
    "    # Ensure shapes align\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        # attempt to resize or raise informative error\n",
    "        # Some implementations return images in [0,1] or [-1,1] and may also return different size.\n",
    "        # We will try to interpolate to input size if spatial dims mismatch.\n",
    "        if reconstructed.dim() == 4 and image_tensor.dim() == 4:\n",
    "            if reconstructed.shape[2:] != image_tensor.shape[2:]:\n",
    "                reconstructed = torch.nn.functional.interpolate(\n",
    "                    reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(f\"Shape mismatch between input {image_tensor.shape} and reconstructed {reconstructed.shape}\")\n",
    "\n",
    "    # Convert to numpy [0,1]\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    # Compute MSE per image\n",
    "    if orig_np.ndim == 3:\n",
    "        orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3:\n",
    "        rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1,2,3))\n",
    "\n",
    "    # Compute SSIM per image (skimage expects H,W,C, values in [0,1])\n",
    "    ssim_scores = []\n",
    "    for i in range(orig_np.shape[0]):\n",
    "        try:\n",
    "            s = ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0)\n",
    "        except TypeError:\n",
    "            # fallback for older skimage versions using multichannel argument\n",
    "            s = ssim(orig_np[i], rec_np[i], multichannel=True, data_range=1.0)\n",
    "        ssim_scores.append(s)\n",
    "\n",
    "    errors = np.stack([mse_loss, 1.0 - np.array(ssim_scores)], axis=1)\n",
    "    return errors\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Process frames in batches -> return error features\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, label, device='cuda', batch_size=4):\n",
    "    \"\"\"Calcule les erreurs de reconstruction pour chaque image.\"\"\"\n",
    "    errors_list, labels_list = [], []\n",
    "\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Traitement de {len(frame_paths)} images pour '{label}' dans {folder_path}...\")\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        print(f\"‚ö†Ô∏è Aucune image trouv√©e dans {folder_path}\")\n",
    "        return np.zeros((0,2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),  # range [0,1]\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                img_tensor = transform(img)\n",
    "                batch_images.append(img_tensor)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur chargement {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "\n",
    "        try:\n",
    "            errors = compute_reconstruction_errors(model, batch_tensor, device=device)\n",
    "            errors_list.append(errors)\n",
    "            labels_list += [1 if label == 'fake' else 0] * len(errors)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur pendant reconstruction/mesure sur un batch: {e}\")\n",
    "            # pour robustesse on skip ce batch\n",
    "            continue\n",
    "        finally:\n",
    "            del batch_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if len(errors_list) == 0:\n",
    "        return np.zeros((0,2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    errors_array = np.vstack(errors_list)\n",
    "    return errors_array.astype(np.float32), np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Chargement du mod√®le e4e/pSp\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "\n",
    "    print('üîç Chargement du mod√®le pSp/e4e...')\n",
    "    assert os.path.exists(ckpt_path), f\"‚ö†Ô∏è Checkpoint introuvable : {ckpt_path}\"\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"üìå Mod√®le charg√© sur : {device}\")\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Entra√Ænement du classificateur + visualisation\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"Aucun √©chantillon fourni au classifieur.\")\n",
    "\n",
    "    print(\"üìä Taille dataset avant √©quilibrage :\", np.bincount(y))\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print(\"‚úÖ Taille apr√®s √©quilibrage :\", np.bincount(y_res))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc_train = accuracy_score(y_train, clf.predict(X_train))\n",
    "    acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"üéØ Accuracy (train): {acc_train:.4f}\")\n",
    "    print(f\"üéØ Accuracy (test) : {acc_test:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('üß© Matrice de confusion')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return clf, scaler\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN PIPELINE\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan'\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    # Ajuste batch_size selon ta VRAM si besoin\n",
    "    errors_real, labels_real = process_frames_from_folder(path_real, model, 'real', device=device, batch_size=4)\n",
    "    errors_fake, labels_fake = process_frames_from_folder(path_fake, model, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    if errors_real.size == 0 and errors_fake.size == 0:\n",
    "        raise RuntimeError(\"Aucune erreur calcul√©e ‚Äî v√©rifie les chemins et le checkpoint du mod√®le.\")\n",
    "\n",
    "    X = np.vstack([errors_real, errors_fake]) if errors_real.size and errors_fake.size else (errors_real if errors_fake.size == 0 else errors_fake)\n",
    "    y = np.concatenate([labels_real, labels_fake]) if (labels_real.size and labels_fake.size) else (labels_real if labels_fake.size == 0 else labels_fake)\n",
    "    print(f\"üìä Total samples: {len(X)} (Real: {len(errors_real)}, Fake: {len(errors_fake)})\")\n",
    "\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Mod√®le et scaler sauvegard√©s dans : {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08c78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading pSp/e4e model...\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üìå Model loaded on cuda\n",
      "üîÑ Processing 790 images for 'real' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [04:49<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing 3975 images for 'fake' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [22:45<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total samples: 4765 (Real: 790, Fake: 3975)\n",
      "üìä Dataset size before balancing: [ 790 3975]\n",
      "‚úÖ Dataset size after balancing: [3975 3975]\n",
      "üéØ Accuracy (train): 0.7984\n",
      "üéØ Accuracy (test): 0.7208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.82      0.57      0.67       795\n",
      "        Fake       0.67      0.88      0.76       795\n",
      "\n",
      "    accuracy                           0.72      1590\n",
      "   macro avg       0.74      0.72      0.71      1590\n",
      "weighted avg       0.74      0.72      0.71      1590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12832\\3221495025.py:202: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12832\\3221495025.py:203: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classifier and scaler saved to: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# üöÄ Pipeline complet : GAN Inversion + DeepFake Detection\n",
    "# ==============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Convert torch tensor to numpy [0,1]\n",
    "# ---------------------------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1, 1)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Compute reconstruction errors\n",
    "# ---------------------------------------------\n",
    "def compute_reconstruction_errors(model, image_tensor, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    # Resize if necessary\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        if reconstructed.dim() == 4 and image_tensor.dim() == 4:\n",
    "            if reconstructed.shape[2:] != image_tensor.shape[2:]:\n",
    "                reconstructed = torch.nn.functional.interpolate(\n",
    "                    reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(f\"Shape mismatch {image_tensor.shape} vs {reconstructed.shape}\")\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3:\n",
    "        orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3:\n",
    "        rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1, 2, 3))\n",
    "\n",
    "    ssim_scores = []\n",
    "    for i in range(orig_np.shape[0]):\n",
    "        try:\n",
    "            s = ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0)\n",
    "        except TypeError:\n",
    "            s = ssim(orig_np[i], rec_np[i], multichannel=True, data_range=1.0)\n",
    "        ssim_scores.append(s)\n",
    "\n",
    "    errors = np.stack([mse_loss, 1.0 - np.array(ssim_scores)], axis=1)\n",
    "    return errors\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Process folder images in batches\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, label, device='cuda', batch_size=4):\n",
    "    errors_list, labels_list = [], []\n",
    "\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Processing {len(frame_paths)} images for '{label}' in {folder_path}\")\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        print(f\"‚ö†Ô∏è No images found in {folder_path}\")\n",
    "        return np.zeros((0, 2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i + batch_size]\n",
    "        batch_images = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                img_tensor = transform(img)\n",
    "                batch_images.append(img_tensor)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed loading {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "\n",
    "        try:\n",
    "            errors = compute_reconstruction_errors(model, batch_tensor, device=device)\n",
    "            errors_list.append(errors)\n",
    "            labels_list += [1 if label == 'fake' else 0] * len(errors)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in batch: {e}\")\n",
    "            continue\n",
    "        finally:\n",
    "            del batch_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if len(errors_list) == 0:\n",
    "        return np.zeros((0, 2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    errors_array = np.vstack(errors_list)\n",
    "    return errors_array.astype(np.float32), np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Load e4e/pSp model\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "\n",
    "    print('üîç Loading pSp/e4e model...')\n",
    "    assert os.path.exists(ckpt_path), f\"‚ö†Ô∏è Checkpoint not found: {ckpt_path}\"\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"üìå Model loaded on {device}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Train classifier and visualize\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    if len(X) == 0:\n",
    "        raise ValueError(\"No samples provided.\")\n",
    "\n",
    "    print(\"üìä Dataset size before balancing:\", np.bincount(y))\n",
    "\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "    print(\"‚úÖ Dataset size after balancing:\", np.bincount(y_res))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    "    )\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"üéØ Accuracy (train): {accuracy_score(y_train, clf.predict(X_train)):.4f}\")\n",
    "    print(f\"üéØ Accuracy (test): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('üß© Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return clf, scaler\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN PIPELINE\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    # Batch size adjustable\n",
    "    errors_real, labels_real = process_frames_from_folder(path_real, model, 'real', device=device, batch_size=4)\n",
    "    errors_fake, labels_fake = process_frames_from_folder(path_fake, model, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    if errors_real.size == 0 and errors_fake.size == 0:\n",
    "        raise RuntimeError(\"No errors computed ‚Äî check paths and model checkpoint.\")\n",
    "\n",
    "    X = np.vstack([errors_real, errors_fake]) if errors_real.size and errors_fake.size else (errors_real if errors_fake.size == 0 else errors_fake)\n",
    "    y = np.concatenate([labels_real, labels_fake]) if (labels_real.size and labels_fake.size) else (labels_real if labels_fake.size == 0 else labels_fake)\n",
    "\n",
    "    print(f\"üìä Total samples: {len(X)} (Real: {len(errors_real)}, Fake: {len(errors_fake)})\")\n",
    "\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Classifier and scaler saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa48bcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üé¨ id0_0000.mp4 --> Fake (fake_ratio=0.70)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import joblib\n",
    "from argparse import Namespace\n",
    "\n",
    "# ------------------------\n",
    "# Config\n",
    "# ------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_ckpt = r\"encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\"\n",
    "classifier_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan\\deepfake_classifier.pkl\"\n",
    "scaler_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan\\scaler.pkl\"\n",
    "video_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\real\\id0_0000.mp4\"\n",
    "frame_step = 5  # tester 1 frame toutes les 5 pour acc√©l√©rer\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ------------------------\n",
    "# Fonctions utilitaires\n",
    "# ------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    else:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    arr = np.clip((arr + 1)/2.0, 0.0, 1.0) if arr.min() < 0 else np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    opts = ckpt['opts']\n",
    "    opts['checkpoint_path'] = ckpt_path\n",
    "    opts = Namespace(**opts)\n",
    "    model = pSp(opts)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "def compute_errors(img, model):\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(img_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(img_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    if reconstructed.shape != img_tensor.shape:\n",
    "        reconstructed = F.interpolate(reconstructed, size=img_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "    orig_np = tensor_to_numpy01(img_tensor)[0]\n",
    "    rec_np = tensor_to_numpy01(reconstructed)[0]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np)**2)\n",
    "    try:\n",
    "        s = ssim(orig_np, rec_np, channel_axis=2, data_range=1.0)\n",
    "    except TypeError:\n",
    "        s = ssim(orig_np, rec_np, multichannel=True, data_range=1.0)\n",
    "\n",
    "    return np.array([[mse_loss, 1 - s]], dtype=np.float32)\n",
    "\n",
    "def predict_frame(img, model, classifier, scaler):\n",
    "    errors = compute_errors(img, model)\n",
    "    errors_scaled = scaler.transform(errors)\n",
    "    pred = classifier.predict(errors_scaled)\n",
    "    return pred[0]\n",
    "\n",
    "# ------------------------\n",
    "# Charger mod√®le et classifieur\n",
    "# ------------------------\n",
    "model = load_model(model_ckpt, device=device)\n",
    "classifier = joblib.load(classifier_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# ------------------------\n",
    "# Tester vid√©o\n",
    "# ------------------------\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_preds = []\n",
    "frame_idx = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_idx % frame_step == 0:  # √©chantillonnage\n",
    "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        try:\n",
    "            pred = predict_frame(img, model, classifier, scaler)\n",
    "            frame_preds.append(pred)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Frame error: {e}\")\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "\n",
    "if frame_preds:\n",
    "    fake_ratio = np.mean(frame_preds)\n",
    "    video_label = \"Fake\" if fake_ratio > 0.5 else \"Real\"\n",
    "    print(f\"üé¨ {os.path.basename(video_path)} --> {video_label} (fake_ratio={fake_ratio:.2f})\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun frame trait√©, v√©rifie la vid√©o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c05624e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.6/124.9 MB 9.3 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 4.2/124.9 MB 10.5 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 6.0/124.9 MB 10.2 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 8.7/124.9 MB 10.7 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 10.7/124.9 MB 10.6 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 13.4/124.9 MB 10.9 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 14.7/124.9 MB 11.0 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 16.5/124.9 MB 10.1 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 19.1/124.9 MB 10.3 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 21.8/124.9 MB 10.5 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 24.4/124.9 MB 10.6 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 26.7/124.9 MB 10.7 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 29.4/124.9 MB 10.8 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 32.0/124.9 MB 10.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 34.6/124.9 MB 10.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 37.0/124.9 MB 11.0 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 39.6/124.9 MB 11.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 41.9/124.9 MB 11.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 44.6/124.9 MB 11.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 46.9/124.9 MB 11.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 49.5/124.9 MB 11.2 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 51.9/124.9 MB 11.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 54.5/124.9 MB 11.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 56.9/124.9 MB 11.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 59.5/124.9 MB 11.3 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 62.1/124.9 MB 11.3 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 64.5/124.9 MB 11.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 67.1/124.9 MB 11.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 69.7/124.9 MB 11.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 72.1/124.9 MB 11.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 74.4/124.9 MB 11.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 77.1/124.9 MB 11.4 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 79.4/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 82.1/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 84.7/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 87.0/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 89.7/124.9 MB 11.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 92.0/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 94.4/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 97.0/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 99.6/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 101.7/124.9 MB 11.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 104.3/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 106.7/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 109.3/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 111.9/124.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 114.3/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 116.9/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 119.3/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  121.9/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.3/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  124.8/124.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 124.9/124.9 MB 11.3 MB/s  0:00:11\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91780469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üîÑ Processing 790 images for 'real' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [06:56<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing 3975 images for 'fake' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [22:15<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total samples: 4765 (Real: 790, Fake: 3975)\n",
      "üéØ Accuracy (train): 0.7053\n",
      "üéØ Accuracy (test) : 0.6264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.63      0.63      0.63       795\n",
      "        Fake       0.63      0.63      0.63       795\n",
      "\n",
      "    accuracy                           0.63      1590\n",
      "   macro avg       0.63      0.63      0.63      1590\n",
      "weighted avg       0.63      0.63      0.63      1590\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12832\\2818429873.py:178: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_12832\\2818429873.py:179: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classifier and scaler saved in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import xgboost as xgb  # ‚úÖ Utilisation de XGBoost\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Helper : convert tensor image to numpy [0,1]\n",
    "# ---------------------------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Compute reconstruction errors\n",
    "# ---------------------------------------------\n",
    "def compute_reconstruction_errors(model, image_tensor, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        if reconstructed.dim() == 4 and image_tensor.dim() == 4:\n",
    "            if reconstructed.shape[2:] != image_tensor.shape[2:]:\n",
    "                reconstructed = torch.nn.functional.interpolate(\n",
    "                    reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(f\"Shape mismatch {image_tensor.shape} vs {reconstructed.shape}\")\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3:\n",
    "        orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3:\n",
    "        rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1,2,3))\n",
    "    ssim_scores = []\n",
    "    for i in range(orig_np.shape[0]):\n",
    "        try:\n",
    "            s = ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0)\n",
    "        except TypeError:\n",
    "            s = ssim(orig_np[i], rec_np[i], multichannel=True, data_range=1.0)\n",
    "        ssim_scores.append(s)\n",
    "\n",
    "    errors = np.stack([mse_loss, 1.0 - np.array(ssim_scores)], axis=1)\n",
    "    return errors\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Process frames from folder\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, label, device='cuda', batch_size=4):\n",
    "    errors_list, labels_list = [], []\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Processing {len(frame_paths)} images for '{label}' in {folder_path}...\")\n",
    "    if len(frame_paths) == 0:\n",
    "        return np.zeros((0,2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                batch_images.append(transform(img))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed loading {path}: {e}\")\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        try:\n",
    "            errors = compute_reconstruction_errors(model, batch_tensor, device=device)\n",
    "            errors_list.append(errors)\n",
    "            labels_list += [1 if label=='fake' else 0]*len(errors)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in batch: {e}\")\n",
    "        finally:\n",
    "            del batch_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if len(errors_list) == 0:\n",
    "        return np.zeros((0,2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "    errors_array = np.vstack(errors_list)\n",
    "    return errors_array.astype(np.float32), np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Load e4e/pSp model\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Train classifier (XGBoost)\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, stratify=y_res, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"üéØ Accuracy (train): {accuracy_score(y_train, clf.predict(X_train)):.4f}\")\n",
    "    print(f\"üéØ Accuracy (test) : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real','Fake']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Real','Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('üß© Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return clf, scaler\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing','pretrained_models','e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    errors_real, labels_real = process_frames_from_folder(path_real, model, 'real', device=device, batch_size=4)\n",
    "    errors_fake, labels_fake = process_frames_from_folder(path_fake, model, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    if errors_real.size==0 and errors_fake.size==0:\n",
    "        raise RuntimeError(\"No errors computed. Check paths or model checkpoint.\")\n",
    "\n",
    "    X = np.vstack([errors_real, errors_fake])\n",
    "    y = np.concatenate([labels_real, labels_fake])\n",
    "    print(f\"üìä Total samples: {len(X)} (Real: {len(errors_real)}, Fake: {len(errors_fake)})\")\n",
    "\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier_xgb.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Classifier and scaler saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cfd855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üîÑ Processing 790 images for 'real' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [05:01<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing 3975 images for 'fake' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [23:32<00:00,  1.42s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total samples: 4765 (Real: 790, Fake: 3975)\n",
      "üéØ Accuracy (train): 0.7066\n",
      "üéØ Accuracy (test) : 0.6314\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.63      0.63      0.63       795\n",
      "        Fake       0.63      0.64      0.63       795\n",
      "\n",
      "    accuracy                           0.63      1590\n",
      "   macro avg       0.63      0.63      0.63      1590\n",
      "weighted avg       0.63      0.63      0.63      1590\n",
      "\n",
      "‚úÖ Classifier and scaler saved in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_8780\\80568076.py:178: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_8780\\80568076.py:179: UserWarning: Glyph 129513 (\\N{JIGSAW PUZZLE PIECE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import xgboost as xgb  # ‚úÖ Utilisation de XGBoost\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Helper : convert tensor image to numpy [0,1]\n",
    "# ---------------------------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Compute reconstruction errors\n",
    "# ---------------------------------------------\n",
    "def compute_reconstruction_errors(model, image_tensor, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        if reconstructed.dim() == 4 and image_tensor.dim() == 4:\n",
    "            if reconstructed.shape[2:] != image_tensor.shape[2:]:\n",
    "                reconstructed = torch.nn.functional.interpolate(\n",
    "                    reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "                )\n",
    "        else:\n",
    "            raise RuntimeError(f\"Shape mismatch {image_tensor.shape} vs {reconstructed.shape}\")\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3:\n",
    "        orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3:\n",
    "        rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1,2,3))\n",
    "    ssim_scores = []\n",
    "    for i in range(orig_np.shape[0]):\n",
    "        try:\n",
    "            s = ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0)\n",
    "        except TypeError:\n",
    "            s = ssim(orig_np[i], rec_np[i], multichannel=True, data_range=1.0)\n",
    "        ssim_scores.append(s)\n",
    "\n",
    "    errors = np.stack([mse_loss, 1.0 - np.array(ssim_scores)], axis=1)\n",
    "    return errors\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Process frames from folder\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, label, device='cuda', batch_size=4):\n",
    "    errors_list, labels_list = [], []\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Processing {len(frame_paths)} images for '{label}' in {folder_path}...\")\n",
    "    if len(frame_paths) == 0:\n",
    "        return np.zeros((0,2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                batch_images.append(transform(img))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed loading {path}: {e}\")\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        try:\n",
    "            errors = compute_reconstruction_errors(model, batch_tensor, device=device)\n",
    "            errors_list.append(errors)\n",
    "            labels_list += [1 if label=='fake' else 0]*len(errors)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in batch: {e}\")\n",
    "        finally:\n",
    "            del batch_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if len(errors_list) == 0:\n",
    "        return np.zeros((0,2), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "    errors_array = np.vstack(errors_list)\n",
    "    return errors_array.astype(np.float32), np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Load e4e/pSp model\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Train classifier (XGBoost)\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, stratify=y_res, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=600,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"üéØ Accuracy (train): {accuracy_score(y_train, clf.predict(X_train)):.4f}\")\n",
    "    print(f\"üéØ Accuracy (test) : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real','Fake']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Real','Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('üß© Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return clf, scaler\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing','pretrained_models','e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust6'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    errors_real, labels_real = process_frames_from_folder(path_real, model, 'real', device=device, batch_size=4)\n",
    "    errors_fake, labels_fake = process_frames_from_folder(path_fake, model, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    if errors_real.size==0 and errors_fake.size==0:\n",
    "        raise RuntimeError(\"No errors computed. Check paths or model checkpoint.\")\n",
    "\n",
    "    X = np.vstack([errors_real, errors_fake])\n",
    "    y = np.concatenate([labels_real, labels_fake])\n",
    "    print(f\"üìä Total samples: {len(X)} (Real: {len(errors_real)}, Fake: {len(errors_fake)})\")\n",
    "\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier_xgb.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Classifier and scaler saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46983f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement du mod√®le e4e/pSp ...\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üì¶ Chargement du classifieur et du scaler ...\n",
      "üéûÔ∏è Total frames: 529\n",
      "\n",
      "üìà Moyenne probabilit√© 'Fake' : 0.503\n",
      "‚úÖ R√©sultat final pour la vid√©o : üü© REAL\n",
      "\n",
      "‚úÖ Verdict final : üü© REAL (score moyen = 0.503)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_8780\\3741065254.py:132: UserWarning: Glyph 129001 (\\N{LARGE GREEN SQUARE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_8780\\3741065254.py:133: UserWarning: Glyph 129001 (\\N{LARGE GREEN SQUARE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, f\"predictions_{os.path.basename(video_path)}.png\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from encoder4editing.models.psp import pSp\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================\n",
    "# üîß Fonctions utilitaires\n",
    "# =============================================\n",
    "\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "def compute_reconstruction_errors(model, image_tensor, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        reconstructed = torch.nn.functional.interpolate(\n",
    "            reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3: orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3: rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1,2,3))\n",
    "    ssim_scores = [ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
    "    errors = np.stack([mse_loss, 1.0 - np.array(ssim_scores)], axis=1)\n",
    "    return errors\n",
    "\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =============================================\n",
    "# üé• Test d'une vid√©o\n",
    "# =============================================\n",
    "\n",
    "def test_video(video_path, model, classifier, scaler, device='cuda', output_dir='./results_test', frame_skip=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"‚ùå Impossible d'ouvrir la vid√©o: {video_path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"üéûÔ∏è Total frames: {total_frames}\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    all_errors = []\n",
    "    predictions = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_skip == 0:\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(img)\n",
    "            tensor_img = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "            try:\n",
    "                errors = compute_reconstruction_errors(model, tensor_img, device=device)\n",
    "                scaled = scaler.transform(errors.astype(np.float32))\n",
    "                prob = classifier.predict_proba(scaled)[0, 1]  # probabilit√© de \"Fake\"\n",
    "                predictions.append(prob)\n",
    "                all_errors.append(errors[0])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Frame {frame_idx}: {e}\")\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # üìä R√©sum√© et visualisation\n",
    "    # --------------------------------------------\n",
    "    all_errors = np.array(all_errors)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_fake_prob = predictions.mean()\n",
    "\n",
    "    print(f\"\\nüìà Moyenne probabilit√© 'Fake' : {mean_fake_prob:.3f}\")\n",
    "    verdict = \"üü• FAKE\" if mean_fake_prob > 0.6 else \"üü© REAL\"\n",
    "    print(f\"‚úÖ R√©sultat final pour la vid√©o : {verdict}\")\n",
    "\n",
    "    # Sauvegarde du graphique\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(predictions, label='Probabilit√© Fake par frame', color='red')\n",
    "    plt.axhline(0.5, color='gray', linestyle='--')\n",
    "    plt.title(f\"D√©tection GAN inversion ‚Äî {verdict}\")\n",
    "    plt.xlabel(\"Frames (√©chantillons)\")\n",
    "    plt.ylabel(\"Probabilit√© Fake\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"predictions_{os.path.basename(video_path)}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return verdict, mean_fake_prob, predictions\n",
    "\n",
    "# =============================================\n",
    "# üöÄ MAIN\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # ‚öôÔ∏è Chemins √† adapter\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    classifier_path = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust6\\deepfake_classifier_xgb.pkl'\n",
    "    scaler_path = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust6\\scaler.pkl'\n",
    "\n",
    "    # üé• ‚úÖ Vid√©o √† tester\n",
    "    video_path = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\real\\id0_0003.mp4'\n",
    "\n",
    "    # üìÅ Dossier de sortie\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_test_video'\n",
    "\n",
    "    print(\"üì¶ Chargement du mod√®le e4e/pSp ...\")\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    print(\"üì¶ Chargement du classifieur et du scaler ...\")\n",
    "    classifier = joblib.load(classifier_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "\n",
    "    # üöÄ Test de la vid√©o\n",
    "    verdict, score, preds = test_video(video_path, model, classifier, scaler, device=device, output_dir=output_dir)\n",
    "\n",
    "    print(f\"\\n‚úÖ Verdict final : {verdict} (score moyen = {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8244b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement du mod√®le e4e/pSp ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_24692\\687490820.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Chargement du classifieur et du scaler ...\n",
      "üéûÔ∏è Total frames: 303\n",
      "\n",
      "üìà Moyenne probabilit√© 'Fake' : 0.594\n",
      "‚úÖ R√©sultat final pour la vid√©o : üü• FAKE\n",
      "\n",
      "‚úÖ Verdict final : üü• FAKE (score moyen = 0.594)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_24692\\687490820.py:132: UserWarning: Glyph 128997 (\\N{LARGE RED SQUARE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_24692\\687490820.py:133: UserWarning: Glyph 128997 (\\N{LARGE RED SQUARE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(os.path.join(output_dir, f\"predictions_{os.path.basename(video_path)}.png\"))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from encoder4editing.models.psp import pSp\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================================\n",
    "# üîß Fonctions utilitaires\n",
    "# =============================================\n",
    "\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "def compute_reconstruction_errors(model, image_tensor, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        reconstructed = torch.nn.functional.interpolate(\n",
    "            reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3: orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3: rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1,2,3))\n",
    "    ssim_scores = [ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
    "    errors = np.stack([mse_loss, 1.0 - np.array(ssim_scores)], axis=1)\n",
    "    return errors\n",
    "\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# =============================================\n",
    "# üé• Test d'une vid√©o\n",
    "# =============================================\n",
    "\n",
    "def test_video(video_path, model, classifier, scaler, device='cuda', output_dir='./results_test', frame_skip=10):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"‚ùå Impossible d'ouvrir la vid√©o: {video_path}\")\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"üéûÔ∏è Total frames: {total_frames}\")\n",
    "\n",
    "    frame_idx = 0\n",
    "    all_errors = []\n",
    "    predictions = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_idx % frame_skip == 0:\n",
    "            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(img)\n",
    "            tensor_img = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "            try:\n",
    "                errors = compute_reconstruction_errors(model, tensor_img, device=device)\n",
    "                scaled = scaler.transform(errors.astype(np.float32))\n",
    "                prob = classifier.predict_proba(scaled)[0, 1]  # probabilit√© de \"Fake\"\n",
    "                predictions.append(prob)\n",
    "                all_errors.append(errors[0])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Frame {frame_idx}: {e}\")\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # üìä R√©sum√© et visualisation\n",
    "    # --------------------------------------------\n",
    "    all_errors = np.array(all_errors)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_fake_prob = predictions.mean()\n",
    "\n",
    "    print(f\"\\nüìà Moyenne probabilit√© 'Fake' : {mean_fake_prob:.3f}\")\n",
    "    verdict = \"üü• FAKE\" if mean_fake_prob > 0.5 else \"üü© REAL\"\n",
    "    print(f\"‚úÖ R√©sultat final pour la vid√©o : {verdict}\")\n",
    "\n",
    "    # Sauvegarde du graphique\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(predictions, label='Probabilit√© Fake par frame', color='red')\n",
    "    plt.axhline(0.5, color='gray', linestyle='--')\n",
    "    plt.title(f\"D√©tection GAN inversion ‚Äî {verdict}\")\n",
    "    plt.xlabel(\"Frames (√©chantillons)\")\n",
    "    plt.ylabel(\"Probabilit√© Fake\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"predictions_{os.path.basename(video_path)}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return verdict, mean_fake_prob, predictions\n",
    "\n",
    "# =============================================\n",
    "# üöÄ MAIN\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # ‚öôÔ∏è Chemins √† adapter\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    classifier_path = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust6\\deepfake_classifier_xgb.pkl'\n",
    "    scaler_path = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust6\\scaler.pkl'\n",
    "\n",
    "    # üé• ‚úÖ Vid√©o √† tester\n",
    "    video_path = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\fake\\id0_id17_0001.mp4'\n",
    "\n",
    "    # üìÅ Dossier de sortie\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_test_video'\n",
    "\n",
    "    print(\"üì¶ Chargement du mod√®le e4e/pSp ...\")\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    print(\"üì¶ Chargement du classifieur et du scaler ...\")\n",
    "    classifier = joblib.load(classifier_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "\n",
    "    # üöÄ Test de la vid√©o\n",
    "    verdict, score, preds = test_video(video_path, model, classifier, scaler, device=device, output_dir=output_dir)\n",
    "\n",
    "    print(f\"\\n‚úÖ Verdict final : {verdict} (score moyen = {score:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ed37a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lpips\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (2025.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision>=0.2.1->lpips) (11.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "Installing collected packages: lpips\n",
      "Successfully installed lpips-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710a375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading e4e model...\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "üîπ Initializing LPIPS metric...\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\EliteLaptop/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 233M/233M [00:25<00:00, 9.42MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "üîÑ Processing 790 images for 'real' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [06:04<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing 3975 images for 'fake' in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [24:51<00:00,  1.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total samples: 4765 (Real: 790, Fake: 3975)\n",
      "üéØ Accuracy (train): 0.9605\n",
      "üéØ Accuracy (test) : 0.8805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.85      0.93      0.89       795\n",
      "        Fake       0.92      0.83      0.87       795\n",
      "\n",
      "    accuracy                           0.88      1590\n",
      "   macro avg       0.88      0.88      0.88      1590\n",
      "weighted avg       0.88      0.88      0.88      1590\n",
      "\n",
      "‚úÖ Classifier and scaler saved in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust_optimized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "import lpips  # ‚úÖ pip install lpips\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Convert tensor image to numpy [0,1]\n",
    "# ---------------------------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Compute reconstruction + perceptual errors\n",
    "# ---------------------------------------------\n",
    "def compute_reconstruction_errors(model, image_tensor, lpips_fn, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    # Ajuste les dimensions si n√©cessaire\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        reconstructed = torch.nn.functional.interpolate(\n",
    "            reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3:\n",
    "        orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3:\n",
    "        rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    # üîπ MSE & SSIM\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1, 2, 3))\n",
    "    ssim_scores = [ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
    "\n",
    "    # üîπ PSNR\n",
    "    psnr_vals = [20 * np.log10(1.0 / np.sqrt(m + 1e-8)) for m in mse_loss]\n",
    "\n",
    "    # üîπ LPIPS (perceptual similarity)\n",
    "    with torch.no_grad():\n",
    "        lpips_vals = lpips_fn(image_tensor, reconstructed).detach().cpu().numpy().flatten()\n",
    "\n",
    "    # üîπ Cosine similarity\n",
    "    cosine_vals = [\n",
    "        F.cosine_similarity(image_tensor[i].flatten(), reconstructed[i].flatten(), dim=0).item()\n",
    "        for i in range(image_tensor.shape[0])\n",
    "    ]\n",
    "\n",
    "    # ‚úÖ Final feature vector\n",
    "    errors = np.stack([\n",
    "        mse_loss,                 # 1\n",
    "        1.0 - np.array(ssim_scores),  # 2\n",
    "        np.array(psnr_vals),      # 3\n",
    "        np.array(lpips_vals),     # 4\n",
    "        1.0 - np.array(cosine_vals)  # 5\n",
    "    ], axis=1)\n",
    "    return errors\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîß Process frames from folder\n",
    "# ---------------------------------------------\n",
    "def process_frames_from_folder(folder_path, model, lpips_fn, label, device='cuda', batch_size=4):\n",
    "    errors_list, labels_list = [], []\n",
    "    exts = ['jpg', 'jpeg', 'png']\n",
    "    frame_paths = sorted(sum([\n",
    "        glob(os.path.join(folder_path, '**', f'*.{ext}'), recursive=True)\n",
    "        for ext in exts\n",
    "    ], []))\n",
    "\n",
    "    print(f\"üîÑ Processing {len(frame_paths)} images for '{label}' in {folder_path}...\")\n",
    "    if len(frame_paths) == 0:\n",
    "        return np.zeros((0, 5), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=f\"Processing {label}\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                batch_images.append(transform(img))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed loading {path}: {e}\")\n",
    "        if not batch_images:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_images).to(device)\n",
    "        try:\n",
    "            errors = compute_reconstruction_errors(model, batch_tensor, lpips_fn, device=device)\n",
    "            errors_list.append(errors)\n",
    "            labels_list += [1 if label == 'fake' else 0] * len(errors)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in batch: {e}\")\n",
    "        finally:\n",
    "            del batch_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if len(errors_list) == 0:\n",
    "        return np.zeros((0, 5), dtype=np.float32), np.array([], dtype=np.int32)\n",
    "    errors_array = np.vstack(errors_list)\n",
    "    return errors_array.astype(np.float32), np.array(labels_list, dtype=np.int32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üß† Load e4e/pSp model\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "    assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ‚öôÔ∏è Train classifier (Optimized XGBoost)\n",
    "# ---------------------------------------------\n",
    "def train_balanced_classifier(X, y, output_dir):\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_res).astype(np.float32)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_res, test_size=0.2, stratify=y_res, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=1200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        gamma=0.1,\n",
    "        reg_lambda=1.2,\n",
    "        reg_alpha=0.5,\n",
    "        min_child_weight=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"üéØ Accuracy (train): {accuracy_score(y_train, clf.predict(X_train)):.4f}\")\n",
    "    print(f\"üéØ Accuracy (test) : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['Real', 'Fake'])\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return clf, scaler\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üöÄ MAIN\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "    path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust_optimized'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(\"üîπ Loading e4e model...\")\n",
    "    model = load_model(ckpt_path, device=device)\n",
    "\n",
    "    print(\"üîπ Initializing LPIPS metric...\")\n",
    "    lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "    # üî∏ Extract features\n",
    "    errors_real, labels_real = process_frames_from_folder(path_real, model, lpips_fn, 'real', device=device, batch_size=4)\n",
    "    errors_fake, labels_fake = process_frames_from_folder(path_fake, model, lpips_fn, 'fake', device=device, batch_size=4)\n",
    "\n",
    "    if errors_real.size == 0 and errors_fake.size == 0:\n",
    "        raise RuntimeError(\"No errors computed. Check paths or model checkpoint.\")\n",
    "\n",
    "    X = np.vstack([errors_real, errors_fake])\n",
    "    y = np.concatenate([labels_real, labels_fake])\n",
    "    print(f\"üìä Total samples: {len(X)} (Real: {len(errors_real)}, Fake: {len(errors_fake)})\")\n",
    "\n",
    "    # üî∏ Train classifier\n",
    "    classifier, scaler = train_balanced_classifier(X, y, output_dir)\n",
    "\n",
    "    joblib.dump(classifier, os.path.join(output_dir, 'deepfake_classifier_xgb.pkl'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.pkl'))\n",
    "    print(f\"‚úÖ Classifier and scaler saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0a04e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_24692\\1999592791.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "üîç Analyse de la vid√©o : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\fake\\id0_id1_0002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
      "Processing frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [03:03<00:00, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Moyenne probabilit√© 'fake' sur 35 frames : 0.7703\n",
      "‚úÖ Verdict global : FAKE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import lpips\n",
    "from encoder4editing.models.psp import pSp\n",
    "from argparse import Namespace\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üîß Reprend les fonctions de ton script d‚Äôentra√Ænement\n",
    "# ---------------------------------------------------------\n",
    "def tensor_to_numpy01(tensor):\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.dim() == 3:\n",
    "        arr = t.permute(1, 2, 0).numpy()\n",
    "    elif t.dim() == 4:\n",
    "        arr = t.permute(0, 2, 3, 1).numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D or 4D\")\n",
    "    if arr.min() < -0.1 or arr.max() > 1.1:\n",
    "        arr = np.clip(arr, -1.0, 1.0)\n",
    "        arr = (arr + 1.0) / 2.0\n",
    "    else:\n",
    "        arr = np.clip(arr, 0.0, 1.0)\n",
    "    return arr\n",
    "\n",
    "def compute_reconstruction_errors(model, image_tensor, lpips_fn, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            out = model(image_tensor, randomize_noise=False, resize=False)\n",
    "        except TypeError:\n",
    "            out = model(image_tensor)\n",
    "        reconstructed = out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "    if reconstructed.shape != image_tensor.shape:\n",
    "        reconstructed = torch.nn.functional.interpolate(\n",
    "            reconstructed, size=image_tensor.shape[2:], mode='bilinear', align_corners=False\n",
    "        )\n",
    "\n",
    "    orig_np = tensor_to_numpy01(image_tensor)\n",
    "    rec_np = tensor_to_numpy01(reconstructed)\n",
    "\n",
    "    if orig_np.ndim == 3:\n",
    "        orig_np = orig_np[np.newaxis, ...]\n",
    "    if rec_np.ndim == 3:\n",
    "        rec_np = rec_np[np.newaxis, ...]\n",
    "\n",
    "    mse_loss = np.mean((orig_np - rec_np) ** 2, axis=(1, 2, 3))\n",
    "    ssim_scores = [ssim(orig_np[i], rec_np[i], channel_axis=2, data_range=1.0) for i in range(orig_np.shape[0])]\n",
    "    psnr_vals = [20 * np.log10(1.0 / np.sqrt(m + 1e-8)) for m in mse_loss]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lpips_vals = lpips_fn(image_tensor, reconstructed).detach().cpu().numpy().flatten()\n",
    "\n",
    "    cosine_vals = [\n",
    "        F.cosine_similarity(image_tensor[i].flatten(), reconstructed[i].flatten(), dim=0).item()\n",
    "        for i in range(image_tensor.shape[0])\n",
    "    ]\n",
    "\n",
    "    errors = np.stack([\n",
    "        mse_loss,\n",
    "        1.0 - np.array(ssim_scores),\n",
    "        np.array(psnr_vals),\n",
    "        np.array(lpips_vals),\n",
    "        1.0 - np.array(cosine_vals)\n",
    "    ], axis=1)\n",
    "    return errors\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üß© Fonction pour extraire des frames d‚Äôune vid√©o\n",
    "# ---------------------------------------------------------\n",
    "def extract_frames(video_path, output_size=(256, 256), frame_skip=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(output_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    frames = []\n",
    "    count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if count % frame_skip == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_tensor = transform(frame_rgb)\n",
    "            frames.append(frame_tensor)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üß† Charger le mod√®le e4e + classifieur\n",
    "# ---------------------------------------------------------\n",
    "def load_e4e_model(ckpt_path, device='cuda'):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üöÄ Pr√©diction sur une vid√©o\n",
    "# ---------------------------------------------------------\n",
    "def predict_video(video_path, model, lpips_fn, clf, scaler, device='cuda', batch_size=4):\n",
    "    print(f\"üîç Analyse de la vid√©o : {video_path}\")\n",
    "    frames = extract_frames(video_path, frame_skip=10)\n",
    "    if len(frames) == 0:\n",
    "        print(\"‚ùå Aucune frame extraite.\")\n",
    "        return None\n",
    "\n",
    "    all_errors = []\n",
    "    for i in tqdm(range(0, len(frames), batch_size), desc=\"Processing frames\"):\n",
    "        batch = torch.stack(frames[i:i+batch_size]).to(device)\n",
    "        errors = compute_reconstruction_errors(model, batch, lpips_fn, device)\n",
    "        all_errors.append(errors)\n",
    "    X = np.vstack(all_errors).astype(np.float32)\n",
    "\n",
    "    X_scaled = scaler.transform(X)\n",
    "    preds = clf.predict_proba(X_scaled)[:, 1]  # proba \"fake\"\n",
    "    mean_score = np.mean(preds)\n",
    "\n",
    "    print(f\"üìä Moyenne probabilit√© 'fake' sur {len(preds)} frames : {mean_score:.4f}\")\n",
    "    verdict = \"FAKE\" if mean_score > 0.5 else \"REAL\"\n",
    "    print(f\"‚úÖ Verdict global : {verdict}\")\n",
    "    return mean_score, verdict\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# üèÅ MAIN\n",
    "# ---------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "    model = load_e4e_model(ckpt_path, device)\n",
    "    lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "    output_dir = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_Xgboust_optimized'\n",
    "    clf = joblib.load(os.path.join(output_dir, 'deepfake_classifier_xgb.pkl'))\n",
    "    scaler = joblib.load(os.path.join(output_dir, 'scaler.pkl'))\n",
    "\n",
    "    # üî∏ Teste une vid√©o\n",
    "    video_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\\fake\\id0_id1_0002.mp4\"\n",
    "    predict_video(video_path, model, lpips_fn, clf, scaler, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee02d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "Found 790 frames in C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [05:58<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (790, 18, 512)\n",
      "Mean temporal variance of latent vectors: 0.094481\n",
      "Latents saved to video_latents_real.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Config\n",
    "# ---------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 4  # taille du batch\n",
    "\n",
    "# Chemins fournis\n",
    "path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "\n",
    "# Choisir quel dossier traiter\n",
    "video_frames_folder = path_real  # ou path_fake\n",
    "output_latents_file = \"video_latents_real.npy\"  # ou \"video_latents_fake.npy\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Load e4e / pSp model\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "model = load_model(ckpt_path, device=device)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Transform for images\n",
    "# ---------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Gather all frames (recursive)\n",
    "# ---------------------------------------------\n",
    "frame_paths = sorted(glob(os.path.join(video_frames_folder, '**', '*.*'), recursive=True))\n",
    "frame_paths = [f for f in frame_paths if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "print(f\"Found {len(frame_paths)} frames in {video_frames_folder}.\")\n",
    "\n",
    "if len(frame_paths) == 0:\n",
    "    raise RuntimeError(f\"Aucune image trouv√©e dans {video_frames_folder} ! V√©rifie les sous-dossiers et extensions.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Frame-by-frame GAN Inversion en batch\n",
    "# ---------------------------------------------\n",
    "all_latents = []\n",
    "reconstructed_frames = []\n",
    "\n",
    "for i in tqdm(range(0, len(frame_paths), batch_size), desc=\"Inverting frames\"):\n",
    "    batch_paths = frame_paths[i:i+batch_size]\n",
    "    batch_imgs = []\n",
    "    for path in batch_paths:\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img_tensor = transform(img)\n",
    "            batch_imgs.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed loading {path}: {e}\")\n",
    "\n",
    "    if not batch_imgs:\n",
    "        continue\n",
    "\n",
    "    batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_tensor, return_latents=True)\n",
    "\n",
    "        # Si sortie tuple (reconstruction, w)\n",
    "        if isinstance(outputs, (tuple, list)):\n",
    "            reconstructed, w = outputs\n",
    "        else:\n",
    "            w = outputs\n",
    "            reconstructed = model.decoder(w, input_is_latent=True)[0]\n",
    "\n",
    "        # Sauvegarder latents et reconstructions\n",
    "        all_latents.append(w.cpu().numpy())\n",
    "        reconstructed_frames.append(reconstructed.cpu().numpy())\n",
    "\n",
    "    # lib√©ration VRAM entre les batchs\n",
    "    del batch_tensor, batch_imgs, w, reconstructed\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ V√©rification avant vstack\n",
    "# ---------------------------------------------\n",
    "if len(all_latents) == 0:\n",
    "    raise RuntimeError(\"Aucun latent calcul√©. V√©rifie tes images et ton mod√®le.\")\n",
    "else:\n",
    "    all_latents = np.vstack(all_latents)\n",
    "    print(f\"Latents shape: {all_latents.shape}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Temporal variation analysis\n",
    "# ---------------------------------------------\n",
    "latent_variance = np.var(all_latents, axis=0)\n",
    "mean_variance = np.mean(latent_variance)\n",
    "print(f\"Mean temporal variance of latent vectors: {mean_variance:.6f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Save results\n",
    "# ---------------------------------------------\n",
    "np.save(output_latents_file, all_latents)\n",
    "print(f\"Latents saved to {output_latents_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a271454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      " Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real \n",
      "==============================\n",
      "Found 790 frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [04:54<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (790, 18, 512)\n",
      "Mean temporal variance: 0.094481\n",
      "Latents saved ‚Üí video_latents_real.npy\n",
      "\n",
      "==============================\n",
      " Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake \n",
      "==============================\n",
      "Found 3975 frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [22:50<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (3975, 18, 512)\n",
      "Mean temporal variance: 0.090602\n",
      "Latents saved ‚Üí video_latents_fake.npy\n",
      "\n",
      "==============================\n",
      "     üîç FINAL COMPARISON      \n",
      "==============================\n",
      "Real video variance  : 0.094481\n",
      "Fake video variance  : 0.090602\n",
      "\n",
      "üîµ Les deux vid√©os semblent naturelles ‚Üí pas de signature GAN claire.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Config\n",
    "# ---------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 4\n",
    "\n",
    "# Dossiers des frames\n",
    "path_real = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real'\n",
    "path_fake = r'C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake'\n",
    "\n",
    "# Dossiers de sauvegarde des reconstructions\n",
    "recon_real = \"./reconstruction_real\"\n",
    "recon_fake = \"./reconstruction_fake\"\n",
    "os.makedirs(recon_real, exist_ok=True)\n",
    "os.makedirs(recon_fake, exist_ok=True)\n",
    "\n",
    "ckpt_path = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Load e4e model\n",
    "# ---------------------------------------------\n",
    "def load_model(ckpt_path, device='cuda'):\n",
    "    from encoder4editing.models.psp import pSp\n",
    "    from argparse import Namespace\n",
    "    ckpt = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "model = load_model(ckpt_path, device=device)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Transform for images\n",
    "# ---------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Function to invert a video (real or fake)\n",
    "# ---------------------------------------------\n",
    "def process_video(video_frames_folder, output_latents_file, recon_output_folder):\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\" Processing: {video_frames_folder} \")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # Gather frames\n",
    "    frame_paths = sorted(glob(os.path.join(video_frames_folder, '**', '*.*'), recursive=True))\n",
    "    frame_paths = [f for f in frame_paths if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    print(f\"Found {len(frame_paths)} frames.\")\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        raise RuntimeError(\"‚ö† Aucun frame trouv√© !\")\n",
    "\n",
    "    all_latents = []\n",
    "\n",
    "    # Batch processing\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=\"Inverting frames\"):\n",
    "        batch_paths = frame_paths[i:i + batch_size]\n",
    "        batch_imgs = []\n",
    "\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert('RGB')\n",
    "                img_tensor = transform(img)\n",
    "                batch_imgs.append(img_tensor)\n",
    "            except:\n",
    "                print(f\"‚ùå Failed loading: {path}\")\n",
    "\n",
    "        if not batch_imgs:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_tensor, return_latents=True)\n",
    "\n",
    "            # True GAN inversion: Encoder ‚Üí W+ ‚Üí Generator\n",
    "            if isinstance(outputs, (tuple, list)):\n",
    "                reconstructed, w = outputs\n",
    "            else:\n",
    "                w = outputs\n",
    "                reconstructed = model.decoder(w, input_is_latent=True)[0]\n",
    "\n",
    "        # üîπ Save reconstructed images (GAN reconstruction)\n",
    "        for n in range(reconstructed.shape[0]):\n",
    "            filename = os.path.basename(batch_paths[n])\n",
    "            save_path = os.path.join(recon_output_folder, filename)\n",
    "            vutils.save_image(reconstructed[n], save_path, normalize=True)\n",
    "\n",
    "        all_latents.append(w.cpu().numpy())\n",
    "\n",
    "        del batch_tensor, batch_imgs, w, reconstructed\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate\n",
    "    all_latents = np.vstack(all_latents)\n",
    "    print(f\"Latents shape: {all_latents.shape}\")\n",
    "\n",
    "    # Temporal variance\n",
    "    latent_variance = np.var(all_latents, axis=0)\n",
    "    mean_variance = float(np.mean(latent_variance))\n",
    "    print(f\"Mean temporal variance: {mean_variance:.6f}\")\n",
    "\n",
    "    # Save\n",
    "    np.save(output_latents_file, all_latents)\n",
    "    print(f\"Latents saved ‚Üí {output_latents_file}\")\n",
    "\n",
    "    return mean_variance\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# üîπ Run for REAL and FAKE\n",
    "# ---------------------------------------------\n",
    "real_variance = process_video(path_real, \"video_latents_real.npy\", recon_real)\n",
    "fake_variance = process_video(path_fake, \"video_latents_fake.npy\", recon_fake)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"     üîç FINAL COMPARISON      \")\n",
    "print(\"==============================\")\n",
    "print(f\"Real video variance  : {real_variance:.6f}\")\n",
    "print(f\"Fake video variance  : {fake_variance:.6f}\")\n",
    "\n",
    "if fake_variance < real_variance * 0.7:\n",
    "    print(\"\\nüü¢ Fake vid√©o d√©tect√©e : variance faible ‚Üí tr√®s stable ‚Üí GAN signature\")\n",
    "else:\n",
    "    print(\"\\nüîµ Les deux vid√©os semblent naturelles ‚Üí pas de signature GAN claire.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdc75d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading StyleGAN2 generator...\n",
      "‚úî Generator loaded\n",
      "\n",
      "==============================\n",
      " Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real \n",
      "==============================\n",
      "‚úî Found 790 frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Step 0/100 - Loss: 0.1758\n",
      "Step 50/100 - Loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   1%|          | 1/198 [18:53<62:01:53, 1133.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100 - Loss: 0.1297\n",
      "Step 50/100 - Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   1%|          | 2/198 [36:58<60:09:33, 1104.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100 - Loss: 0.1052\n",
      "Step 50/100 - Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   2%|‚ñè         | 3/198 [55:23<59:50:27, 1104.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100 - Loss: 0.1074\n",
      "Step 50/100 - Loss: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   2%|‚ñè         | 4/198 [1:14:08<59:58:02, 1112.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100 - Loss: 0.1468\n",
      "Step 50/100 - Loss: 0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   3%|‚ñé         | 5/198 [1:32:51<59:52:12, 1116.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/100 - Loss: 0.1547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   3%|‚ñé         | 5/198 [1:41:04<65:01:16, 1212.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 138\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variance\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# üîπ RUN ON REAL & FAKE VIDEO FRAMES\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m var_real \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo_latents_real.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecon_real\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m var_fake \u001b[38;5;241m=\u001b[39m process_video(path_fake, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_latents_fake.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, recon_fake)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==============================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 118\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(folder, save_npy, save_recon)\u001b[0m\n\u001b[0;32m    115\u001b[0m imgs \u001b[38;5;241m=\u001b[39m [transform(Image\u001b[38;5;241m.\u001b[39mopen(p)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m batch_paths]\n\u001b[0;32m    116\u001b[0m img_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(imgs)  \u001b[38;5;66;03m# [B, 3, 1024, 1024]\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ws_batch, recon_imgs_batch \u001b[38;5;241m=\u001b[39m \u001b[43minvert_images_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m all_latents\u001b[38;5;241m.\u001b[39mextend(ws_batch)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Save reconstructed images\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m, in \u001b[0;36minvert_images_batch\u001b[1;34m(img_tensors)\u001b[0m\n\u001b[0;32m     79\u001b[0m synth \u001b[38;5;241m=\u001b[39m (synth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     80\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(synth, img_tensors)\n\u001b[1;32m---> 81\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================\n",
    "# üîπ Add StyleGAN2-ADA-PyTorch folder to Python path\n",
    "# ============================================================\n",
    "repo_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ada-pytorch\"\n",
    "sys.path.insert(0, repo_path)\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "# ============================================================\n",
    "# üîπ CONFIGURATION\n",
    "# ============================================================\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "steps = 100\n",
    "lr = 0.05\n",
    "batch_size = 4  # üîπ Utilisation du batch\n",
    "\n",
    "# Paths\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "recon_real = \"./reconstruction_real\"\n",
    "recon_fake = \"./reconstruction_fake\"\n",
    "os.makedirs(recon_real, exist_ok=True)\n",
    "os.makedirs(recon_fake, exist_ok=True)\n",
    "\n",
    "ckpt_path = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ffhq-config-f.pkl\"\n",
    "\n",
    "# ============================================================\n",
    "# üîπ LOAD STYLEGAN2 GENERATOR (G_ema)\n",
    "# ============================================================\n",
    "print(\"üîπ Loading StyleGAN2 generator...\")\n",
    "with open(ckpt_path, \"rb\") as f:\n",
    "    G = legacy.load_network_pkl(f)[\"G_ema\"].to(device)\n",
    "G.eval()\n",
    "print(\"‚úî Generator loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# üîπ TRANSFORMER\n",
    "# ============================================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ============================================================\n",
    "# üîπ INVERSION : optimize W latent using gradient descent (batch)\n",
    "# ============================================================\n",
    "def invert_images_batch(img_tensors):\n",
    "    \"\"\"\n",
    "    img_tensors: Tensor of shape [B, 3, 1024, 1024]\n",
    "    Returns:\n",
    "        ws: [B, num_ws, 512]\n",
    "        recon_imgs: [B, 3, 1024, 1024]\n",
    "    \"\"\"\n",
    "    B = img_tensors.size(0)\n",
    "    img_tensors = img_tensors.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w_opt = G.mapping.w_avg.repeat(B, G.num_ws, 1)\n",
    "\n",
    "    w_opt = w_opt.clone().detach().requires_grad_(True)\n",
    "    optimizer = optim.Adam([w_opt], lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        synth = G.synthesis(w_opt, noise_mode=\"const\")\n",
    "        synth = (synth + 1) / 2\n",
    "        loss = loss_fn(synth, img_tensors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}/{steps} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_imgs = G.synthesis(w_opt, noise_mode=\"const\")\n",
    "        final_imgs = (final_imgs + 1) / 2\n",
    "\n",
    "    return w_opt.detach().cpu().numpy(), final_imgs.detach()\n",
    "\n",
    "# ============================================================\n",
    "# üîπ PROCESS VIDEO FRAMES IN BATCH\n",
    "# ============================================================\n",
    "def process_video(folder, save_npy, save_recon):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\" Processing: {folder} \")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # Recursive glob for nested subfolders\n",
    "    frame_paths = sorted(glob(os.path.join(folder, \"**\", \"*.*\"), recursive=True))\n",
    "    frame_paths = [f for f in frame_paths if f.lower().endswith((\"jpg\", \"jpeg\", \"png\"))]\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        raise RuntimeError(\"‚ùå Aucun frame trouv√© !\")\n",
    "\n",
    "    print(f\"‚úî Found {len(frame_paths)} frames.\")\n",
    "\n",
    "    all_latents = []\n",
    "\n",
    "    # Traitement par batch\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=\"Inverting\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        imgs = [transform(Image.open(p).convert(\"RGB\")) for p in batch_paths]\n",
    "        img_tensors = torch.stack(imgs)  # [B, 3, 1024, 1024]\n",
    "\n",
    "        ws_batch, recon_imgs_batch = invert_images_batch(img_tensors)\n",
    "        all_latents.extend(ws_batch)\n",
    "\n",
    "        # Save reconstructed images\n",
    "        for j, p in enumerate(batch_paths):\n",
    "            filename = os.path.basename(p)\n",
    "            save_path = os.path.join(save_recon, filename)\n",
    "            utils.save_image(recon_imgs_batch[j], save_path, normalize=True)\n",
    "\n",
    "    all_latents = np.array(all_latents)\n",
    "    np.save(save_npy, all_latents)\n",
    "    print(\"‚úî Latents saved:\", save_npy)\n",
    "\n",
    "    variance = np.mean(np.var(all_latents, axis=0))\n",
    "    print(f\"Temporal variance: {variance:.6f}\")\n",
    "    return variance\n",
    "\n",
    "# ============================================================\n",
    "# üîπ RUN ON REAL & FAKE VIDEO FRAMES\n",
    "# ============================================================\n",
    "var_real = process_video(path_real, \"video_latents_real.npy\", recon_real)\n",
    "var_fake = process_video(path_fake, \"video_latents_fake.npy\", recon_fake)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"        üîç FINAL REPORT\")\n",
    "print(\"==============================\")\n",
    "print(f\"Variance REAL : {var_real:.6f}\")\n",
    "print(f\"Variance FAKE : {var_fake:.6f}\")\n",
    "\n",
    "if var_fake < var_real * 0.7:\n",
    "    print(\"\\nüü¢ Fake d√©tect√©e : variance faible ‚Üí GAN signature\")\n",
    "else:\n",
    "    print(\"\\nüîµ Vid√©o naturelle : pas de signature GAN claire\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50366820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1 | CUDA available: True | Device: cuda\n",
      "Loading StyleGAN2 generator from: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ffhq-config-f.pkl\n",
      "‚úî StyleGAN2 loaded (G_ema)\n",
      "Loading e4e encoder from: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_6540\\4033722950.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî e4e encoder loaded\n",
      "\n",
      "==============================\n",
      " Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\n",
      "==============================\n",
      "Found 790 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [06:05<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latents to: video_latents_real.npy\n",
      "Mean temporal variance: 0.09448085725307465\n",
      "\n",
      "==============================\n",
      " Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\n",
      "==============================\n",
      "Found 3975 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3975/3975 [33:44<00:00,  1.96it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latents to: video_latents_fake.npy\n",
      "Mean temporal variance: 0.09060238301753998\n",
      "\n",
      "===== FINAL REPORT =====\n",
      "Real variance : 0.09448085725307465\n",
      "Fake variance : 0.09060238301753998\n",
      "=> Looks natural.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# ---------------------------\n",
    "# Paths (ADJUST if needed)\n",
    "# ---------------------------\n",
    "stylegan_repo = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ada-pytorch\"\n",
    "encoder_repo = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\"  # folder containing encoder4editing package\n",
    "e4e_ckpt = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\"\n",
    "stylegan_ckpt = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ffhq-config-f.pkl\"\n",
    "\n",
    "# Frame folders\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "# Output folders\n",
    "recon_real = \"./recon_real\"\n",
    "recon_fake = \"./recon_fake\"\n",
    "os.makedirs(recon_real, exist_ok=True)\n",
    "os.makedirs(recon_fake, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Add repos to PYTHONPATH\n",
    "# ---------------------------\n",
    "if os.path.isdir(stylegan_repo):\n",
    "    sys.path.insert(0, stylegan_repo)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"StyleGAN repo not found at: {stylegan_repo}\")\n",
    "\n",
    "if os.path.isdir(encoder_repo):\n",
    "    sys.path.insert(0, encoder_repo)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Encoder4Editing repo not found at: {encoder_repo}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Imports that depend on repos\n",
    "# ---------------------------\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "# We'll import pSp (e4e model)\n",
    "try:\n",
    "    from encoder4editing.models.psp import pSp\n",
    "except Exception as e:\n",
    "    # give a clear error if the import fails\n",
    "    raise ImportError(\"Failed to import pSp from encoder4editing. Make sure the encoder4editing repo is the correct one and in PYTHONPATH.\") from e\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "# ---------------------------\n",
    "# Device & transforms\n",
    "# ---------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Torch version: {torch.__version__} | CUDA available: {torch.cuda.is_available()} | Device: {device}\")\n",
    "\n",
    "img_res = 256  # e4e expects smaller input for encoder preprocessing (we use 256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_res, img_res)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Load StyleGAN2 generator\n",
    "# ---------------------------\n",
    "print(\"Loading StyleGAN2 generator from:\", stylegan_ckpt)\n",
    "if not os.path.isfile(stylegan_ckpt):\n",
    "    raise FileNotFoundError(f\"StyleGAN2 checkpoint not found at: {stylegan_ckpt}\")\n",
    "\n",
    "with open(stylegan_ckpt, \"rb\") as f:\n",
    "    G = legacy.load_network_pkl(f)[\"G_ema\"].to(device)\n",
    "G.eval()\n",
    "print(\"‚úî StyleGAN2 loaded (G_ema)\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load e4e encoder (pSp)\n",
    "# ---------------------------\n",
    "def load_e4e(ckpt_path, device='cuda'):\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        raise FileNotFoundError(f\"e4e checkpoint not found at: {ckpt_path}\")\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    if 'opts' not in ckpt:\n",
    "        # Some saved checkpoints may have different key names; attempt to handle common cases\n",
    "        if 'state_dict' in ckpt and 'opts' not in ckpt:\n",
    "            raise RuntimeError(\"Loaded checkpoint does not contain 'opts' required to build pSp. Make sure you use the e4e pretrained checkpoint from encoder4editing repository.\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Checkpoint format unexpected: missing 'opts'.\")\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts)\n",
    "    model.to(device)\n",
    "    # load weights\n",
    "    try:\n",
    "        state_dict = ckpt.get('state_dict', ckpt.get('model_state_dict', None))\n",
    "        if state_dict is None:\n",
    "            # some checkpoints store weights at top level\n",
    "            model.load_state_dict(ckpt, strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "    except Exception as e:\n",
    "        # load with partial strictness if shapes/names mismatch\n",
    "        model.load_state_dict(ckpt.get('state_dict', {}), strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "print(\"Loading e4e encoder from:\", e4e_ckpt)\n",
    "encoder = load_e4e(e4e_ckpt, device=device)\n",
    "print(\"‚úî e4e encoder loaded\")\n",
    "\n",
    "# ---------------------------\n",
    "# Inversion helper using e4e\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def invert_e4e(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: [1,3,H,W] float tensor in [0,1], already resized to img_res\n",
    "    returns: w (numpy) and reconstruction tensor [1,3,Hg,Wg] (generator output)\n",
    "    \"\"\"\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    # Some pSp/e4e models expect input normalized to [-1,1]. Confirm and convert if required.\n",
    "    # Many encoder4editing implementations internally normalize; if not, uncomment the next line:\n",
    "    # x_in = img_tensor * 2 - 1\n",
    "    x_in = img_tensor  # keep as [0,1] ‚Äî the pSp in encoder4editing repo usually handles preprocessing\n",
    "\n",
    "    outputs = encoder(x_in, return_latents=True)\n",
    "    # encoder may return (recon_images, w) or dict-like; handle common patterns:\n",
    "    if isinstance(outputs, (tuple, list)):\n",
    "        # common: (reconstructed_img, w)\n",
    "        if len(outputs) == 2:\n",
    "            recon_from_enc, w = outputs\n",
    "        else:\n",
    "            # fallback: try last element as w\n",
    "            w = outputs[-1]\n",
    "            recon_from_enc = None\n",
    "    elif isinstance(outputs, dict):\n",
    "        w = outputs.get('w', outputs.get('latent', None))\n",
    "        recon_from_enc = outputs.get('image', None)\n",
    "    else:\n",
    "        # unknown API: assume encoder returns w directly\n",
    "        w = outputs\n",
    "        recon_from_enc = None\n",
    "\n",
    "    # ensure w shape: [1, num_ws, w_dim] or [1, w_dim]\n",
    "    if isinstance(w, torch.Tensor) and w.ndim == 2:\n",
    "        # shape [1, w_dim] -> expand to W+ if needed\n",
    "        w = w.unsqueeze(1).repeat(1, G.num_ws, 1)\n",
    "\n",
    "    # generate with StyleGAN\n",
    "    with torch.no_grad():\n",
    "        synth = G.synthesis(w, noise_mode='const')\n",
    "        synth = (synth + 1) / 2  # to [0,1]\n",
    "\n",
    "    return w.cpu().numpy(), synth.cpu()\n",
    "\n",
    "# ---------------------------\n",
    "# Process video frames\n",
    "# ---------------------------\n",
    "def process_video(folder, save_npy, save_recon):\n",
    "    print(\"\\n==============================\")\n",
    "    print(\" Processing:\", folder)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    frame_paths = sorted(glob(os.path.join(folder, \"**\", \"*.*\"), recursive=True))\n",
    "    frame_paths = [p for p in frame_paths if p.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    print(\"Found\", len(frame_paths), \"frames\")\n",
    "\n",
    "    if len(frame_paths) == 0:\n",
    "        raise RuntimeError(\"No frames found in folder: \" + folder)\n",
    "\n",
    "    os.makedirs(save_recon, exist_ok=True)\n",
    "    all_latents = []\n",
    "\n",
    "    for p in tqdm(frame_paths, desc=\"Inverting\"):\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to open:\", p, \"|\", e)\n",
    "            continue\n",
    "\n",
    "        img_t = transform(img).unsqueeze(0)  # [1,3,H,W]\n",
    "        w, recon = invert_e4e(img_t)         # w: numpy, recon: tensor [1,3,Hg,Wg]\n",
    "\n",
    "        all_latents.append(w[0])              # store single example\n",
    "\n",
    "        # Save reconstruction as image\n",
    "        save_path = os.path.join(save_recon, os.path.basename(p))\n",
    "        utils.save_image(recon[0], save_path)\n",
    "\n",
    "    all_latents = np.array(all_latents)  # shape (N, num_ws, w_dim) or (N, w_dim)\n",
    "    np.save(save_npy, all_latents)\n",
    "    print(\"Saved latents to:\", save_npy)\n",
    "\n",
    "    # temporal variance\n",
    "    latent_variance = np.var(all_latents, axis=0)\n",
    "    mean_variance = float(np.mean(latent_variance))\n",
    "    print(\"Mean temporal variance:\", mean_variance)\n",
    "    return mean_variance\n",
    "\n",
    "# ---------------------------\n",
    "# Run for real and fake\n",
    "# ---------------------------\n",
    "var_real = process_video(path_real, \"video_latents_real.npy\", recon_real)\n",
    "var_fake = process_video(path_fake, \"video_latents_fake.npy\", recon_fake)\n",
    "\n",
    "print(\"\\n===== FINAL REPORT =====\")\n",
    "print(\"Real variance :\", var_real)\n",
    "print(\"Fake variance :\", var_fake)\n",
    "if var_fake < var_real * 0.7:\n",
    "    print(\"=> GAN detected (fake).\")\n",
    "else:\n",
    "    print(\"=> Looks natural.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6930ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_16308\\2697679075.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting:   0%|          | 0/198 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [1:39:21<00:00, 30.11s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean temporal variance: 0.14745275676250458\n",
      "\n",
      "Processing: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inverting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 994/994 [7:42:49<00:00, 27.94s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean temporal variance: 0.14742860198020935\n",
      "\n",
      "===== FINAL REPORT =====\n",
      "Real variance : 0.14745275676250458\n",
      "Fake variance : 0.14742860198020935\n",
      "=> Looks natural.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# ---------------------------\n",
    "# Paths\n",
    "# ---------------------------\n",
    "stylegan_repo = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ada-pytorch\"\n",
    "encoder_repo = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\"\n",
    "e4e_ckpt = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\"\n",
    "stylegan_ckpt = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\stylegan2-ffhq-config-f.pkl\"\n",
    "\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "recon_real = \"./recon_real\"\n",
    "recon_fake = \"./recon_fake\"\n",
    "os.makedirs(recon_real, exist_ok=True)\n",
    "os.makedirs(recon_fake, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Add repos to PYTHONPATH  (FIXED)\n",
    "# ---------------------------\n",
    "# IMPORTANT : StyleGAN MUST come FIRST to avoid import conflict\n",
    "sys.path.insert(0, stylegan_repo)\n",
    "\n",
    "# encoder4editing goes AFTER to prevent overriding 'training'\n",
    "sys.path.append(encoder_repo)\n",
    "\n",
    "import dnnlib\n",
    "import legacy\n",
    "from encoder4editing.models.psp import pSp\n",
    "from argparse import Namespace\n",
    "\n",
    "# ---------------------------\n",
    "# Device & transforms\n",
    "# ---------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "img_res = 256\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_res, img_res)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Load StyleGAN2\n",
    "# ---------------------------\n",
    "with open(stylegan_ckpt, \"rb\") as f:\n",
    "    G = legacy.load_network_pkl(f)[\"G_ema\"].to(device)\n",
    "G.eval()\n",
    "\n",
    "# ---------------------------\n",
    "# Load e4e encoder\n",
    "# ---------------------------\n",
    "def load_e4e(ckpt_path, device='cuda'):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    opts = Namespace(**ckpt['opts'])\n",
    "    opts.checkpoint_path = ckpt_path\n",
    "    model = pSp(opts).to(device)\n",
    "    state_dict = ckpt.get('state_dict', ckpt.get('model_state_dict', None))\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "encoder = load_e4e(e4e_ckpt, device=device)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper: recadrage central\n",
    "# ---------------------------\n",
    "def crop_center(img, size=256):\n",
    "    w, h = img.size\n",
    "    left = max((w - size) // 2, 0)\n",
    "    top = max((h - size) // 2, 0)\n",
    "    right = min(left + size, w)\n",
    "    bottom = min(top + size, h)\n",
    "    return img.crop((left, top, right, bottom))\n",
    "\n",
    "# ---------------------------\n",
    "# Inversion e4e\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def invert_e4e(img_tensor):\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    outputs = encoder(img_tensor, return_latents=True)\n",
    "\n",
    "    if isinstance(outputs, (tuple, list)) and len(outputs) == 2:\n",
    "        recon_from_enc, w = outputs\n",
    "    elif isinstance(outputs, dict):\n",
    "        w = outputs.get('w', outputs.get('latent'))\n",
    "        recon_from_enc = outputs.get('image', None)\n",
    "    else:\n",
    "        w = outputs\n",
    "        recon_from_enc = None\n",
    "\n",
    "    if w.ndim == 2:\n",
    "        w = w.unsqueeze(1).repeat(1, G.num_ws, 1)\n",
    "\n",
    "    synth = G.synthesis(w, noise_mode='const')\n",
    "    synth = (synth + 1) / 2\n",
    "\n",
    "    return w.cpu().float().numpy(), synth.cpu()\n",
    "\n",
    "# ---------------------------\n",
    "# Traitement des frames par batch\n",
    "# ---------------------------\n",
    "def process_video(folder, save_npy, save_recon, batch_size=4):\n",
    "    print(\"\\nProcessing:\", folder)\n",
    "    frame_paths = sorted(glob(os.path.join(folder, \"**\", \"*.*\"), recursive=True))\n",
    "    frame_paths = [p for p in frame_paths if p.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    os.makedirs(save_recon, exist_ok=True)\n",
    "\n",
    "    all_latents = []\n",
    "\n",
    "    for i in tqdm(range(0, len(frame_paths), batch_size), desc=\"Inverting\"):\n",
    "        batch_paths = frame_paths[i:i+batch_size]\n",
    "        batch_imgs = []\n",
    "\n",
    "        for p in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                img = crop_center(img, size=img_res)\n",
    "                batch_imgs.append(transform(img))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if len(batch_imgs) == 0:\n",
    "            continue\n",
    "\n",
    "        batch_tensor = torch.stack(batch_imgs, dim=0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = encoder(batch_tensor, return_latents=True)\n",
    "            if isinstance(outputs, (tuple, list)) and len(outputs) == 2:\n",
    "                recon_batch, w_batch = outputs\n",
    "            elif isinstance(outputs, dict):\n",
    "                w_batch = outputs.get('w', outputs.get('latent'))\n",
    "                recon_batch = outputs.get('image', None)\n",
    "            else:\n",
    "                w_batch = outputs\n",
    "                recon_batch = None\n",
    "\n",
    "            if w_batch.ndim == 2:\n",
    "                w_batch = w_batch.unsqueeze(1).repeat(1, G.num_ws, 1)\n",
    "\n",
    "            synth_batch = G.synthesis(w_batch, noise_mode='const')\n",
    "            synth_batch = (synth_batch + 1) / 2\n",
    "\n",
    "        for idx, p in enumerate(batch_paths):\n",
    "            latent = w_batch[idx].cpu().numpy()\n",
    "            if np.isnan(latent).any() or np.isinf(latent).any():\n",
    "                continue\n",
    "            all_latents.append(latent)\n",
    "            utils.save_image(synth_batch[idx].cpu(), os.path.join(save_recon, os.path.basename(p)))\n",
    "\n",
    "    if len(all_latents) == 0:\n",
    "        print(\"No valid latents found in folder:\", folder)\n",
    "        return float('nan')\n",
    "\n",
    "    all_latents = np.array(all_latents)\n",
    "    np.save(save_npy, all_latents)\n",
    "\n",
    "    latent_variance = np.var(all_latents, axis=0)\n",
    "    mean_variance = float(np.mean(latent_variance))\n",
    "    print(\"Mean temporal variance:\", mean_variance)\n",
    "    return mean_variance\n",
    "\n",
    "# ---------------------------\n",
    "# Ex√©cution\n",
    "# ---------------------------\n",
    "var_real = process_video(path_real, \"video_latents_real.npy\", recon_real, batch_size=4)\n",
    "var_fake = process_video(path_fake, \"video_latents_fake.npy\", recon_fake, batch_size=4)\n",
    "\n",
    "print(\"\\n===== FINAL REPORT =====\")\n",
    "print(\"Real variance :\", var_real)\n",
    "print(\"Fake variance :\", var_fake)\n",
    "if not np.isnan(var_real) and not np.isnan(var_fake):\n",
    "    if var_fake < var_real * 0.7:\n",
    "        print(\"=> GAN detected (fake).\")\n",
    "    else:\n",
    "        print(\"=> Looks natural.\")\n",
    "else:\n",
    "    print(\"=> Unable to determine: NaN values detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878c4493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from facenet-pytorch) (1.26.4)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from facenet-pytorch) (2.32.5)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from facenet-pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from facenet-pytorch) (0.17.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from facenet-pytorch) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.10.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.14.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from tqdm<5.0.0,>=4.0.0->facenet-pytorch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2746f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\pip-uninstall-1m713zf8'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "facenet-pytorch 2.6.0 requires numpy<2.0.0,>=1.24.0, but you have numpy 2.0.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scipy) (2.0.2)\n",
      "Requirement already satisfied: lpips in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.14.3 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (1.13.1)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (4.14.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch>=0.4.0->lpips) (2025.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision>=0.2.1->lpips) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from tqdm>=4.28.1->lpips) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy\n",
    "!pip install --upgrade scipy\n",
    "!pip install lpips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef47dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.0.2\n",
      "Uninstalling numpy-2.0.2:\n",
      "  Successfully uninstalled numpy-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\pip-uninstall-0x7fpt3j'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\Lib\\site-packages\\~~mpy.libs'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04479077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c452ffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_9820\\905531602.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4e loaded successfully.\n",
      "Using encoder: e4e\n",
      "Found REAL: 790\n",
      "Found FAKE: 3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding REAL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [01:21<00:00,  9.70it/s]\n",
      "Encoding FAKE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3975/3975 [06:59<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (4765, 9216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy des mod√®les :\n",
      " - svm: 0.8395\n",
      " - rf: 0.8353\n",
      " - mlp: 0.8468\n",
      " - xgb: 0.8321\n",
      "\n",
      "Pipeline termin√© ! R√©sultats dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_pipeline\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# NOTEBOOK UNIQUE : Deepfake detection pipeline\n",
    "# ==========================================================\n",
    "# Requirements:\n",
    "# pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# sklearn & others\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Tes dossiers contenant des frames D√âJ√Ä extraites\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "# checkpoint e4e si disponible\n",
    "ckpt_e4e = os.path.join('encoder4editing','pretrained_models','e4e_ffhq_encode.pt')\n",
    "\n",
    "# dossier de sortie\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_pipeline\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True     # True = essayer e4e, sinon fallback ResNet50\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CHARGEMENT DU MODELE : e4e OU RESNET50\n",
    "# ----------------------------------------------------------\n",
    "e4e_model = None\n",
    "resnet_model = None\n",
    "\n",
    "transform_resnet = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# tentative e4e\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from encoder4editing.models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:,:,::-1]\n",
    "            tf = T.Compose([T.ToPILImage(), T.Resize((256,256)), T.ToTensor()])\n",
    "            t = tf(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "                return out.cpu().numpy().reshape(-1)\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:,:,::-1]\n",
    "        t = transform_resnet(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            return model(t).cpu().numpy().reshape(-1)\n",
    "    return encode\n",
    "\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    e4e_model = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "encoder_fn = e4e_model if e4e_model is not None else load_resnet_fallback()\n",
    "print(\"Using encoder:\", \"e4e\" if e4e_model else \"ResNet50\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) LECTURE DES IMAGES (supporte les sous-dossiers)\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                all_files.append(os.path.join(root, f))\n",
    "    return all_files\n",
    "\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "\n",
    "print(\"Found REAL:\", len(imgs_real))\n",
    "print(\"Found FAKE:\", len(imgs_fake))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) EXTRACTION DES FEATURES / LATENTS\n",
    "# ----------------------------------------------------------\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Unable to read image \" + p)\n",
    "    return img\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for p in tqdm(imgs_real, desc=\"Encoding REAL\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(0)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "for p in tqdm(imgs_fake, desc=\"Encoding FAKE\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "print(\"Latents shape:\", X.shape)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir, \"labels.npy\"), y)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) ANALYSE DE L‚ÄôESPACE LATENT\n",
    "# ----------------------------------------------------------\n",
    "results_summary = {}\n",
    "\n",
    "# PCA\n",
    "if X.shape[0] >= 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    plt.scatter(X_pca[:,0], X_pca[:,1], c=y)\n",
    "    plt.title(\"PCA 2D\")\n",
    "    plt.savefig(os.path.join(output_dir, \"pca.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# t-SNE\n",
    "if X.shape[0] >= 5:\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.savefig(os.path.join(output_dir, \"tsne.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# UMAP\n",
    "if X.shape[0] >= 5:\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "    plt.scatter(X_umap[:,0], X_umap[:,1], c=y)\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.savefig(os.path.join(output_dir, \"umap.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# distances intra/inter\n",
    "if len(np.unique(y)) == 2:\n",
    "    X_real = X[y==0]\n",
    "    X_fake = X[y==1]\n",
    "\n",
    "    dist_real = np.mean(cdist(X_real, X_real)) if len(X_real)>=2 else None\n",
    "    dist_fake = np.mean(cdist(X_fake, X_fake)) if len(X_fake)>=2 else None\n",
    "    dist_inter = np.mean(cdist(X_real, X_fake)) if len(X_real)>=1 and len(X_fake)>=1 else None\n",
    "\n",
    "    results_summary.update({\n",
    "        'intra_real': dist_real,\n",
    "        'intra_fake': dist_fake,\n",
    "        'inter': dist_inter\n",
    "    })\n",
    "\n",
    "with open(os.path.join(output_dir, \"latent_analysis.json\"), \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) MACHINE LEARNING\n",
    "# ----------------------------------------------------------\n",
    "if X.shape[0] < 10:\n",
    "    print(\"Pas assez d‚Äô√©chantillons pour ML !\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    models_results = {}\n",
    "\n",
    "    # SVM\n",
    "    svm = SVC(kernel='rbf')\n",
    "    svm.fit(X_train, y_train)\n",
    "    pred = svm.predict(X_test)\n",
    "    models_results[\"svm\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(svm, os.path.join(output_dir, \"model_svm.joblib\"))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred = rf.predict(X_test)\n",
    "    models_results[\"rf\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(rf, os.path.join(output_dir, \"model_rf.joblib\"))\n",
    "\n",
    "    # MLP\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(512,256), max_iter=500)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    pred = mlp.predict(X_test)\n",
    "    models_results[\"mlp\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(mlp, os.path.join(output_dir, \"model_mlp.joblib\"))\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    pred = xgb.predict(X_test)\n",
    "    models_results[\"xgb\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(xgb, os.path.join(output_dir, \"model_xgb.joblib\"))\n",
    "\n",
    "    joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "\n",
    "    with open(os.path.join(output_dir, \"results_models.json\"), \"w\") as f:\n",
    "        json.dump(models_results, f, indent=2)\n",
    "\n",
    "    print(\"\\nAccuracy des mod√®les :\")\n",
    "    for m,a in models_results.items():\n",
    "        print(f\" - {m}: {a:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# FIN\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nPipeline termin√© ! R√©sultats dans :\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "527fdc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Using cached albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from albumentations) (1.13.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from albumentations) (4.14.1)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Using cached albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting eval-type-backport (from albumentations)\n",
      "  Using cached eval_type_backport-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Using cached opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Using cached stringzilla-4.4.0.tar.gz (549 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Using cached simsimd-6.5.3-cp39-cp39-win_amd64.whl.metadata (71 kB)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached pydantic_core-2.41.5-cp39-cp39-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Using cached albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Using cached albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-win_amd64.whl (38.9 MB)\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/38.9 MB 212.3 kB/s eta 0:03:01\n",
      "    --------------------------------------- 0.5/38.9 MB 212.3 kB/s eta 0:03:01\n",
      "    --------------------------------------- 0.5/38.9 MB 212.3 kB/s eta 0:03:01\n",
      "    --------------------------------------- 0.5/38.9 MB 212.3 kB/s eta 0:03:01\n",
      "    --------------------------------------- 0.5/38.9 MB 212.3 kB/s eta 0:03:01\n",
      "    --------------------------------------- 0.5/38.9 MB 212.3 kB/s eta 0:03:01\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "    --------------------------------------- 0.8/38.9 MB 205.8 kB/s eta 0:03:06\n",
      "   - -------------------------------------- 1.0/38.9 MB 136.8 kB/s eta 0:04:37\n",
      "   - -------------------------------------- 1.0/38.9 MB 136.8 kB/s eta 0:04:37\n",
      "   - -------------------------------------- 1.0/38.9 MB 136.8 kB/s eta 0:04:37\n",
      "   - -------------------------------------- 1.3/38.9 MB 160.2 kB/s eta 0:03:55\n",
      "   - -------------------------------------- 1.3/38.9 MB 160.2 kB/s eta 0:03:55\n",
      "   - -------------------------------------- 1.3/38.9 MB 160.2 kB/s eta 0:03:55\n",
      "   - -------------------------------------- 1.3/38.9 MB 160.2 kB/s eta 0:03:55\n",
      "   - -------------------------------------- 1.3/38.9 MB 160.2 kB/s eta 0:03:55\n",
      "   - -------------------------------------- 1.3/38.9 MB 160.2 kB/s eta 0:03:55\n",
      "   - -------------------------------------- 1.6/38.9 MB 169.5 kB/s eta 0:03:41\n",
      "   - -------------------------------------- 1.6/38.9 MB 169.5 kB/s eta 0:03:41\n",
      "   - -------------------------------------- 1.6/38.9 MB 169.5 kB/s eta 0:03:41\n",
      "   - -------------------------------------- 1.8/38.9 MB 189.9 kB/s eta 0:03:16\n",
      "   - -------------------------------------- 1.8/38.9 MB 189.9 kB/s eta 0:03:16\n",
      "   - -------------------------------------- 1.8/38.9 MB 189.9 kB/s eta 0:03:16\n",
      "   -- ------------------------------------- 2.1/38.9 MB 204.2 kB/s eta 0:03:01\n",
      "   -- ------------------------------------- 2.1/38.9 MB 204.2 kB/s eta 0:03:01\n",
      "   -- ------------------------------------- 2.4/38.9 MB 221.8 kB/s eta 0:02:45\n",
      "   -- ------------------------------------- 2.4/38.9 MB 221.8 kB/s eta 0:02:45\n",
      "   -- ------------------------------------- 2.4/38.9 MB 221.8 kB/s eta 0:02:45\n",
      "   -- ------------------------------------- 2.6/38.9 MB 236.7 kB/s eta 0:02:34\n",
      "   -- ------------------------------------- 2.9/38.9 MB 253.4 kB/s eta 0:02:23\n",
      "   -- ------------------------------------- 2.9/38.9 MB 253.4 kB/s eta 0:02:23\n",
      "   --- ------------------------------------ 3.1/38.9 MB 268.6 kB/s eta 0:02:14\n",
      "   --- ------------------------------------ 3.1/38.9 MB 268.6 kB/s eta 0:02:14\n",
      "   --- ------------------------------------ 3.4/38.9 MB 282.4 kB/s eta 0:02:06\n",
      "   --- ------------------------------------ 3.4/38.9 MB 282.4 kB/s eta 0:02:06\n",
      "   --- ------------------------------------ 3.7/38.9 MB 293.1 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 3.7/38.9 MB 293.1 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 3.7/38.9 MB 293.1 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 3.7/38.9 MB 293.1 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 3.7/38.9 MB 293.1 kB/s eta 0:02:01\n",
      "   --- ------------------------------------ 3.7/38.9 MB 293.1 kB/s eta 0:02:01\n",
      "   ---- ----------------------------------- 3.9/38.9 MB 287.1 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 3.9/38.9 MB 287.1 kB/s eta 0:02:02\n",
      "   ---- ----------------------------------- 4.2/38.9 MB 297.5 kB/s eta 0:01:57\n",
      "   ---- ----------------------------------- 4.2/38.9 MB 297.5 kB/s eta 0:01:57\n",
      "   ---- ----------------------------------- 4.5/38.9 MB 307.8 kB/s eta 0:01:52\n",
      "   ---- ----------------------------------- 4.5/38.9 MB 307.8 kB/s eta 0:01:52\n",
      "   ---- ----------------------------------- 4.7/38.9 MB 315.1 kB/s eta 0:01:49\n",
      "   ---- ----------------------------------- 4.7/38.9 MB 315.1 kB/s eta 0:01:49\n",
      "   ---- ----------------------------------- 4.7/38.9 MB 315.1 kB/s eta 0:01:49\n",
      "   ----- ---------------------------------- 5.0/38.9 MB 320.6 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 5.0/38.9 MB 320.6 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 5.0/38.9 MB 320.6 kB/s eta 0:01:46\n",
      "   ----- ---------------------------------- 5.2/38.9 MB 324.3 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 5.2/38.9 MB 324.3 kB/s eta 0:01:44\n",
      "   ----- ---------------------------------- 5.5/38.9 MB 330.3 kB/s eta 0:01:42\n",
      "   ----- ---------------------------------- 5.5/38.9 MB 330.3 kB/s eta 0:01:42\n",
      "   ----- ---------------------------------- 5.5/38.9 MB 330.3 kB/s eta 0:01:42\n",
      "   ----- ---------------------------------- 5.8/38.9 MB 336.2 kB/s eta 0:01:39\n",
      "   ----- ---------------------------------- 5.8/38.9 MB 336.2 kB/s eta 0:01:39\n",
      "   ------ --------------------------------- 6.0/38.9 MB 340.8 kB/s eta 0:01:37\n",
      "   ------ --------------------------------- 6.0/38.9 MB 340.8 kB/s eta 0:01:37\n",
      "   ------ --------------------------------- 6.0/38.9 MB 340.8 kB/s eta 0:01:37\n",
      "   ------ --------------------------------- 6.3/38.9 MB 346.4 kB/s eta 0:01:35\n",
      "   ------ --------------------------------- 6.3/38.9 MB 346.4 kB/s eta 0:01:35\n",
      "   ------ --------------------------------- 6.6/38.9 MB 351.7 kB/s eta 0:01:33\n",
      "   ------ --------------------------------- 6.6/38.9 MB 351.7 kB/s eta 0:01:33\n",
      "   ------- -------------------------------- 6.8/38.9 MB 356.7 kB/s eta 0:01:31\n",
      "   ------- -------------------------------- 6.8/38.9 MB 356.7 kB/s eta 0:01:31\n",
      "   ------- -------------------------------- 6.8/38.9 MB 356.7 kB/s eta 0:01:31\n",
      "   ------- -------------------------------- 7.1/38.9 MB 361.1 kB/s eta 0:01:29\n",
      "   ------- -------------------------------- 7.1/38.9 MB 361.1 kB/s eta 0:01:29\n",
      "   ------- -------------------------------- 7.1/38.9 MB 361.1 kB/s eta 0:01:29\n",
      "   ------- -------------------------------- 7.3/38.9 MB 360.9 kB/s eta 0:01:28\n",
      "   ------- -------------------------------- 7.3/38.9 MB 360.9 kB/s eta 0:01:28\n",
      "   ------- -------------------------------- 7.3/38.9 MB 360.9 kB/s eta 0:01:28\n",
      "   ------- -------------------------------- 7.3/38.9 MB 360.9 kB/s eta 0:01:28\n",
      "   ------- -------------------------------- 7.6/38.9 MB 360.5 kB/s eta 0:01:27\n",
      "   ------- -------------------------------- 7.6/38.9 MB 360.5 kB/s eta 0:01:27\n",
      "   ------- -------------------------------- 7.6/38.9 MB 360.5 kB/s eta 0:01:27\n",
      "   -------- ------------------------------- 7.9/38.9 MB 363.4 kB/s eta 0:01:26\n",
      "   -------- ------------------------------- 7.9/38.9 MB 363.4 kB/s eta 0:01:26\n",
      "   -------- ------------------------------- 7.9/38.9 MB 363.4 kB/s eta 0:01:26\n",
      "   -------- ------------------------------- 8.1/38.9 MB 364.2 kB/s eta 0:01:25\n",
      "   -------- ------------------------------- 8.1/38.9 MB 364.2 kB/s eta 0:01:25\n",
      "   -------- ------------------------------- 8.4/38.9 MB 366.8 kB/s eta 0:01:24\n",
      "   -------- ------------------------------- 8.4/38.9 MB 366.8 kB/s eta 0:01:24\n",
      "   -------- ------------------------------- 8.4/38.9 MB 366.8 kB/s eta 0:01:24\n",
      "   -------- ------------------------------- 8.7/38.9 MB 368.0 kB/s eta 0:01:23\n",
      "   -------- ------------------------------- 8.7/38.9 MB 368.0 kB/s eta 0:01:23\n",
      "   -------- ------------------------------- 8.7/38.9 MB 368.0 kB/s eta 0:01:23\n",
      "   -------- ------------------------------- 8.7/38.9 MB 368.0 kB/s eta 0:01:23\n",
      "   --------- ------------------------------ 8.9/38.9 MB 365.9 kB/s eta 0:01:23\n",
      "   --------- ------------------------------ 8.9/38.9 MB 365.9 kB/s eta 0:01:23\n",
      "   --------- ------------------------------ 9.2/38.9 MB 370.6 kB/s eta 0:01:21\n",
      "   --------- ------------------------------ 9.2/38.9 MB 370.6 kB/s eta 0:01:21\n",
      "   --------- ------------------------------ 9.2/38.9 MB 370.6 kB/s eta 0:01:21\n",
      "   --------- ------------------------------ 9.2/38.9 MB 370.6 kB/s eta 0:01:21\n",
      "   --------- ------------------------------ 9.4/38.9 MB 368.8 kB/s eta 0:01:20\n",
      "   --------- ------------------------------ 9.4/38.9 MB 368.8 kB/s eta 0:01:20\n",
      "   --------- ------------------------------ 9.7/38.9 MB 374.7 kB/s eta 0:01:18\n",
      "   --------- ------------------------------ 9.7/38.9 MB 374.7 kB/s eta 0:01:18\n",
      "   ---------- ----------------------------- 10.0/38.9 MB 379.0 kB/s eta 0:01:17\n",
      "   ---------- ----------------------------- 10.0/38.9 MB 379.0 kB/s eta 0:01:17\n",
      "   ---------- ----------------------------- 10.2/38.9 MB 380.8 kB/s eta 0:01:16\n",
      "   ---------- ----------------------------- 10.2/38.9 MB 380.8 kB/s eta 0:01:16\n",
      "   ---------- ----------------------------- 10.2/38.9 MB 380.8 kB/s eta 0:01:16\n",
      "   ---------- ----------------------------- 10.5/38.9 MB 383.3 kB/s eta 0:01:15\n",
      "   ---------- ----------------------------- 10.5/38.9 MB 383.3 kB/s eta 0:01:15\n",
      "   ----------- ---------------------------- 10.7/38.9 MB 385.2 kB/s eta 0:01:14\n",
      "   ----------- ---------------------------- 10.7/38.9 MB 385.2 kB/s eta 0:01:14\n",
      "   ----------- ---------------------------- 11.0/38.9 MB 388.8 kB/s eta 0:01:12\n",
      "   ----------- ---------------------------- 11.0/38.9 MB 388.8 kB/s eta 0:01:12\n",
      "   ----------- ---------------------------- 11.0/38.9 MB 388.8 kB/s eta 0:01:12\n",
      "   ----------- ---------------------------- 11.3/38.9 MB 390.6 kB/s eta 0:01:11\n",
      "   ----------- ---------------------------- 11.3/38.9 MB 390.6 kB/s eta 0:01:11\n",
      "   ----------- ---------------------------- 11.5/38.9 MB 392.5 kB/s eta 0:01:10\n",
      "   ----------- ---------------------------- 11.5/38.9 MB 392.5 kB/s eta 0:01:10\n",
      "   ----------- ---------------------------- 11.5/38.9 MB 392.5 kB/s eta 0:01:10\n",
      "   ------------ --------------------------- 11.8/38.9 MB 394.3 kB/s eta 0:01:09\n",
      "   ------------ --------------------------- 11.8/38.9 MB 394.3 kB/s eta 0:01:09\n",
      "   ------------ --------------------------- 11.8/38.9 MB 394.3 kB/s eta 0:01:09\n",
      "   ------------ --------------------------- 12.1/38.9 MB 395.5 kB/s eta 0:01:08\n",
      "   ------------ --------------------------- 12.1/38.9 MB 395.5 kB/s eta 0:01:08\n",
      "   ------------ --------------------------- 12.1/38.9 MB 395.5 kB/s eta 0:01:08\n",
      "   ------------ --------------------------- 12.3/38.9 MB 403.1 kB/s eta 0:01:06\n",
      "   ------------ --------------------------- 12.3/38.9 MB 403.1 kB/s eta 0:01:06\n",
      "   ------------ --------------------------- 12.3/38.9 MB 403.1 kB/s eta 0:01:06\n",
      "   ------------ --------------------------- 12.3/38.9 MB 403.1 kB/s eta 0:01:06\n",
      "   ------------ --------------------------- 12.6/38.9 MB 409.4 kB/s eta 0:01:05\n",
      "   ------------ --------------------------- 12.6/38.9 MB 409.4 kB/s eta 0:01:05\n",
      "   ------------- -------------------------- 12.8/38.9 MB 412.3 kB/s eta 0:01:04\n",
      "   ------------- -------------------------- 12.8/38.9 MB 412.3 kB/s eta 0:01:04\n",
      "   ------------- -------------------------- 12.8/38.9 MB 412.3 kB/s eta 0:01:04\n",
      "   ------------- -------------------------- 12.8/38.9 MB 412.3 kB/s eta 0:01:04\n",
      "   ------------- -------------------------- 12.8/38.9 MB 412.3 kB/s eta 0:01:04\n",
      "   ------------- -------------------------- 13.1/38.9 MB 445.8 kB/s eta 0:00:58\n",
      "   ------------- -------------------------- 13.1/38.9 MB 445.8 kB/s eta 0:00:58\n",
      "   ------------- -------------------------- 13.1/38.9 MB 445.8 kB/s eta 0:00:58\n",
      "   ------------- -------------------------- 13.1/38.9 MB 445.8 kB/s eta 0:00:58\n",
      "   ------------- -------------------------- 13.4/38.9 MB 440.3 kB/s eta 0:00:59\n",
      "   ------------- -------------------------- 13.4/38.9 MB 440.3 kB/s eta 0:00:59\n",
      "   ------------- -------------------------- 13.4/38.9 MB 440.3 kB/s eta 0:00:59\n",
      "   ------------- -------------------------- 13.4/38.9 MB 440.3 kB/s eta 0:00:59\n",
      "   -------------- ------------------------- 13.6/38.9 MB 437.9 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.6/38.9 MB 437.9 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.6/38.9 MB 437.9 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.6/38.9 MB 437.9 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.9/38.9 MB 432.7 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.9/38.9 MB 432.7 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.9/38.9 MB 432.7 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.9/38.9 MB 432.7 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.9/38.9 MB 432.7 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 13.9/38.9 MB 432.7 kB/s eta 0:00:58\n",
      "   -------------- ------------------------- 14.2/38.9 MB 436.3 kB/s eta 0:00:57\n",
      "   -------------- ------------------------- 14.2/38.9 MB 436.3 kB/s eta 0:00:57\n",
      "   -------------- ------------------------- 14.4/38.9 MB 437.5 kB/s eta 0:00:57\n",
      "   -------------- ------------------------- 14.4/38.9 MB 437.5 kB/s eta 0:00:57\n",
      "   -------------- ------------------------- 14.4/38.9 MB 437.5 kB/s eta 0:00:57\n",
      "   --------------- ------------------------ 14.7/38.9 MB 435.6 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 14.7/38.9 MB 435.6 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 14.7/38.9 MB 435.6 kB/s eta 0:00:56\n",
      "   --------------- ------------------------ 14.9/38.9 MB 438.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 14.9/38.9 MB 438.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 14.9/38.9 MB 438.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 14.9/38.9 MB 438.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.2/38.9 MB 432.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.2/38.9 MB 432.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.2/38.9 MB 432.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.2/38.9 MB 432.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.2/38.9 MB 432.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.2/38.9 MB 432.2 kB/s eta 0:00:55\n",
      "   --------------- ------------------------ 15.5/38.9 MB 413.7 kB/s eta 0:00:57\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 415.7 kB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 415.7 kB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 415.7 kB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 415.7 kB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 15.7/38.9 MB 415.7 kB/s eta 0:00:56\n",
      "   ---------------- ----------------------- 16.0/38.9 MB 417.2 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.0/38.9 MB 417.2 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.0/38.9 MB 417.2 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.0/38.9 MB 417.2 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.3/38.9 MB 412.6 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.3/38.9 MB 412.6 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.3/38.9 MB 412.6 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.3/38.9 MB 412.6 kB/s eta 0:00:55\n",
      "   ---------------- ----------------------- 16.3/38.9 MB 412.6 kB/s eta 0:00:55\n",
      "   ----------------- ---------------------- 16.8/38.9 MB 408.6 kB/s eta 0:00:55\n",
      "   ----------------- ---------------------- 16.8/38.9 MB 408.6 kB/s eta 0:00:55\n",
      "   ----------------- ---------------------- 16.8/38.9 MB 408.6 kB/s eta 0:00:55\n",
      "   ----------------- ---------------------- 16.8/38.9 MB 408.6 kB/s eta 0:00:55\n",
      "   ----------------- ---------------------- 17.0/38.9 MB 406.0 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.0/38.9 MB 406.0 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.0/38.9 MB 406.0 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ----------------- ---------------------- 17.3/38.9 MB 404.3 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 17.6/38.9 MB 389.6 kB/s eta 0:00:55\n",
      "   ------------------ --------------------- 17.8/38.9 MB 395.9 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 17.8/38.9 MB 395.9 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 17.8/38.9 MB 395.9 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 17.8/38.9 MB 395.9 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.1/38.9 MB 386.7 kB/s eta 0:00:54\n",
      "   ------------------ --------------------- 18.4/38.9 MB 369.5 kB/s eta 0:00:56\n",
      "   ------------------ --------------------- 18.4/38.9 MB 369.5 kB/s eta 0:00:56\n",
      "   ------------------ --------------------- 18.4/38.9 MB 369.5 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.6/38.9 MB 371.5 kB/s eta 0:00:55\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 18.9/38.9 MB 358.3 kB/s eta 0:00:56\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.1/38.9 MB 330.4 kB/s eta 0:01:00\n",
      "   ------------------- -------------------- 19.4/38.9 MB 311.3 kB/s eta 0:01:03\n",
      "   ------------------- -------------------- 19.4/38.9 MB 311.3 kB/s eta 0:01:03\n",
      "   ------------------- -------------------- 19.4/38.9 MB 311.3 kB/s eta 0:01:03\n",
      "   ------------------- -------------------- 19.4/38.9 MB 311.3 kB/s eta 0:01:03\n",
      "   ------------------- -------------------- 19.4/38.9 MB 311.3 kB/s eta 0:01:03\n",
      "   ------------------- -------------------- 19.4/38.9 MB 311.3 kB/s eta 0:01:03\n",
      "   -------------------- ------------------- 19.7/38.9 MB 299.8 kB/s eta 0:01:05\n",
      "   -------------------- ------------------- 19.7/38.9 MB 299.8 kB/s eta 0:01:05\n",
      "   -------------------- ------------------- 19.7/38.9 MB 299.8 kB/s eta 0:01:05\n",
      "   -------------------- ------------------- 19.9/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   -------------------- ------------------- 19.9/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   -------------------- ------------------- 19.9/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   -------------------- ------------------- 20.2/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   -------------------- ------------------- 20.2/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   -------------------- ------------------- 20.2/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   -------------------- ------------------- 20.2/38.9 MB 297.4 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 20.4/38.9 MB 290.9 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 20.4/38.9 MB 290.9 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 20.4/38.9 MB 290.9 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 20.4/38.9 MB 290.9 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 20.4/38.9 MB 290.9 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 20.7/38.9 MB 283.5 kB/s eta 0:01:05\n",
      "   --------------------- ------------------ 20.7/38.9 MB 283.5 kB/s eta 0:01:05\n",
      "   --------------------- ------------------ 20.7/38.9 MB 283.5 kB/s eta 0:01:05\n",
      "   --------------------- ------------------ 20.7/38.9 MB 283.5 kB/s eta 0:01:05\n",
      "   --------------------- ------------------ 20.7/38.9 MB 283.5 kB/s eta 0:01:05\n",
      "   --------------------- ------------------ 21.0/38.9 MB 281.8 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.0/38.9 MB 281.8 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.0/38.9 MB 281.8 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.0/38.9 MB 281.8 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.2/38.9 MB 278.0 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.2/38.9 MB 278.0 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.2/38.9 MB 278.0 kB/s eta 0:01:04\n",
      "   --------------------- ------------------ 21.2/38.9 MB 278.0 kB/s eta 0:01:04\n",
      "   ---------------------- ----------------- 21.5/38.9 MB 278.9 kB/s eta 0:01:03\n",
      "   ---------------------- ----------------- 21.5/38.9 MB 278.9 kB/s eta 0:01:03\n",
      "   ---------------------- ----------------- 21.8/38.9 MB 283.2 kB/s eta 0:01:01\n",
      "   ---------------------- ----------------- 21.8/38.9 MB 283.2 kB/s eta 0:01:01\n",
      "   ---------------------- ----------------- 22.0/38.9 MB 289.0 kB/s eta 0:00:59\n",
      "   ---------------------- ----------------- 22.3/38.9 MB 293.4 kB/s eta 0:00:57\n",
      "   ---------------------- ----------------- 22.3/38.9 MB 293.4 kB/s eta 0:00:57\n",
      "   ----------------------- ---------------- 22.5/38.9 MB 299.1 kB/s eta 0:00:55\n",
      "   ----------------------- ---------------- 22.8/38.9 MB 305.9 kB/s eta 0:00:53\n",
      "   ----------------------- ---------------- 22.8/38.9 MB 305.9 kB/s eta 0:00:53\n",
      "   ----------------------- ---------------- 23.1/38.9 MB 310.2 kB/s eta 0:00:52\n",
      "   ----------------------- ---------------- 23.1/38.9 MB 310.2 kB/s eta 0:00:52\n",
      "   ----------------------- ---------------- 23.3/38.9 MB 314.7 kB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 23.3/38.9 MB 314.7 kB/s eta 0:00:50\n",
      "   ------------------------ --------------- 23.6/38.9 MB 322.8 kB/s eta 0:00:48\n",
      "   ------------------------ --------------- 23.6/38.9 MB 322.8 kB/s eta 0:00:48\n",
      "   ------------------------ --------------- 23.9/38.9 MB 326.0 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 23.9/38.9 MB 326.0 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 23.9/38.9 MB 326.0 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 23.9/38.9 MB 326.0 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 23.9/38.9 MB 326.0 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 23.9/38.9 MB 326.0 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 24.1/38.9 MB 317.2 kB/s eta 0:00:47\n",
      "   ------------------------ --------------- 24.1/38.9 MB 317.2 kB/s eta 0:00:47\n",
      "   ------------------------- -------------- 24.4/38.9 MB 318.1 kB/s eta 0:00:46\n",
      "   ------------------------- -------------- 24.4/38.9 MB 318.1 kB/s eta 0:00:46\n",
      "   ------------------------- -------------- 24.4/38.9 MB 318.1 kB/s eta 0:00:46\n",
      "   ------------------------- -------------- 24.4/38.9 MB 318.1 kB/s eta 0:00:46\n",
      "   ------------------------- -------------- 24.6/38.9 MB 319.1 kB/s eta 0:00:45\n",
      "   ------------------------- -------------- 24.6/38.9 MB 319.1 kB/s eta 0:00:45\n",
      "   ------------------------- -------------- 24.9/38.9 MB 323.6 kB/s eta 0:00:44\n",
      "   ------------------------- -------------- 24.9/38.9 MB 323.6 kB/s eta 0:00:44\n",
      "   ------------------------- -------------- 25.2/38.9 MB 332.3 kB/s eta 0:00:42\n",
      "   ------------------------- -------------- 25.2/38.9 MB 332.3 kB/s eta 0:00:42\n",
      "   -------------------------- ------------- 25.4/38.9 MB 336.8 kB/s eta 0:00:41\n",
      "   -------------------------- ------------- 25.4/38.9 MB 336.8 kB/s eta 0:00:41\n",
      "   -------------------------- ------------- 25.4/38.9 MB 336.8 kB/s eta 0:00:41\n",
      "   -------------------------- ------------- 25.7/38.9 MB 332.6 kB/s eta 0:00:40\n",
      "   -------------------------- ------------- 25.7/38.9 MB 332.6 kB/s eta 0:00:40\n",
      "   -------------------------- ------------- 26.0/38.9 MB 338.6 kB/s eta 0:00:39\n",
      "   -------------------------- ------------- 26.0/38.9 MB 338.6 kB/s eta 0:00:39\n",
      "   -------------------------- ------------- 26.2/38.9 MB 343.3 kB/s eta 0:00:38\n",
      "   -------------------------- ------------- 26.2/38.9 MB 343.3 kB/s eta 0:00:38\n",
      "   --------------------------- ------------ 26.5/38.9 MB 349.2 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 26.5/38.9 MB 349.2 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 26.5/38.9 MB 349.2 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 26.7/38.9 MB 349.9 kB/s eta 0:00:35\n",
      "   --------------------------- ------------ 27.0/38.9 MB 337.3 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 27.0/38.9 MB 337.3 kB/s eta 0:00:36\n",
      "   --------------------------- ------------ 27.0/38.9 MB 337.3 kB/s eta 0:00:36\n",
      "   ---------------------------- ----------- 27.3/38.9 MB 338.8 kB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 27.3/38.9 MB 338.8 kB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 27.3/38.9 MB 338.8 kB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 27.5/38.9 MB 341.0 kB/s eta 0:00:34\n",
      "   ---------------------------- ----------- 27.5/38.9 MB 341.0 kB/s eta 0:00:34\n",
      "   ---------------------------- ----------- 27.8/38.9 MB 350.1 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 27.8/38.9 MB 350.1 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 27.8/38.9 MB 350.1 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 27.8/38.9 MB 350.1 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 27.8/38.9 MB 350.1 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 28.0/38.9 MB 342.9 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 28.0/38.9 MB 342.9 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 28.0/38.9 MB 342.9 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 28.0/38.9 MB 342.9 kB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 28.3/38.9 MB 342.0 kB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 28.3/38.9 MB 342.0 kB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 28.6/38.9 MB 356.2 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 28.6/38.9 MB 356.2 kB/s eta 0:00:30\n",
      "   ----------------------------- ---------- 28.8/38.9 MB 359.8 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 28.8/38.9 MB 359.8 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 28.8/38.9 MB 359.8 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 28.8/38.9 MB 359.8 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 28.8/38.9 MB 359.8 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 28.8/38.9 MB 359.8 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 29.1/38.9 MB 352.8 kB/s eta 0:00:28\n",
      "   ----------------------------- ---------- 29.1/38.9 MB 352.8 kB/s eta 0:00:28\n",
      "   ----------------------------- ---------- 29.1/38.9 MB 352.8 kB/s eta 0:00:28\n",
      "   ------------------------------ --------- 29.4/38.9 MB 364.1 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 29.4/38.9 MB 364.1 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 29.6/38.9 MB 367.1 kB/s eta 0:00:26\n",
      "   ------------------------------ --------- 29.6/38.9 MB 367.1 kB/s eta 0:00:26\n",
      "   ------------------------------ --------- 29.6/38.9 MB 367.1 kB/s eta 0:00:26\n",
      "   ------------------------------ --------- 29.9/38.9 MB 369.9 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 29.9/38.9 MB 369.9 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 29.9/38.9 MB 369.9 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 29.9/38.9 MB 369.9 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 29.9/38.9 MB 369.9 kB/s eta 0:00:25\n",
      "   ------------------------------ --------- 30.1/38.9 MB 389.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 30.1/38.9 MB 389.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 30.1/38.9 MB 389.3 kB/s eta 0:00:23\n",
      "   ------------------------------- -------- 30.4/38.9 MB 391.2 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 30.4/38.9 MB 391.2 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 30.4/38.9 MB 391.2 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 30.7/38.9 MB 392.5 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 30.7/38.9 MB 392.5 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 30.9/38.9 MB 395.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 30.9/38.9 MB 395.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 30.9/38.9 MB 395.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 30.9/38.9 MB 395.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 30.9/38.9 MB 395.3 kB/s eta 0:00:21\n",
      "   -------------------------------- ------- 31.2/38.9 MB 398.0 kB/s eta 0:00:20\n",
      "   -------------------------------- ------- 31.2/38.9 MB 398.0 kB/s eta 0:00:20\n",
      "   -------------------------------- ------- 31.2/38.9 MB 398.0 kB/s eta 0:00:20\n",
      "   -------------------------------- ------- 31.5/38.9 MB 407.6 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 31.5/38.9 MB 407.6 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 31.5/38.9 MB 407.6 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 31.7/38.9 MB 407.9 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 31.7/38.9 MB 407.9 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 31.7/38.9 MB 407.9 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 32.0/38.9 MB 408.6 kB/s eta 0:00:17\n",
      "   -------------------------------- ------- 32.0/38.9 MB 408.6 kB/s eta 0:00:17\n",
      "   -------------------------------- ------- 32.0/38.9 MB 408.6 kB/s eta 0:00:17\n",
      "   -------------------------------- ------- 32.0/38.9 MB 408.6 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 32.2/38.9 MB 404.9 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 32.2/38.9 MB 404.9 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 32.5/38.9 MB 409.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 32.5/38.9 MB 409.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 32.5/38.9 MB 409.4 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 32.8/38.9 MB 411.6 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 32.8/38.9 MB 411.6 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 33.0/38.9 MB 420.3 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 33.0/38.9 MB 420.3 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 33.0/38.9 MB 420.3 kB/s eta 0:00:15\n",
      "   --------------------------------- ------ 33.0/38.9 MB 420.3 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 33.3/38.9 MB 423.2 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 33.3/38.9 MB 423.2 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 33.3/38.9 MB 423.2 kB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 33.6/38.9 MB 421.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 33.6/38.9 MB 421.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 33.6/38.9 MB 421.4 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 33.8/38.9 MB 426.3 kB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 33.8/38.9 MB 426.3 kB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 33.8/38.9 MB 426.3 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 34.1/38.9 MB 430.6 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 34.1/38.9 MB 430.6 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 34.1/38.9 MB 430.6 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 34.1/38.9 MB 430.6 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 34.3/38.9 MB 424.1 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 34.3/38.9 MB 424.1 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 34.3/38.9 MB 424.1 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 34.6/38.9 MB 420.3 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 34.6/38.9 MB 420.3 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 34.9/38.9 MB 412.8 kB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 34.9/38.9 MB 412.8 kB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 34.9/38.9 MB 412.8 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 35.1/38.9 MB 404.7 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 35.1/38.9 MB 404.7 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 35.1/38.9 MB 404.7 kB/s eta 0:00:10\n",
      "   ------------------------------------ --- 35.4/38.9 MB 398.2 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.4/38.9 MB 398.2 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.4/38.9 MB 398.2 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.4/38.9 MB 398.2 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.7/38.9 MB 396.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.7/38.9 MB 396.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.7/38.9 MB 396.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 35.9/38.9 MB 402.9 kB/s eta 0:00:08\n",
      "   ------------------------------------ --- 35.9/38.9 MB 402.9 kB/s eta 0:00:08\n",
      "   ------------------------------------ --- 35.9/38.9 MB 402.9 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 36.2/38.9 MB 402.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.2/38.9 MB 402.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.2/38.9 MB 402.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.2/38.9 MB 402.6 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.4/38.9 MB 400.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.4/38.9 MB 400.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.4/38.9 MB 400.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.4/38.9 MB 400.5 kB/s eta 0:00:07\n",
      "   ------------------------------------- -- 36.7/38.9 MB 393.4 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 36.7/38.9 MB 393.4 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 37.0/38.9 MB 395.5 kB/s eta 0:00:05\n",
      "   ------------------------------------- -- 37.0/38.9 MB 395.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 37.2/38.9 MB 394.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 37.2/38.9 MB 394.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 37.2/38.9 MB 394.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 37.5/38.9 MB 396.3 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 37.5/38.9 MB 396.3 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 37.7/38.9 MB 393.6 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 37.7/38.9 MB 393.6 kB/s eta 0:00:03\n",
      "   ---------------------------------------  38.0/38.9 MB 389.6 kB/s eta 0:00:03\n",
      "   ---------------------------------------  38.0/38.9 MB 389.6 kB/s eta 0:00:03\n",
      "   ---------------------------------------  38.3/38.9 MB 393.4 kB/s eta 0:00:02\n",
      "   ---------------------------------------  38.3/38.9 MB 393.4 kB/s eta 0:00:02\n",
      "   ---------------------------------------  38.5/38.9 MB 397.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.9 MB 397.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.9 MB 410.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.9 MB 410.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.9 MB 410.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.9 MB 410.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.9/38.9 MB 403.9 kB/s  0:01:45\n",
      "Using cached numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 621.2 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.8/2.0 MB 645.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.8/2.0 MB 595.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 637.7 kB/s  0:00:03\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading simsimd-6.5.3-cp39-cp39-win_amd64.whl (94 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading eval_type_backport-0.3.1-py3-none-any.whl (6.1 kB)\n",
      "Building wheels for collected packages: stringzilla\n",
      "  Building wheel for stringzilla (pyproject.toml): started\n",
      "  Building wheel for stringzilla (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for stringzilla: filename=stringzilla-4.4.0-cp39-cp39-win_amd64.whl size=131007 sha256=7c9b1111ef86cbaa52e399f328c1bce692a8aeee4a4a6f9bd028f31c896e2af2\n",
      "  Stored in directory: c:\\users\\elitelaptop\\appdata\\local\\pip\\cache\\wheels\\1f\\a7\\b9\\ffe4f7bd40039e2aafed51e3f5f17c2a22f0f319cb5b2cfe1c\n",
      "Successfully built stringzilla\n",
      "Installing collected packages: simsimd, typing-inspection, stringzilla, pydantic-core, numpy, eval-type-backport, annotated-types, pydantic, opencv-python-headless, albucore, albumentations\n",
      "\n",
      "   ---------- -----------------------------  3/11 [pydantic-core]\n",
      "  Attempting uninstall: numpy\n",
      "   ---------- -----------------------------  3/11 [pydantic-core]\n",
      "    Found existing installation: numpy 1.26.4\n",
      "   ---------- -----------------------------  3/11 [pydantic-core]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   -------------- -------------------------  4/11 [numpy]\n",
      "   ------------------------- --------------  7/11 [pydantic]\n",
      "   ------------------------- --------------  7/11 [pydantic]\n",
      "   ------------------------- --------------  7/11 [pydantic]\n",
      "   ------------------------- --------------  7/11 [pydantic]\n",
      "   ----------------------------- ----------  8/11 [opencv-python-headless]\n",
      "   ----------------------------- ----------  8/11 [opencv-python-headless]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\pip-uninstall-46hakhlw'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\Lib\\site-packages\\~0mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\EliteLaptop\\\\miniconda3\\\\envs\\\\torch_gpu\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ca49db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_15792\\1003403170.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4e loaded successfully.\n",
      "Using encoder: e4e\n",
      "Found REAL: 790\n",
      "Found FAKE: 3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding REAL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [02:40<00:00,  4.92it/s]\n",
      "Encoding FAKE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3975/3975 [07:06<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (5555, 9216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset apr√®s SMOTE : [3975 3975]\n",
      "\n",
      "Accuracy des mod√®les :\n",
      " - svm: 0.8119\n",
      " - rf: 0.8535\n",
      " - mlp: 0.9308\n",
      " - xgb: 0.8736\n",
      " - voting: 0.9283\n",
      "\n",
      "Pipeline termin√© ! R√©sultats dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_pipeline_optimise\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Deepfake detection pipeline - VERSION OPTIMIS√âE\n",
    "# ==========================================================\n",
    "# Requirements:\n",
    "# pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python imbalanced-learn albumentations\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.spatial.distance import cdist\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "ckpt_e4e = os.path.join('encoder4editing','pretrained_models','e4e_ffhq_encode.pt')\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_pipeline_optimise\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CHARGEMENT MODELE\n",
    "# ----------------------------------------------------------\n",
    "e4e_model = None\n",
    "\n",
    "transform_resnet = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from encoder4editing.models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:,:,::-1]\n",
    "            tf = T.Compose([T.ToPILImage(), T.Resize((256,256)), T.ToTensor()])\n",
    "            t = tf(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "                return out.cpu().numpy().reshape(-1)\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:,:,::-1]\n",
    "        t = transform_resnet(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            return model(t).cpu().numpy().reshape(-1)\n",
    "    return encode\n",
    "\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    e4e_model = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "encoder_fn = e4e_model if e4e_model is not None else load_resnet_fallback()\n",
    "print(\"Using encoder:\", \"e4e\" if e4e_model else \"ResNet50\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# LISTE DES IMAGES\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                all_files.append(os.path.join(root, f))\n",
    "    return all_files\n",
    "\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "print(\"Found REAL:\", len(imgs_real))\n",
    "print(\"Found FAKE:\", len(imgs_fake))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# AUGMENTATION DATA REAL\n",
    "# ----------------------------------------------------------\n",
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "])\n",
    "\n",
    "def augment_image(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    augmented = augment(image=img)['image']\n",
    "    return cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# EXTRACTION DES LATENTS\n",
    "# ----------------------------------------------------------\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Unable to read image \" + p)\n",
    "    return img\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for p in tqdm(imgs_real, desc=\"Encoding REAL\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(0)\n",
    "        # Augmentation 1x pour REAL\n",
    "        img_aug = augment_image(img)\n",
    "        X_list.append(encoder_fn(img_aug))\n",
    "        y_list.append(0)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "for p in tqdm(imgs_fake, desc=\"Encoding FAKE\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "print(\"Latents shape:\", X.shape)\n",
    "np.save(os.path.join(output_dir, \"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir, \"labels.npy\"), y)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# REDUCTION DE DIMENSION POUR ML\n",
    "# ----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=512, random_state=42)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "joblib.dump(pca, os.path.join(output_dir, \"pca.joblib\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# √âQUILIBRAGE DATASET\n",
    "# ----------------------------------------------------------\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_reduced, y)\n",
    "print(\"Dataset apr√®s SMOTE :\", np.bincount(y_res))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# TRAIN / TEST SPLIT\n",
    "# ----------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODELES AVEC HYPERPARAM OPTIMIS√âS\n",
    "# ----------------------------------------------------------\n",
    "models_results = {}\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "pred_svm = svm.predict(X_test)\n",
    "models_results[\"svm\"] = float(accuracy_score(y_test, pred_svm))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=None, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "models_results[\"rf\"] = float(accuracy_score(y_test, pred_rf))\n",
    "\n",
    "# MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256,128), max_iter=700, activation='relu', random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "pred_mlp = mlp.predict(X_test)\n",
    "models_results[\"mlp\"] = float(accuracy_score(y_test, pred_mlp))\n",
    "\n",
    "# XGBoost\n",
    "ratio = np.sum(y_res==0)/np.sum(y_res==1)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=ratio,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "models_results[\"xgb\"] = float(accuracy_score(y_test, pred_xgb))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# VotingClassifier (ensemble)\n",
    "# ----------------------------------------------------------\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('mlp', mlp),\n",
    "    ('xgb', xgb)\n",
    "], voting='soft')\n",
    "\n",
    "voting.fit(X_train, y_train)\n",
    "pred_voting = voting.predict(X_test)\n",
    "models_results[\"voting\"] = float(accuracy_score(y_test, pred_voting))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SAVE MODELS ET RESULTATS\n",
    "# ----------------------------------------------------------\n",
    "joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "joblib.dump(svm, os.path.join(output_dir, \"model_svm.joblib\"))\n",
    "joblib.dump(rf, os.path.join(output_dir, \"model_rf.joblib\"))\n",
    "joblib.dump(mlp, os.path.join(output_dir, \"model_mlp.joblib\"))\n",
    "joblib.dump(xgb, os.path.join(output_dir, \"model_xgb.joblib\"))\n",
    "joblib.dump(voting, os.path.join(output_dir, \"model_voting.joblib\"))\n",
    "\n",
    "with open(os.path.join(output_dir, \"results_models.json\"), \"w\") as f:\n",
    "    json.dump(models_results, f, indent=2)\n",
    "\n",
    "print(\"\\nAccuracy des mod√®les :\")\n",
    "for m,a in models_results.items():\n",
    "    print(f\" - {m}: {a:.4f}\")\n",
    "\n",
    "print(\"\\nPipeline termin√© ! R√©sultats dans :\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cec5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_15792\\1550336598.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4e loaded successfully.\n",
      "Using encoder: e4e\n",
      "Found REAL: 790\n",
      "Found FAKE: 3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding REAL: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [02:44<00:00,  4.79it/s]\n",
      "Encoding FAKE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3975/3975 [07:05<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (5555, 9216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset apr√®s SMOTE : [3975 3975]\n",
      "\n",
      "Accuracy des mod√®les :\n",
      " - svm: 0.8214\n",
      " - rf: 0.8428\n",
      " - mlp: 0.9252\n",
      " - xgb: 0.8566\n",
      " - voting: 0.9170\n",
      "\n",
      "Pipeline termin√© ! R√©sultats dans : C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_LSA_pipeline_optimised\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Deepfake detection pipeline - VERSION OPTIMIS√âE + VISUALISATIONS\n",
    "# ==========================================================\n",
    "# Requirements:\n",
    "# pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python imbalanced-learn albumentations seaborn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "ckpt_e4e = os.path.join('encoder4editing','pretrained_models','e4e_ffhq_encode.pt')\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_LSA_pipeline_optimised\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CHARGEMENT MODELE\n",
    "# ----------------------------------------------------------\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from encoder4editing.models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            tf = T.Compose([T.ToPILImage(), T.Resize((256,256)), T.ToTensor()])\n",
    "            t = tf(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "                return out.cpu().numpy().reshape(-1)\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        t = transform_resnet(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            return model(t).cpu().numpy().reshape(-1)\n",
    "    return encode\n",
    "\n",
    "\n",
    "transform_resnet = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "e4e_model = None\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    e4e_model = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "encoder_fn = e4e_model if e4e_model is not None else load_resnet_fallback()\n",
    "print(\"Using encoder:\", \"e4e\" if e4e_model else \"ResNet50\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# LISTE DES IMAGES\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                all_files.append(os.path.join(root, f))\n",
    "    return all_files\n",
    "\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "print(\"Found REAL:\", len(imgs_real))\n",
    "print(\"Found FAKE:\", len(imgs_fake))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# AUGMENTATION DATA REAL\n",
    "# ----------------------------------------------------------\n",
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "])\n",
    "\n",
    "def augment_image(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    augmented = augment(image=img)['image']\n",
    "    return cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# EXTRACTION DES LATENTS\n",
    "# ----------------------------------------------------------\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Unable to read image \" + p)\n",
    "    return img\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for p in tqdm(imgs_real, desc=\"Encoding REAL\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(0)\n",
    "        img_aug = augment_image(img)\n",
    "        X_list.append(encoder_fn(img_aug))\n",
    "        y_list.append(0)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "for p in tqdm(imgs_fake, desc=\"Encoding FAKE\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "print(\"Latents shape:\", X.shape)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir, \"labels.npy\"), y)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# REDUCTION DE DIMENSION\n",
    "# ----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=512, random_state=42)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "\n",
    "joblib.dump(pca, os.path.join(output_dir, \"pca.joblib\"))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# VISUALISATION 3D_features.png\n",
    "# ----------------------------------------------------------\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_3d[:,0], X_3d[:,1], X_3d[:,2], c=y, cmap='coolwarm', alpha=0.7)\n",
    "ax.set_title(\"3D Feature Space (PCA 3D)\")\n",
    "plt.colorbar(scatter)\n",
    "plt.savefig(os.path.join(output_dir, \"3D_features.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SMOTE\n",
    "# ----------------------------------------------------------\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_reduced, y)\n",
    "print(\"Dataset apr√®s SMOTE :\", np.bincount(y_res))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# TRAIN / TEST SPLIT\n",
    "# ----------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, stratify=y_res, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODELES\n",
    "# ----------------------------------------------------------\n",
    "models_results = {}\n",
    "\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "pred_svm = svm.predict(X_test)\n",
    "models_results[\"svm\"] = float(accuracy_score(y_test, pred_svm))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "models_results[\"rf\"] = float(accuracy_score(y_test, pred_rf))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256,128), max_iter=700, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "pred_mlp = mlp.predict(X_test)\n",
    "models_results[\"mlp\"] = float(accuracy_score(y_test, pred_mlp))\n",
    "\n",
    "ratio = np.sum(y_res==0)/np.sum(y_res==1)\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=ratio,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "models_results[\"xgb\"] = float(accuracy_score(y_test, pred_xgb))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODEL FINAL : VOTING ENSEMBLE\n",
    "# ----------------------------------------------------------\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('rf', rf), ('mlp', mlp), ('xgb', xgb)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting.fit(X_train, y_train)\n",
    "pred_voting = voting.predict(X_test)\n",
    "\n",
    "models_results[\"voting\"] = float(accuracy_score(y_test, pred_voting))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFUSION MATRIX\n",
    "# ----------------------------------------------------------\n",
    "cm = confusion_matrix(y_test, pred_voting)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"REAL\",\"FAKE\"],\n",
    "            yticklabels=[\"REAL\",\"FAKE\"])\n",
    "plt.title(\"Confusion Matrix - Final Model\")\n",
    "plt.savefig(os.path.join(output_dir, \"confusion_matrix_final.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# LEARNING CURVE\n",
    "# ----------------------------------------------------------\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    voting, X_train, y_train, cv=5,\n",
    "    train_sizes=np.linspace(0.1,1.0,10),\n",
    "    scoring='accuracy',\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "test_mean = test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Training Score\")\n",
    "plt.plot(train_sizes, test_mean, label=\"Validation Score\")\n",
    "plt.xlabel(\"Training Samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Learning Curve - Final Model\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(output_dir, \"learning_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SAVE MODELS\n",
    "# ----------------------------------------------------------\n",
    "joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "joblib.dump(svm, os.path.join(output_dir, \"model_svm.joblib\"))\n",
    "joblib.dump(rf, os.path.join(output_dir, \"model_rf.joblib\"))\n",
    "joblib.dump(mlp, os.path.join(output_dir, \"model_mlp.joblib\"))\n",
    "joblib.dump(xgb, os.path.join(output_dir, \"model_xgb.joblib\"))\n",
    "joblib.dump(voting, os.path.join(output_dir, \"model_voting.joblib\"))\n",
    "\n",
    "with open(os.path.join(output_dir, \"results_models.json\"), \"w\") as f:\n",
    "    json.dump(models_results, f, indent=2)\n",
    "\n",
    "\n",
    "print(\"\\nAccuracy des mod√®les :\")\n",
    "for m, a in models_results.items():\n",
    "    print(f\" - {m}: {a:.4f}\")\n",
    "\n",
    "print(\"\\nPipeline termin√© ! R√©sultats dans :\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3dcd97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EliteLaptop\\AppData\\Local\\Temp\\ipykernel_17152\\429933128.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "e4e loaded successfully.\n",
      "Using encoder: e4e\n",
      "Found REAL: 12361 FAKE: 62877\n",
      "Encoding REAL (with augmentation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12361/12361 [1:03:04<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding FAKE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62877/62877 [2:26:35<00:00,  7.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: (87599, 18, 512)\n",
      "Latents averaged over layers -> new shape: (87599, 512)\n",
      "Saved latent_analysis_extended.json (compact stats)\n",
      "Incremental PCA -> shape: (87599, 512)\n",
      "Saved 3D_features.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE class counts: [62877 62877]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 413\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# Random Forest\u001b[39;00m\n\u001b[0;32m    412\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 413\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m pred_rf \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    415\u001b[0m models_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(accuracy_score(y_test, pred_rf))\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\joblib\\parallel.py:1986\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1984\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1985\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1993\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\joblib\\parallel.py:1914\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1914\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    198\u001b[0m         X,\n\u001b[0;32m    199\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    203\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Deepfake detection pipeline - VERSION OPTIMIS√âE + 3D VIS + SAFE DISTANCES\n",
    "# ==========================================================\n",
    "# Requirements:\n",
    "# pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python imbalanced-learn albumentations seaborn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import albumentations as A\n",
    "\n",
    "# Additional analyses\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# paths (tes frames)\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\artifacts\\frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\artifacts\\frames\\fake\"\n",
    "\n",
    "ckpt_e4e = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_LSA_pipeline_optimiseddd\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODEL LOADING (e4e or ResNet fallback)\n",
    "# ----------------------------------------------------------\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from encoder4editing.models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "\n",
    "        # single-image encode (keeps original behavior)\n",
    "        tf_e4e = T.Compose([T.ToPILImage(), T.Resize((256,256)), T.ToTensor()])\n",
    "\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            t = tf_e4e(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "            return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "        # batch encoder wrapper (list of BGR images)\n",
    "        def encode_batch(imgs_bgr_list):\n",
    "            tensors = []\n",
    "            for img_bgr in imgs_bgr_list:\n",
    "                img_rgb = img_bgr[:, :, ::-1]\n",
    "                tensors.append(tf_e4e(img_rgb))\n",
    "            batch_t = torch.stack(tensors).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(batch_t)\n",
    "            return out.cpu().numpy()\n",
    "\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode, encode_batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None, None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    tf_res = T.Compose([\n",
    "        T.ToPILImage(), T.Resize((224,224)), T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        t = tf_res(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(t)\n",
    "        return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "    def encode_batch(imgs_bgr_list):\n",
    "        tensors = []\n",
    "        for img_bgr in imgs_bgr_list:\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            tensors.append(tf_res(img_rgb))\n",
    "        batch_t = torch.stack(tensors).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(batch_t)\n",
    "        return out.cpu().numpy()\n",
    "\n",
    "    return encode, encode_batch\n",
    "\n",
    "# instantiate encoders\n",
    "encode_single = None\n",
    "encode_batch = None\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    encode_single, encode_batch = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "if encode_single is None:\n",
    "    encode_single, encode_batch = load_resnet_fallback()\n",
    "\n",
    "print(\"Using encoder:\", \"e4e\" if os.path.exists(ckpt_e4e) and encode_single is not None else \"ResNet50\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# IO helpers\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    files = []\n",
    "    for root, _, names in os.walk(folder):\n",
    "        for f in names:\n",
    "            if f.lower().endswith(exts):\n",
    "                files.append(os.path.join(root, f))\n",
    "    return sorted(files)\n",
    "\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Unable to read image \" + p)\n",
    "    return img\n",
    "\n",
    "# augmentation (albumentations)\n",
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "])\n",
    "\n",
    "def augment_image(img):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aug = augment(image=img_rgb)['image']\n",
    "    return cv2.cvtColor(aug, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# LIST IMAGES\n",
    "# ----------------------------------------------------------\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "print(\"Found REAL:\", len(imgs_real), \"FAKE:\", len(imgs_fake))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ENCODING (BATCH = 8) with augmentation for REAL\n",
    "# ----------------------------------------------------------\n",
    "batch_size = 16\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "paths_list = []\n",
    "\n",
    "def process_paths_in_batches(paths, label, augment_real=False, batch_size=8):\n",
    "    buff_imgs = []\n",
    "    buff_paths = []\n",
    "    for p in tqdm(paths):\n",
    "        try:\n",
    "            img = safe_imread(p)\n",
    "            buff_imgs.append(img)\n",
    "            buff_paths.append(p)\n",
    "\n",
    "            # if augmentation requested (only for REAL), we append an augmented image to buffer\n",
    "            if augment_real:\n",
    "                img_aug = augment_image(img)\n",
    "                buff_imgs.append(img_aug)\n",
    "                buff_paths.append(p + \"||aug\")\n",
    "\n",
    "            # when buffer reaches batch_size (or more), encode them in batch\n",
    "            while len(buff_imgs) >= batch_size:\n",
    "                batch_imgs = buff_imgs[:batch_size]\n",
    "                batch_paths = buff_paths[:batch_size]\n",
    "                # encode_batch returns (n, latent_dim)\n",
    "                latents = encode_batch(batch_imgs)\n",
    "                for latent, path in zip(latents, batch_paths):\n",
    "                    X_list.append(latent)\n",
    "                    y_list.append(label)\n",
    "                    paths_list.append(path)\n",
    "                # pop used\n",
    "                buff_imgs = buff_imgs[batch_size:]\n",
    "                buff_paths = buff_paths[batch_size:]\n",
    "        except Exception as e:\n",
    "            print(\"Error reading/encoding:\", p, e)\n",
    "\n",
    "    # flush remainder\n",
    "    if len(buff_imgs) > 0:\n",
    "        latents = encode_batch(buff_imgs)\n",
    "        for latent, path in zip(latents, buff_paths):\n",
    "            X_list.append(latent)\n",
    "            y_list.append(label)\n",
    "            paths_list.append(path)\n",
    "\n",
    "# run encoding\n",
    "print(\"Encoding REAL (with augmentation)...\")\n",
    "process_paths_in_batches(imgs_real, label=0, augment_real=True, batch_size=batch_size)\n",
    "\n",
    "print(\"Encoding FAKE ...\")\n",
    "process_paths_in_batches(imgs_fake, label=1, augment_real=False, batch_size=batch_size)\n",
    "# ----------------------------------------------------------\n",
    "# convert to arrays\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "print(\"Latents shape:\", X.shape)\n",
    "\n",
    "# Flatten / average latents if 3D\n",
    "if X.ndim == 3:\n",
    "    X = X.mean(axis=1)\n",
    "    print(\"Latents averaged over layers -> new shape:\", X.shape)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir, \"labels.npy\"), y)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SAFE STATISTICS: mean/var (dimension-aggregated) + sampled distances\n",
    "# ----------------------------------------------------------\n",
    "if len(np.unique(y)) == 2:\n",
    "    X_real = X[y == 0]\n",
    "    X_fake = X[y == 1]\n",
    "else:\n",
    "    X_real = X\n",
    "    X_fake = np.zeros((0, X.shape[1]))\n",
    "\n",
    "# compute per-class mean/var (dimension-wise)\n",
    "mean_real_dimmean = float(np.mean(X_real.mean(axis=0))) if X_real.shape[0] > 0 else None\n",
    "var_real_dimmean = float(np.mean(X_real.var(axis=0))) if X_real.shape[0] > 0 else None\n",
    "mean_fake_dimmean = float(np.mean(X_fake.mean(axis=0))) if X_fake.shape[0] > 0 else None\n",
    "var_fake_dimmean = float(np.mean(X_fake.var(axis=0))) if X_fake.shape[0] > 0 else None\n",
    "\n",
    "# distances: sample to avoid huge memory\n",
    "MAX_SAMPLES = 1500\n",
    "def sample_array(arr, max_n):\n",
    "    n = len(arr)\n",
    "    if n <= max_n:\n",
    "        return arr\n",
    "    idx = np.random.choice(n, max_n, replace=False)\n",
    "    return arr[idx]\n",
    "\n",
    "Xr = sample_array(X_real, MAX_SAMPLES)\n",
    "Xf = sample_array(X_fake, MAX_SAMPLES)\n",
    "\n",
    "intra_real = None\n",
    "intra_fake = None\n",
    "inter_mean = None\n",
    "\n",
    "if Xr.shape[0] > 1:\n",
    "    dr = cdist(Xr, Xr, metric='euclidean')\n",
    "    intra_real = float(np.mean(dr[np.triu_indices_from(dr, 1)]))\n",
    "if Xf.shape[0] > 1:\n",
    "    df = cdist(Xf, Xf, metric='euclidean')\n",
    "    intra_fake = float(np.mean(df[np.triu_indices_from(df, 1)]))\n",
    "if Xr.shape[0] > 0 and Xf.shape[0] > 0:\n",
    "    di = cdist(Xr, Xf, metric='euclidean')\n",
    "    inter_mean = float(np.mean(di))\n",
    "\n",
    "analysis_compact = {\n",
    "    \"real_count\": int(len(X_real)),\n",
    "    \"fake_count\": int(len(X_fake)),\n",
    "    \"sampled_real\": int(len(Xr)),\n",
    "    \"sampled_fake\": int(len(Xf)),\n",
    "    \"mean_of_mean_real\": mean_real_dimmean,\n",
    "    \"var_of_mean_real\": var_real_dimmean,\n",
    "    \"mean_of_mean_fake\": mean_fake_dimmean,\n",
    "    \"var_of_mean_fake\": var_fake_dimmean,\n",
    "    \"intra_real_distance_sampled\": intra_real,\n",
    "    \"intra_fake_distance_sampled\": intra_fake,\n",
    "    \"inter_mean_distance_sampled\": inter_mean\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"latent_analysis_extended.json\"), \"w\") as f:\n",
    "    json.dump(analysis_compact, f, indent=2)\n",
    "\n",
    "print(\"Saved latent_analysis_extended.json (compact stats)\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# STANDARDIZE and INCREMENTAL PCA (memory-friendly)\n",
    "# ----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # X is already in memory as np.array\n",
    "joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "\n",
    "# Incremental PCA to 512 components\n",
    "n_components_big = 512\n",
    "ipca = IncrementalPCA(n_components=n_components_big, batch_size=1024)\n",
    "for start in range(0, X_scaled.shape[0], 1024):\n",
    "    ipca.partial_fit(X_scaled[start:start+1024])\n",
    "X_reduced = ipca.transform(X_scaled)\n",
    "joblib.dump(ipca, os.path.join(output_dir, \"incremental_pca_512.joblib\"))\n",
    "print(\"Incremental PCA -> shape:\", X_reduced.shape)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PCA 3D visualization (from X_reduced -> PCA 3 components)\n",
    "# ----------------------------------------------------------\n",
    "pca3 = PCA(n_components=3, random_state=42)\n",
    "X_3d = pca3.fit_transform(X_reduced)  # shape (N,3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_3d[:,0], X_3d[:,1], X_3d[:,2], c=y, cmap='coolwarm', s=4, alpha=0.7)\n",
    "ax.set_title(\"3D Feature Space (PCA 3D) on reduced features\")\n",
    "ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.set_zlabel(\"PC3\")\n",
    "plt.colorbar(scatter, ax=ax, fraction=0.02, pad=0.1)\n",
    "plt.savefig(os.path.join(output_dir, \"3D_features.png\"), dpi=150)\n",
    "plt.close()\n",
    "print(\"Saved 3D_features.png\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PCA 2D (for visualization)\n",
    "# ----------------------------------------------------------\n",
    "pca2 = PCA(n_components=2, random_state=42)\n",
    "X_pca2 = pca2.fit_transform(X_reduced)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca2[:,0], X_pca2[:,1], c=y, cmap='coolwarm', s=4, alpha=0.7)\n",
    "plt.title(\"PCA 2D (on reduced features)\")\n",
    "plt.savefig(os.path.join(output_dir, \"pca_2d.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# t-SNE and UMAP (on a sample for performance)\n",
    "# ----------------------------------------------------------\n",
    "TS_NEIGHBORS = 3000  # sample size for TSNE/UMAP\n",
    "if X_reduced.shape[0] > TS_NEIGHBORS:\n",
    "    idx_sample = np.random.choice(X_reduced.shape[0], TS_NEIGHBORS, replace=False)\n",
    "    X_vis = X_reduced[idx_sample]\n",
    "    y_vis = y[idx_sample]\n",
    "else:\n",
    "    X_vis = X_reduced\n",
    "    y_vis = y\n",
    "\n",
    "# t-SNE (2D)\n",
    "tsne = TSNE(n_components=2, perplexity=40, n_iter=1000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_vis)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_vis, cmap='coolwarm', s=4, alpha=0.7)\n",
    "plt.title(\"t-SNE (2D) on a sample\")\n",
    "plt.savefig(os.path.join(output_dir, \"tsne_2d.png\"))\n",
    "plt.close()\n",
    "\n",
    "# UMAP (2D)\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_vis)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_umap[:,0], X_umap[:,1], c=y_vis, cmap='coolwarm', s=4, alpha=0.7)\n",
    "plt.title(\"UMAP (2D) on a sample\")\n",
    "plt.savefig(os.path.join(output_dir, \"umap_2d.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CLUSTERING (KMeans 2 clusters) on a sample and silhouette\n",
    "# ----------------------------------------------------------\n",
    "km_sample = X_vis  # use sampled subset\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(km_sample)\n",
    "from sklearn.metrics import silhouette_score\n",
    "sil = silhouette_score(km_sample, cluster_labels) if len(km_sample) > 1 else None\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca2[:,0], X_pca2[:,1], c=cluster_labels.repeat(int(np.ceil(len(X_pca2)/len(cluster_labels))))[:len(X_pca2)], cmap='tab10', s=4, alpha=0.7)\n",
    "plt.title(f\"KMeans clusters (sample) ‚Äî silhouette={sil}\")\n",
    "plt.savefig(os.path.join(output_dir, \"kmeans_pca2d.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SMOTE balancing on X_reduced\n",
    "# ----------------------------------------------------------\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_reduced, y)\n",
    "print(\"After SMOTE class counts:\", np.bincount(y_res))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# TRAIN / TEST SPLIT\n",
    "# ----------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, stratify=y_res, random_state=42)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODELS (SVM, RF, MLP, XGB) and Voting Ensemble\n",
    "# ----------------------------------------------------------\n",
    "models_results = {}\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "pred_svm = svm.predict(X_test)\n",
    "models_results[\"svm\"] = float(accuracy_score(y_test, pred_svm))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "pred_rf = rf.predict(X_test)\n",
    "models_results[\"rf\"] = float(accuracy_score(y_test, pred_rf))\n",
    "\n",
    "# MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256,128), max_iter=700, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "pred_mlp = mlp.predict(X_test)\n",
    "models_results[\"mlp\"] = float(accuracy_score(y_test, pred_mlp))\n",
    "\n",
    "# XGBoost\n",
    "ratio = np.sum(y_res==0) / np.sum(y_res==1)\n",
    "xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, eval_metric=\"logloss\", scale_pos_weight=ratio, random_state=42, use_label_encoder=False)\n",
    "xgb.fit(X_train, y_train)\n",
    "pred_xgb = xgb.predict(X_test)\n",
    "models_results[\"xgb\"] = float(accuracy_score(y_test, pred_xgb))\n",
    "\n",
    "# Voting\n",
    "voting = VotingClassifier(estimators=[('rf', rf), ('mlp', mlp), ('xgb', xgb)], voting='soft')\n",
    "voting.fit(X_train, y_train)\n",
    "pred_voting = voting.predict(X_test)\n",
    "models_results[\"voting\"] = float(accuracy_score(y_test, pred_voting))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Confusion matrix (final voting model)\n",
    "# ----------------------------------------------------------\n",
    "cm = confusion_matrix(y_test, pred_voting)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"REAL\",\"FAKE\"], yticklabels=[\"REAL\",\"FAKE\"])\n",
    "plt.title(\"Confusion Matrix - Voting\")\n",
    "plt.savefig(os.path.join(output_dir, \"confusion_matrix_final.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Learning curve (final)\n",
    "# ----------------------------------------------------------\n",
    "train_sizes, train_scores, test_scores = learning_curve(voting, X_train, y_train, cv=5,\n",
    "                                                       train_sizes=np.linspace(0.1,1.0,10),\n",
    "                                                       scoring='accuracy', shuffle=True, random_state=42)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label='Training Score')\n",
    "plt.plot(train_sizes, test_scores.mean(axis=1), label='Validation Score')\n",
    "plt.xlabel(\"Training samples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Learning Curve - Voting\")\n",
    "plt.savefig(os.path.join(output_dir, \"learning_curve.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SAVE MODELS & RESULTS\n",
    "# ----------------------------------------------------------\n",
    "joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "joblib.dump(ipca, os.path.join(output_dir, \"incremental_pca_512.joblib\"))\n",
    "joblib.dump(svm, os.path.join(output_dir, \"model_svm.joblib\"))\n",
    "joblib.dump(rf, os.path.join(output_dir, \"model_rf.joblib\"))\n",
    "joblib.dump(mlp, os.path.join(output_dir, \"model_mlp.joblib\"))\n",
    "joblib.dump(xgb, os.path.join(output_dir, \"model_xgb.joblib\"))\n",
    "joblib.dump(voting, os.path.join(output_dir, \"model_voting.joblib\"))\n",
    "\n",
    "with open(os.path.join(output_dir, \"results_models.json\"), \"w\") as f:\n",
    "    json.dump(models_results, f, indent=2)\n",
    "\n",
    "with open(os.path.join(output_dir, \"latent_analysis_extended.json\"), \"w\") as f:\n",
    "    json.dump(analysis_compact, f, indent=2)\n",
    "\n",
    "print(\"\\nAccuracy des mod√®les :\")\n",
    "for k,v in models_results.items():\n",
    "    print(f\" - {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nPipeline termin√© ! R√©sultats dans :\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31b22eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n",
      "e4e loaded successfully.\n",
      "Using encoder: e4e\n",
      "Found REAL: 790 FAKE: 3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 790/790 [03:10<00:00,  4.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3975/3975 [07:48<00:00,  8.48it/s]\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ACCURACY: 0.9270440251572327\n",
      "PIPELINE TERMIN√â ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# # ==========================================================\n",
    "# # Deepfake detection pipeline - VERSION OPTIMIS√âE + 3D VIS + SAFE DISTANCES\n",
    "# # ==========================================================\n",
    "# # Requirements:\n",
    "# # pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python imbalanced-learn albumentations seaborn\n",
    "\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torchvision.transforms as T\n",
    "# import torchvision.models as models\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# import json\n",
    "\n",
    "# from sklearn.decomposition import PCA, IncrementalPCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split, learning_curve\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# from xgboost import XGBClassifier\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import albumentations as A\n",
    "\n",
    "# # Additional analyses\n",
    "# from sklearn.manifold import TSNE\n",
    "# import umap.umap_ as umap\n",
    "# from scipy.spatial.distance import cdist\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # CONFIGURATION\n",
    "# # ----------------------------------------------------------\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(\"Device:\", device)\n",
    "\n",
    "# # paths (tes frames)\n",
    "# path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "# path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "# ckpt_e4e = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "# output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_LSA_pipeline_optimisedddd\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# use_gan_inversion = True\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # MODEL LOADING (e4e or ResNet fallback)\n",
    "# # ----------------------------------------------------------\n",
    "# def try_load_e4e(ckpt_path):\n",
    "#     try:\n",
    "#         from encoder4editing.models.psp import pSp\n",
    "#         ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "#         opts = ckpt['opts']\n",
    "#         opts['checkpoint_path'] = ckpt_path\n",
    "#         opts = type('Options', (), opts)()\n",
    "#         net = pSp(opts).to(device).eval()\n",
    "\n",
    "#         # single-image encode (keeps original behavior)\n",
    "#         tf_e4e = T.Compose([T.ToPILImage(), T.Resize((256,256)), T.ToTensor()])\n",
    "\n",
    "#         def encode(img_bgr):\n",
    "#             img_rgb = img_bgr[:, :, ::-1]\n",
    "#             t = tf_e4e(img_rgb).unsqueeze(0).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 out = net.encoder(t)\n",
    "#             return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "#         # batch encoder wrapper (list of BGR images)\n",
    "#         def encode_batch(imgs_bgr_list):\n",
    "#             tensors = []\n",
    "#             for img_bgr in imgs_bgr_list:\n",
    "#                 img_rgb = img_bgr[:, :, ::-1]\n",
    "#                 tensors.append(tf_e4e(img_rgb))\n",
    "#             batch_t = torch.stack(tensors).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 out = net.encoder(batch_t)\n",
    "#             return out.cpu().numpy()\n",
    "\n",
    "#         print(\"e4e loaded successfully.\")\n",
    "#         return encode, encode_batch\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(\"Erreur chargement e4e :\", e)\n",
    "#         return None, None\n",
    "\n",
    "# def load_resnet_fallback():\n",
    "#     model = models.resnet50(pretrained=True)\n",
    "#     model.fc = torch.nn.Identity()\n",
    "#     model.to(device).eval()\n",
    "\n",
    "#     tf_res = T.Compose([\n",
    "#         T.ToPILImage(), T.Resize((224,224)), T.ToTensor(),\n",
    "#         T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "#     ])\n",
    "\n",
    "#     def encode(img_bgr):\n",
    "#         img_rgb = img_bgr[:, :, ::-1]\n",
    "#         t = tf_res(img_rgb).unsqueeze(0).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             out = model(t)\n",
    "#         return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "#     def encode_batch(imgs_bgr_list):\n",
    "#         tensors = []\n",
    "#         for img_bgr in imgs_bgr_list:\n",
    "#             img_rgb = img_bgr[:, :, ::-1]\n",
    "#             tensors.append(tf_res(img_rgb))\n",
    "#         batch_t = torch.stack(tensors).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             out = model(batch_t)\n",
    "#         return out.cpu().numpy()\n",
    "\n",
    "#     return encode, encode_batch\n",
    "\n",
    "# # instantiate encoders\n",
    "# encode_single = None\n",
    "# encode_batch = None\n",
    "# if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "#     encode_single, encode_batch = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "# if encode_single is None:\n",
    "#     encode_single, encode_batch = load_resnet_fallback()\n",
    "\n",
    "# print(\"Using encoder:\", \"e4e\" if os.path.exists(ckpt_e4e) and encode_single is not None else \"ResNet50\")\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # IO helpers\n",
    "# # ----------------------------------------------------------\n",
    "# def list_images(folder):\n",
    "#     exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "#     files = []\n",
    "#     for root, _, names in os.walk(folder):\n",
    "#         for f in names:\n",
    "#             if f.lower().endswith(exts):\n",
    "#                 files.append(os.path.join(root, f))\n",
    "#     return sorted(files)\n",
    "\n",
    "# def safe_imread(p):\n",
    "#     img = cv2.imread(p)\n",
    "#     if img is None:\n",
    "#         raise ValueError(\"Unable to read image \" + p)\n",
    "#     return img\n",
    "\n",
    "# # augmentation (albumentations)\n",
    "# augment = A.Compose([\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.3),\n",
    "#     A.Rotate(limit=15, p=0.3),\n",
    "# ])\n",
    "\n",
    "# def augment_image(img):\n",
    "#     img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#     aug = augment(image=img_rgb)['image']\n",
    "#     return cv2.cvtColor(aug, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # LIST IMAGES\n",
    "# # ----------------------------------------------------------\n",
    "# imgs_real = list_images(path_real)\n",
    "# imgs_fake = list_images(path_fake)\n",
    "# print(\"Found REAL:\", len(imgs_real), \"FAKE:\", len(imgs_fake))\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # ENCODING (BATCH = 8) with augmentation for REAL\n",
    "# # ----------------------------------------------------------\n",
    "# batch_size = 16\n",
    "\n",
    "# X_list = []\n",
    "# y_list = []\n",
    "# paths_list = []\n",
    "\n",
    "# def process_paths_in_batches(paths, label, augment_real=False, batch_size=8):\n",
    "#     buff_imgs = []\n",
    "#     buff_paths = []\n",
    "#     for p in tqdm(paths):\n",
    "#         try:\n",
    "#             img = safe_imread(p)\n",
    "#             buff_imgs.append(img)\n",
    "#             buff_paths.append(p)\n",
    "\n",
    "#             # if augmentation requested (only for REAL), we append an augmented image to buffer\n",
    "#             if augment_real:\n",
    "#                 img_aug = augment_image(img)\n",
    "#                 buff_imgs.append(img_aug)\n",
    "#                 buff_paths.append(p + \"||aug\")\n",
    "\n",
    "#             # when buffer reaches batch_size (or more), encode them in batch\n",
    "#             while len(buff_imgs) >= batch_size:\n",
    "#                 batch_imgs = buff_imgs[:batch_size]\n",
    "#                 batch_paths = buff_paths[:batch_size]\n",
    "#                 # encode_batch returns (n, latent_dim)\n",
    "#                 latents = encode_batch(batch_imgs)\n",
    "#                 for latent, path in zip(latents, batch_paths):\n",
    "#                     X_list.append(latent)\n",
    "#                     y_list.append(label)\n",
    "#                     paths_list.append(path)\n",
    "#                 # pop used\n",
    "#                 buff_imgs = buff_imgs[batch_size:]\n",
    "#                 buff_paths = buff_paths[batch_size:]\n",
    "#         except Exception as e:\n",
    "#             print(\"Error reading/encoding:\", p, e)\n",
    "\n",
    "#     # flush remainder\n",
    "#     if len(buff_imgs) > 0:\n",
    "#         latents = encode_batch(buff_imgs)\n",
    "#         for latent, path in zip(latents, buff_paths):\n",
    "#             X_list.append(latent)\n",
    "#             y_list.append(label)\n",
    "#             paths_list.append(path)\n",
    "\n",
    "# # run encoding\n",
    "# print(\"Encoding REAL (with augmentation)...\")\n",
    "# process_paths_in_batches(imgs_real, label=0, augment_real=True, batch_size=batch_size)\n",
    "\n",
    "# print(\"Encoding FAKE ...\")\n",
    "# process_paths_in_batches(imgs_fake, label=1, augment_real=False, batch_size=batch_size)\n",
    "# # ----------------------------------------------------------\n",
    "# # convert to arrays\n",
    "# X = np.array(X_list)\n",
    "# y = np.array(y_list)\n",
    "# print(\"Latents shape:\", X.shape)\n",
    "\n",
    "# # Flatten / average latents if 3D\n",
    "# if X.ndim == 3:\n",
    "#     X = X.mean(axis=1)\n",
    "#     print(\"Latents averaged over layers -> new shape:\", X.shape)\n",
    "\n",
    "# np.save(os.path.join(output_dir, \"latent_vectors.npy\"), X)\n",
    "# np.save(os.path.join(output_dir, \"labels.npy\"), y)\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # SAFE STATISTICS: mean/var (dimension-aggregated) + sampled distances\n",
    "# # ----------------------------------------------------------\n",
    "# if len(np.unique(y)) == 2:\n",
    "#     X_real = X[y == 0]\n",
    "#     X_fake = X[y == 1]\n",
    "# else:\n",
    "#     X_real = X\n",
    "#     X_fake = np.zeros((0, X.shape[1]))\n",
    "\n",
    "# # compute per-class mean/var (dimension-wise)\n",
    "# mean_real_dimmean = float(np.mean(X_real.mean(axis=0))) if X_real.shape[0] > 0 else None\n",
    "# var_real_dimmean = float(np.mean(X_real.var(axis=0))) if X_real.shape[0] > 0 else None\n",
    "# mean_fake_dimmean = float(np.mean(X_fake.mean(axis=0))) if X_fake.shape[0] > 0 else None\n",
    "# var_fake_dimmean = float(np.mean(X_fake.var(axis=0))) if X_fake.shape[0] > 0 else None\n",
    "\n",
    "# # distances: sample to avoid huge memory\n",
    "# MAX_SAMPLES = 1500\n",
    "# def sample_array(arr, max_n):\n",
    "#     n = len(arr)\n",
    "#     if n <= max_n:\n",
    "#         return arr\n",
    "#     idx = np.random.choice(n, max_n, replace=False)\n",
    "#     return arr[idx]\n",
    "\n",
    "# Xr = sample_array(X_real, MAX_SAMPLES)\n",
    "# Xf = sample_array(X_fake, MAX_SAMPLES)\n",
    "\n",
    "# intra_real = None\n",
    "# intra_fake = None\n",
    "# inter_mean = None\n",
    "\n",
    "# if Xr.shape[0] > 1:\n",
    "#     dr = cdist(Xr, Xr, metric='euclidean')\n",
    "#     intra_real = float(np.mean(dr[np.triu_indices_from(dr, 1)]))\n",
    "# if Xf.shape[0] > 1:\n",
    "#     df = cdist(Xf, Xf, metric='euclidean')\n",
    "#     intra_fake = float(np.mean(df[np.triu_indices_from(df, 1)]))\n",
    "# if Xr.shape[0] > 0 and Xf.shape[0] > 0:\n",
    "#     di = cdist(Xr, Xf, metric='euclidean')\n",
    "#     inter_mean = float(np.mean(di))\n",
    "\n",
    "# analysis_compact = {\n",
    "#     \"real_count\": int(len(X_real)),\n",
    "#     \"fake_count\": int(len(X_fake)),\n",
    "#     \"sampled_real\": int(len(Xr)),\n",
    "#     \"sampled_fake\": int(len(Xf)),\n",
    "#     \"mean_of_mean_real\": mean_real_dimmean,\n",
    "#     \"var_of_mean_real\": var_real_dimmean,\n",
    "#     \"mean_of_mean_fake\": mean_fake_dimmean,\n",
    "#     \"var_of_mean_fake\": var_fake_dimmean,\n",
    "#     \"intra_real_distance_sampled\": intra_real,\n",
    "#     \"intra_fake_distance_sampled\": intra_fake,\n",
    "#     \"inter_mean_distance_sampled\": inter_mean\n",
    "# }\n",
    "\n",
    "# with open(os.path.join(output_dir, \"latent_analysis_extended.json\"), \"w\") as f:\n",
    "#     json.dump(analysis_compact, f, indent=2)\n",
    "\n",
    "# print(\"Saved latent_analysis_extended.json (compact stats)\")\n",
    "\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # STANDARDIZE and INCREMENTAL PCA (memory-friendly)\n",
    "# # ----------------------------------------------------------\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)  # X is already in memory as np.array\n",
    "# joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "\n",
    "# # Incremental PCA to 512 components\n",
    "# n_components_big = 512\n",
    "# ipca = IncrementalPCA(n_components=n_components_big, batch_size=1024)\n",
    "# for start in range(0, X_scaled.shape[0], 1024):\n",
    "#     ipca.partial_fit(X_scaled[start:start+1024])\n",
    "# X_reduced = ipca.transform(X_scaled)\n",
    "# joblib.dump(ipca, os.path.join(output_dir, \"incremental_pca_512.joblib\"))\n",
    "# print(\"Incremental PCA -> shape:\", X_reduced.shape)\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # PCA 3D visualization (from X_reduced -> PCA 3 components)\n",
    "# # ----------------------------------------------------------\n",
    "# pca3 = PCA(n_components=3, random_state=42)\n",
    "# X_3d = pca3.fit_transform(X_reduced)  # shape (N,3)\n",
    "\n",
    "# fig = plt.figure(figsize=(10,8))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# scatter = ax.scatter(X_3d[:,0], X_3d[:,1], X_3d[:,2], c=y, cmap='coolwarm', s=4, alpha=0.7)\n",
    "# ax.set_title(\"3D Feature Space (PCA 3D) on reduced features\")\n",
    "# ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.set_zlabel(\"PC3\")\n",
    "# plt.colorbar(scatter, ax=ax, fraction=0.02, pad=0.1)\n",
    "# plt.savefig(os.path.join(output_dir, \"3D_features.png\"), dpi=150)\n",
    "# plt.close()\n",
    "# print(\"Saved 3D_features.png\")\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # PCA 2D (for visualization)\n",
    "# # ----------------------------------------------------------\n",
    "# pca2 = PCA(n_components=2, random_state=42)\n",
    "# X_pca2 = pca2.fit_transform(X_reduced)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(X_pca2[:,0], X_pca2[:,1], c=y, cmap='coolwarm', s=4, alpha=0.7)\n",
    "# plt.title(\"PCA 2D (on reduced features)\")\n",
    "# plt.savefig(os.path.join(output_dir, \"pca_2d.png\"))\n",
    "# plt.close()\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # t-SNE and UMAP (on a sample for performance)\n",
    "# # ----------------------------------------------------------\n",
    "# TS_NEIGHBORS = 3000  # sample size for TSNE/UMAP\n",
    "# if X_reduced.shape[0] > TS_NEIGHBORS:\n",
    "#     idx_sample = np.random.choice(X_reduced.shape[0], TS_NEIGHBORS, replace=False)\n",
    "#     X_vis = X_reduced[idx_sample]\n",
    "#     y_vis = y[idx_sample]\n",
    "# else:\n",
    "#     X_vis = X_reduced\n",
    "#     y_vis = y\n",
    "\n",
    "# # t-SNE (2D)\n",
    "# tsne = TSNE(n_components=2, perplexity=40, n_iter=1000, random_state=42)\n",
    "# X_tsne = tsne.fit_transform(X_vis)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_vis, cmap='coolwarm', s=4, alpha=0.7)\n",
    "# plt.title(\"t-SNE (2D) on a sample\")\n",
    "# plt.savefig(os.path.join(output_dir, \"tsne_2d.png\"))\n",
    "# plt.close()\n",
    "\n",
    "# # UMAP (2D)\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "# X_umap = reducer.fit_transform(X_vis)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(X_umap[:,0], X_umap[:,1], c=y_vis, cmap='coolwarm', s=4, alpha=0.7)\n",
    "# plt.title(\"UMAP (2D) on a sample\")\n",
    "# plt.savefig(os.path.join(output_dir, \"umap_2d.png\"))\n",
    "# plt.close()\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # CLUSTERING (KMeans 2 clusters) on a sample and silhouette\n",
    "# # ----------------------------------------------------------\n",
    "# km_sample = X_vis  # use sampled subset\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# cluster_labels = kmeans.fit_predict(km_sample)\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# sil = silhouette_score(km_sample, cluster_labels) if len(km_sample) > 1 else None\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.scatter(X_pca2[:,0], X_pca2[:,1], c=cluster_labels.repeat(int(np.ceil(len(X_pca2)/len(cluster_labels))))[:len(X_pca2)], cmap='tab10', s=4, alpha=0.7)\n",
    "# plt.title(f\"KMeans clusters (sample) ‚Äî silhouette={sil}\")\n",
    "# plt.savefig(os.path.join(output_dir, \"kmeans_pca2d.png\"))\n",
    "# plt.close()\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # SMOTE balancing on X_reduced\n",
    "# # ----------------------------------------------------------\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_res, y_res = smote.fit_resample(X_reduced, y)\n",
    "# print(\"After SMOTE class counts:\", np.bincount(y_res))\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # TRAIN / TEST SPLIT\n",
    "# # ----------------------------------------------------------\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, stratify=y_res, random_state=42)\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # MODELS (SVM, RF, MLP, XGB) and Voting Ensemble\n",
    "# # ----------------------------------------------------------\n",
    "# models_results = {}\n",
    "\n",
    "# # SVM\n",
    "# svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "# svm.fit(X_train, y_train)\n",
    "# pred_svm = svm.predict(X_test)\n",
    "# models_results[\"svm\"] = float(accuracy_score(y_test, pred_svm))\n",
    "\n",
    "# # Random Forest\n",
    "# rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "# pred_rf = rf.predict(X_test)\n",
    "# models_results[\"rf\"] = float(accuracy_score(y_test, pred_rf))\n",
    "\n",
    "# # MLP\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(512,256,128), max_iter=700, random_state=42)\n",
    "# mlp.fit(X_train, y_train)\n",
    "# pred_mlp = mlp.predict(X_test)\n",
    "# models_results[\"mlp\"] = float(accuracy_score(y_test, pred_mlp))\n",
    "\n",
    "# # XGBoost\n",
    "# ratio = np.sum(y_res==0) / np.sum(y_res==1)\n",
    "# xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, eval_metric=\"logloss\", scale_pos_weight=ratio, random_state=42, use_label_encoder=False)\n",
    "# xgb.fit(X_train, y_train)\n",
    "# pred_xgb = xgb.predict(X_test)\n",
    "# models_results[\"xgb\"] = float(accuracy_score(y_test, pred_xgb))\n",
    "\n",
    "# # Voting\n",
    "# voting = VotingClassifier(estimators=[('rf', rf), ('mlp', mlp), ('xgb', xgb)], voting='soft')\n",
    "# voting.fit(X_train, y_train)\n",
    "# pred_voting = voting.predict(X_test)\n",
    "# models_results[\"voting\"] = float(accuracy_score(y_test, pred_voting))\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # Confusion matrix (final voting model)\n",
    "# # ----------------------------------------------------------\n",
    "# cm = confusion_matrix(y_test, pred_voting)\n",
    "# plt.figure(figsize=(6,5))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"REAL\",\"FAKE\"], yticklabels=[\"REAL\",\"FAKE\"])\n",
    "# plt.title(\"Confusion Matrix - Voting\")\n",
    "# plt.savefig(os.path.join(output_dir, \"confusion_matrix_final.png\"))\n",
    "# plt.close()\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # Learning curve (final)\n",
    "# # ----------------------------------------------------------\n",
    "# train_sizes, train_scores, test_scores = learning_curve(voting, X_train, y_train, cv=5,\n",
    "#                                                        train_sizes=np.linspace(0.1,1.0,10),\n",
    "#                                                        scoring='accuracy', shuffle=True, random_state=42)\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.plot(train_sizes, train_scores.mean(axis=1), label='Training Score')\n",
    "# plt.plot(train_sizes, test_scores.mean(axis=1), label='Validation Score')\n",
    "# plt.xlabel(\"Training samples\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.title(\"Learning Curve - Voting\")\n",
    "# plt.savefig(os.path.join(output_dir, \"learning_curve.png\"))\n",
    "# plt.close()\n",
    "\n",
    "# # ----------------------------------------------------------\n",
    "# # SAVE MODELS & RESULTS\n",
    "# # ----------------------------------------------------------\n",
    "# joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "# joblib.dump(ipca, os.path.join(output_dir, \"incremental_pca_512.joblib\"))\n",
    "# joblib.dump(svm, os.path.join(output_dir, \"model_svm.joblib\"))\n",
    "# joblib.dump(rf, os.path.join(output_dir, \"model_rf.joblib\"))\n",
    "# joblib.dump(mlp, os.path.join(output_dir, \"model_mlp.joblib\"))\n",
    "# joblib.dump(xgb, os.path.join(output_dir, \"model_xgb.joblib\"))\n",
    "# joblib.dump(voting, os.path.join(output_dir, \"model_voting.joblib\"))\n",
    "\n",
    "# with open(os.path.join(output_dir, \"results_models.json\"), \"w\") as f:\n",
    "#     json.dump(models_results, f, indent=2)\n",
    "\n",
    "# with open(os.path.join(output_dir, \"latent_analysis_extended.json\"), \"w\") as f:\n",
    "#     json.dump(analysis_compact, f, indent=2)\n",
    "\n",
    "# print(\"\\nAccuracy des mod√®les :\")\n",
    "# for k,v in models_results.items():\n",
    "#     print(f\" - {k}: {v:.4f}\")\n",
    "\n",
    "# print(\"\\nPipeline termin√© ! R√©sultats dans :\", output_dir)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Deepfake detection pipeline - VERSION OPTIMIS√âE + 3D VIS + SAFE DISTANCES\n",
    "# ==========================================================\n",
    "# Requirements:\n",
    "# pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python imbalanced-learn albumentations seaborn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "ckpt_e4e = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_LSA_pipeline_optimiseddddd\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODEL LOADING (e4e or ResNet fallback)\n",
    "# ----------------------------------------------------------\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from encoder4editing.models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=True)\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "\n",
    "        tf_e4e = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256,256)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            t = tf_e4e(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "            return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "        def encode_batch(imgs_bgr_list):\n",
    "            tensors = []\n",
    "            for img_bgr in imgs_bgr_list:\n",
    "                img_rgb = img_bgr[:, :, ::-1]\n",
    "                tensors.append(tf_e4e(img_rgb))\n",
    "            batch_t = torch.stack(tensors).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(batch_t)\n",
    "            return out.cpu().numpy()\n",
    "\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode, encode_batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None, None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(weights=\"DEFAULT\")\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    tf_res = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        t = tf_res(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(t)\n",
    "        return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "    def encode_batch(imgs_bgr_list):\n",
    "        tensors = []\n",
    "        for img_bgr in imgs_bgr_list:\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            tensors.append(tf_res(img_rgb))\n",
    "        batch_t = torch.stack(tensors).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(batch_t)\n",
    "        return out.cpu().numpy()\n",
    "\n",
    "    return encode, encode_batch\n",
    "\n",
    "encode_single, encode_batch = None, None\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    encode_single, encode_batch = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "if encode_single is None:\n",
    "    encode_single, encode_batch = load_resnet_fallback()\n",
    "\n",
    "print(\"Using encoder:\", \"e4e\" if encode_single else \"ResNet50\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    return [os.path.join(root,f)\n",
    "            for root,_,files in os.walk(folder)\n",
    "            for f in files if f.lower().endswith(exts)]\n",
    "\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image {p}\")\n",
    "    return img\n",
    "\n",
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "])\n",
    "\n",
    "def augment_image(img):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aug = augment(image=img_rgb)['image']\n",
    "    return cv2.cvtColor(aug, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ----------------------------------------------------------\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "print(\"Found REAL:\", len(imgs_real), \"FAKE:\", len(imgs_fake))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ENCODING\n",
    "# ----------------------------------------------------------\n",
    "batch_size = 16\n",
    "X_list, y_list = [], []\n",
    "\n",
    "def process(paths, label, augment_real=False):\n",
    "    buf = []\n",
    "    for p in tqdm(paths):\n",
    "        img = safe_imread(p)\n",
    "        buf.append(img)\n",
    "        if augment_real:\n",
    "            buf.append(augment_image(img))\n",
    "        if len(buf) >= batch_size:\n",
    "            lat = encode_batch(buf)\n",
    "            X_list.extend(lat)\n",
    "            y_list.extend([label]*len(lat))\n",
    "            buf = []\n",
    "    if buf:\n",
    "        lat = encode_batch(buf)\n",
    "        X_list.extend(lat)\n",
    "        y_list.extend([label]*len(lat))\n",
    "\n",
    "process(imgs_real, 0, augment_real=True)\n",
    "process(imgs_fake, 1, augment_real=False)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "if X.ndim == 3:\n",
    "    X = X.mean(axis=1)\n",
    "\n",
    "np.save(os.path.join(output_dir,\"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir,\"labels.npy\"), y)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# STATISTICS (SAFE)\n",
    "# ----------------------------------------------------------\n",
    "def sample(arr, n=1500):\n",
    "    if len(arr)<=n: return arr\n",
    "    idx = np.random.choice(len(arr), n, replace=False)\n",
    "    return arr[idx]\n",
    "\n",
    "Xr, Xf = X[y==0], X[y==1]\n",
    "Xr_s, Xf_s = sample(Xr), sample(Xf)\n",
    "\n",
    "analysis = {\n",
    "    \"intra_real\": float(np.mean(cdist(Xr_s,Xr_s))) if len(Xr_s)>1 else None,\n",
    "    \"intra_fake\": float(np.mean(cdist(Xf_s,Xf_s))) if len(Xf_s)>1 else None,\n",
    "    \"inter\": float(np.mean(cdist(Xr_s,Xf_s))) if len(Xr_s)>0 and len(Xf_s)>0 else None\n",
    "}\n",
    "\n",
    "json.dump(analysis, open(os.path.join(output_dir,\"latent_analysis_extended.json\"),\"w\"), indent=2)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# STANDARDIZE + IPCA\n",
    "# ----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "ipca = IncrementalPCA(n_components=512, batch_size=1024)\n",
    "for i in range(0,len(X_scaled),1024):\n",
    "    ipca.partial_fit(X_scaled[i:i+1024])\n",
    "X_red = ipca.transform(X_scaled)\n",
    "\n",
    "joblib.dump(scaler, os.path.join(output_dir,\"scaler.joblib\"))\n",
    "joblib.dump(ipca, os.path.join(output_dir,\"incremental_pca_512.joblib\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# VISUALISATIONS\n",
    "# ----------------------------------------------------------\n",
    "X_3d = PCA(3).fit_transform(X_red)\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_3d[:,0],X_3d[:,1],X_3d[:,2],c=y,s=4,cmap=\"coolwarm\")\n",
    "plt.savefig(os.path.join(output_dir,\"3D_features.png\"))\n",
    "plt.close()\n",
    "\n",
    "# TSNE FIXED\n",
    "tsne = TSNE(n_components=2, perplexity=40, max_iter=1000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(sample(X_red,3000))\n",
    "plt.scatter(X_tsne[:,0],X_tsne[:,1],s=4)\n",
    "plt.savefig(os.path.join(output_dir,\"tsne_2d.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SMOTE + TRAIN\n",
    "# ----------------------------------------------------------\n",
    "X_res, y_res = SMOTE(random_state=42).fit_resample(X_red,y)\n",
    "Xtr,Xte,ytr,yte = train_test_split(X_res,y_res,test_size=0.2,random_state=42)\n",
    "\n",
    "svm = SVC(C=10,probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256,128), max_iter=700)\n",
    "xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, eval_metric=\"logloss\")\n",
    "\n",
    "for m in [svm,rf,mlp,xgb]:\n",
    "    m.fit(Xtr,ytr)\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[(\"rf\",rf),(\"mlp\",mlp),(\"xgb\",xgb)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "voting.fit(Xtr,ytr)\n",
    "\n",
    "pred = voting.predict(Xte)\n",
    "acc = accuracy_score(yte,pred)\n",
    "\n",
    "cm = confusion_matrix(yte,pred)\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "plt.savefig(os.path.join(output_dir,\"confusion_matrix_final.png\"))\n",
    "plt.close()\n",
    "\n",
    "joblib.dump(voting, os.path.join(output_dir,\"model_voting.joblib\"))\n",
    "\n",
    "print(\"FINAL ACCURACY:\", acc)\n",
    "print(\"PIPELINE TERMIN√â ‚úÖ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72659c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: encoder4editing\\pretrained_models\\e4e_ffhq_encode.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\encoder4editing\\models\\psp.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(self.opts.checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4e loaded successfully.\n",
      "Using encoder: e4e\n",
      "Found REAL: 12361 FAKE: 62877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12361/12361 [32:23<00:00,  6.36it/s]\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 43439/62877 [59:18<26:32, 12.21it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 185\u001b[0m\n\u001b[0;32m    182\u001b[0m         y_list\u001b[38;5;241m.\u001b[39mextend([label]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(lat))\n\u001b[0;32m    184\u001b[0m process(imgs_real, \u001b[38;5;241m0\u001b[39m, augment_real\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 185\u001b[0m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_fake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment_real\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_list)\n\u001b[0;32m    188\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_list)\n",
      "Cell \u001b[1;32mIn[2], line 175\u001b[0m, in \u001b[0;36mprocess\u001b[1;34m(paths, label, augment_real)\u001b[0m\n\u001b[0;32m    173\u001b[0m     buf\u001b[38;5;241m.\u001b[39mappend(augment_image(img))\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buf) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 175\u001b[0m     lat \u001b[38;5;241m=\u001b[39m \u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     X_list\u001b[38;5;241m.\u001b[39mextend(lat)\n\u001b[0;32m    177\u001b[0m     y_list\u001b[38;5;241m.\u001b[39mextend([label]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(lat))\n",
      "Cell \u001b[1;32mIn[2], line 79\u001b[0m, in \u001b[0;36mtry_load_e4e.<locals>.encode_batch\u001b[1;34m(imgs_bgr_list)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     78\u001b[0m     out \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mencoder(batch_t)\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\artifacts\\frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\artifacts\\frames\\fake\"\n",
    "\n",
    "ckpt_e4e = os.path.join('encoder4editing', 'pretrained_models', 'e4e_ffhq_encode.pt')\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_LSA_pipeline_optimisedddddd\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# MODEL LOADING (e4e or ResNet fallback)\n",
    "# ----------------------------------------------------------\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from encoder4editing.models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=True)\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "\n",
    "        tf_e4e = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256,256)),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            t = tf_e4e(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "            return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "        def encode_batch(imgs_bgr_list):\n",
    "            tensors = []\n",
    "            for img_bgr in imgs_bgr_list:\n",
    "                img_rgb = img_bgr[:, :, ::-1]\n",
    "                tensors.append(tf_e4e(img_rgb))\n",
    "            batch_t = torch.stack(tensors).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(batch_t)\n",
    "            return out.cpu().numpy()\n",
    "\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode, encode_batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None, None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(weights=\"DEFAULT\")\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "\n",
    "    tf_res = T.Compose([\n",
    "        T.ToPILImage(),\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:, :, ::-1]\n",
    "        t = tf_res(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(t)\n",
    "        return out.cpu().numpy().reshape(-1)\n",
    "\n",
    "    def encode_batch(imgs_bgr_list):\n",
    "        tensors = []\n",
    "        for img_bgr in imgs_bgr_list:\n",
    "            img_rgb = img_bgr[:, :, ::-1]\n",
    "            tensors.append(tf_res(img_rgb))\n",
    "        batch_t = torch.stack(tensors).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(batch_t)\n",
    "        return out.cpu().numpy()\n",
    "\n",
    "    return encode, encode_batch\n",
    "\n",
    "encode_single, encode_batch = None, None\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    encode_single, encode_batch = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "if encode_single is None:\n",
    "    encode_single, encode_batch = load_resnet_fallback()\n",
    "\n",
    "print(\"Using encoder:\", \"e4e\" if encode_single else \"ResNet50\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# HELPERS\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    return [os.path.join(root,f)\n",
    "            for root,_,files in os.walk(folder)\n",
    "            for f in files if f.lower().endswith(exts)]\n",
    "\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image {p}\")\n",
    "    return img\n",
    "\n",
    "augment = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.Rotate(limit=15, p=0.3),\n",
    "])\n",
    "\n",
    "def augment_image(img):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aug = augment(image=img_rgb)['image']\n",
    "    return cv2.cvtColor(aug, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# ----------------------------------------------------------\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "print(\"Found REAL:\", len(imgs_real), \"FAKE:\", len(imgs_fake))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ENCODING\n",
    "# ----------------------------------------------------------\n",
    "batch_size = 16\n",
    "X_list, y_list = [], []\n",
    "\n",
    "def process(paths, label, augment_real=False):\n",
    "    buf = []\n",
    "    for p in tqdm(paths):\n",
    "        img = safe_imread(p)\n",
    "        buf.append(img)\n",
    "        if augment_real:\n",
    "            buf.append(augment_image(img))\n",
    "        if len(buf) >= batch_size:\n",
    "            lat = encode_batch(buf)\n",
    "            X_list.extend(lat)\n",
    "            y_list.extend([label]*len(lat))\n",
    "            buf = []\n",
    "    if buf:\n",
    "        lat = encode_batch(buf)\n",
    "        X_list.extend(lat)\n",
    "        y_list.extend([label]*len(lat))\n",
    "\n",
    "process(imgs_real, 0, augment_real=True)\n",
    "process(imgs_fake, 1, augment_real=False)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "if X.ndim == 3:\n",
    "    X = X.mean(axis=1)\n",
    "\n",
    "np.save(os.path.join(output_dir,\"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir,\"labels.npy\"), y)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# STATISTICS (SAFE)\n",
    "# ----------------------------------------------------------\n",
    "def sample(arr, n=1500):\n",
    "    if len(arr)<=n: return arr\n",
    "    idx = np.random.choice(len(arr), n, replace=False)\n",
    "    return arr[idx]\n",
    "\n",
    "Xr, Xf = X[y==0], X[y==1]\n",
    "Xr_s, Xf_s = sample(Xr), sample(Xf)\n",
    "\n",
    "analysis = {\n",
    "    \"intra_real\": float(np.mean(cdist(Xr_s,Xr_s))) if len(Xr_s)>1 else None,\n",
    "    \"intra_fake\": float(np.mean(cdist(Xf_s,Xf_s))) if len(Xf_s)>1 else None,\n",
    "    \"inter\": float(np.mean(cdist(Xr_s,Xf_s))) if len(Xr_s)>0 and len(Xf_s)>0 else None\n",
    "}\n",
    "\n",
    "json.dump(analysis, open(os.path.join(output_dir,\"latent_analysis_extended.json\"),\"w\"), indent=2)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# STANDARDIZE + IPCA\n",
    "# ----------------------------------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "ipca = IncrementalPCA(n_components=512, batch_size=1024)\n",
    "for i in range(0,len(X_scaled),1024):\n",
    "    ipca.partial_fit(X_scaled[i:i+1024])\n",
    "X_red = ipca.transform(X_scaled)\n",
    "\n",
    "joblib.dump(scaler, os.path.join(output_dir,\"scaler.joblib\"))\n",
    "joblib.dump(ipca, os.path.join(output_dir,\"incremental_pca_512.joblib\"))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# VISUALISATIONS\n",
    "# ----------------------------------------------------------\n",
    "X_3d = PCA(3).fit_transform(X_red)\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_3d[:,0],X_3d[:,1],X_3d[:,2],c=y,s=4,cmap=\"coolwarm\")\n",
    "plt.savefig(os.path.join(output_dir,\"3D_features.png\"))\n",
    "plt.close()\n",
    "\n",
    "# TSNE FIXED\n",
    "tsne = TSNE(n_components=2, perplexity=40, max_iter=1000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(sample(X_red,3000))\n",
    "plt.scatter(X_tsne[:,0],X_tsne[:,1],s=4)\n",
    "plt.savefig(os.path.join(output_dir,\"tsne_2d.png\"))\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# SMOTE + TRAIN\n",
    "# ----------------------------------------------------------\n",
    "X_res, y_res = SMOTE(random_state=42).fit_resample(X_red,y)\n",
    "Xtr,Xte,ytr,yte = train_test_split(X_res,y_res,test_size=0.2,random_state=42)\n",
    "\n",
    "svm = SVC(C=10,probability=True)\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256,128), max_iter=700)\n",
    "xgb = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, eval_metric=\"logloss\")\n",
    "\n",
    "for m in [svm,rf,mlp,xgb]:\n",
    "    m.fit(Xtr,ytr)\n",
    "\n",
    "voting = VotingClassifier(\n",
    "    estimators=[(\"rf\",rf),(\"mlp\",mlp),(\"xgb\",xgb)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "voting.fit(Xtr,ytr)\n",
    "\n",
    "pred = voting.predict(Xte)\n",
    "acc = accuracy_score(yte,pred)\n",
    "\n",
    "cm = confusion_matrix(yte,pred)\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "plt.savefig(os.path.join(output_dir,\"confusion_matrix_final.png\"))\n",
    "plt.close()\n",
    "\n",
    "joblib.dump(voting, os.path.join(output_dir,\"model_voting.joblib\"))\n",
    "\n",
    "print(\"FINAL ACCURACY:\", acc)\n",
    "print(\"PIPELINE TERMIN√â ‚úÖ\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
