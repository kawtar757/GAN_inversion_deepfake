{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1a24dd",
   "metadata": {},
   "source": [
    "\n",
    "# üé•‚ÜíüñºÔ∏è‚Üíüß¨‚Üíüìà‚Üíü§ñ Deepfake vs R√©el ‚Äî Pipeline Latent + ML (Notebook Complet)\n",
    "\n",
    "Ce notebook couvre **de bout en bout** :\n",
    "1) Extraction de frames depuis des vid√©os  \n",
    "2) Calcul de **vecteurs latents** par *GAN Inversion* (fonction √† brancher)  \n",
    "3) **Analyse de l‚Äôespace latent** (PCA / t‚ÄëSNE / UMAP‚Ä†) + stats (moyenne/variance) + distances/clustering  \n",
    "4) **Apprentissage** (SVM / MLP / RandomForest) + √©valuation + export du mod√®le  \n",
    "5) **Pr√©diction** sur nouvelle vid√©o ou image\n",
    "> ‚Ä†UMAP n√©cessite `umap-learn`. Si non install√©, le code le contournera automatiquement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabd55b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c651226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acffe8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from encoder4editing.models.psp import pSp\n",
    "from encoder4editing.utils.common import tensor2im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48412200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Dossiers pr√™ts: C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\artifacts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Chemins de donn√©es (adapte si besoin) ---\n",
    "from pathlib import Path\n",
    "\n",
    "# Dossiers d'images (FAKE / REAL) d√©j√† extraites ou pr√™tes\n",
    "folders = [\n",
    "    r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\",\n",
    "    r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "]\n",
    "\n",
    "# Dossiers de travail (cr√©√©s automatiquement)\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "FRAMES_DIR = ARTIFACTS_DIR / \"frames\"\n",
    "LATENTS_DIR = ARTIFACTS_DIR / \"latents\"\n",
    "PLOTS_DIR = ARTIFACTS_DIR / \"plots\"\n",
    "MODELS_DIR = ARTIFACTS_DIR / \"models\"\n",
    "\n",
    "for d in [ARTIFACTS_DIR, FRAMES_DIR, LATENTS_DIR, PLOTS_DIR, MODELS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[OK] Dossiers pr√™ts:\", ARTIFACTS_DIR.resolve())\n",
    "\n",
    "# Param√®tres g√©n√©raux\n",
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "LATENT_DIM = 512   # Adapter selon votre mod√®le d'inversion (StyleGAN: souvent 512)\n",
    "MAX_FRAMES_PER_VIDEO = 200  # pour limiter le nombre de frames (modifie selon puissance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec1bc0",
   "metadata": {},
   "source": [
    "## üì¶ D√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d887b40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] numpy d√©j√† install√©.\n",
      "[OK] pillow d√©j√† install√©.\n",
      "[OK] opencv-python d√©j√† install√©.\n",
      "[OK] scikit-learn d√©j√† install√©.\n",
      "[OK] matplotlib d√©j√† install√©.\n",
      "[OK] umap-learn d√©j√† install√©.\n",
      "[OK] joblib d√©j√† install√©.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg, import_name=None):\n",
    "    name = import_name or pkg\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "        print(f\"[OK] {pkg} d√©j√† install√©.\")\n",
    "    except ImportError:\n",
    "        print(f\"[INFO] Installation de {pkg} ...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        importlib.import_module(name)\n",
    "        print(f\"[OK] {pkg} install√©.\")\n",
    "\n",
    "# Essentiels\n",
    "for pkg, imp in [\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"pillow\", \"PIL\"),\n",
    "    (\"opencv-python\", \"cv2\"),\n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"umap-learn\", \"umap\"),  # optionnel, sera g√©r√© par try/except √† l'import r√©elle\n",
    "    (\"joblib\", \"joblib\"),\n",
    "]:\n",
    "    try:\n",
    "        ensure(pkg, imp)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {pkg} non disponible: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1921d3",
   "metadata": {},
   "source": [
    "## üß∞ Utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562d5ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Utils charg√©s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt  # R√®gles: pas de seaborn, pas de couleurs fix√©es\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def list_images_in_folders(folders: List[str]) -> Tuple[list, list]:\n",
    "    \"\"\"Retourne chemins & labels (0=fake,1=real) √† partir de deux dossiers nomm√©s 'fake' et 'real'.\"\"\"\n",
    "    X_paths, y = [], []\n",
    "    labels_map = {\"fake\": 0, \"real\": 1}\n",
    "    for folder in folders:\n",
    "        base = Path(folder)\n",
    "        if not base.exists():\n",
    "            print(f\"[WARN] Dossier inexistant: {base}\")\n",
    "            continue\n",
    "        label = None\n",
    "        # d√©duction robuste du label\n",
    "        parts = str(base).lower().split(os.sep)\n",
    "        if \"fake\" in parts:\n",
    "            label = 0\n",
    "        elif \"real\" in parts:\n",
    "            label = 1\n",
    "        else:\n",
    "            # fallback sur le nom du dossier\n",
    "            label = labels_map.get(base.name.lower(), None)\n",
    "        if label is None:\n",
    "            raise ValueError(f\"Impossible de d√©duire le label pour {base}\")\n",
    "        for fn in base.glob(\"**/*\"):\n",
    "            if fn.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}:\n",
    "                X_paths.append(str(fn))\n",
    "                y.append(label)\n",
    "    return X_paths, y\n",
    "\n",
    "def save_plot(fig, out_path: Path, title: str = \"\"):\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\", dpi=140)\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] Plot enregistr√© ‚Üí {out_path}\")\n",
    "\n",
    "print(\"[OK] Utils charg√©s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f333398",
   "metadata": {},
   "source": [
    "## üéûÔ∏è Extraction de frames depuis des vid√©os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d63153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] id0_id16_0000.mp4: 94 frames\n",
      "[OK] id0_id16_0001.mp4: 61 frames\n",
      "[OK] id0_id16_0002.mp4: 70 frames\n",
      "[OK] id0_id16_0003.mp4: 106 frames\n",
      "[OK] id0_id16_0004.mp4: 66 frames\n",
      "[OK] id0_id16_0005.mp4: 92 frames\n",
      "[OK] id0_id16_0006.mp4: 107 frames\n",
      "[OK] id0_id16_0007.mp4: 96 frames\n",
      "[OK] id0_id16_0008.mp4: 93 frames\n",
      "[OK] id0_id16_0009.mp4: 104 frames\n",
      "[OK] id0_id17_0000.mp4: 94 frames\n",
      "[OK] id0_id17_0001.mp4: 61 frames\n",
      "[OK] id0_id17_0002.mp4: 70 frames\n",
      "[OK] id0_id17_0003.mp4: 106 frames\n",
      "[OK] id0_id17_0005.mp4: 92 frames\n",
      "[OK] id0_id17_0006.mp4: 107 frames\n",
      "[OK] id0_id17_0007.mp4: 96 frames\n",
      "[OK] id0_id17_0009.mp4: 104 frames\n",
      "[OK] id0_id1_0000.mp4: 94 frames\n",
      "[OK] id0_id1_0001.mp4: 61 frames\n",
      "[OK] id0_id1_0002.mp4: 70 frames\n",
      "[OK] id0_id1_0003.mp4: 106 frames\n",
      "[OK] id0_id1_0005.mp4: 92 frames\n",
      "[OK] id0_id1_0006.mp4: 107 frames\n",
      "[OK] id0_id1_0007.mp4: 96 frames\n",
      "[OK] id0_id1_0009.mp4: 104 frames\n",
      "[OK] id0_id2_0000.mp4: 94 frames\n",
      "[OK] id0_id2_0001.mp4: 61 frames\n",
      "[OK] id0_id2_0002.mp4: 70 frames\n",
      "[OK] id0_id2_0003.mp4: 106 frames\n",
      "[OK] id0_id2_0004.mp4: 66 frames\n",
      "[OK] id0_id2_0005.mp4: 92 frames\n",
      "[OK] id0_id2_0006.mp4: 107 frames\n",
      "[OK] id0_id2_0007.mp4: 96 frames\n",
      "[OK] id0_id2_0008.mp4: 93 frames\n",
      "[OK] id0_id2_0009.mp4: 104 frames\n",
      "[OK] id0_id3_0000.mp4: 94 frames\n",
      "[OK] id0_id3_0001.mp4: 61 frames\n",
      "[OK] id0_id3_0002.mp4: 70 frames\n",
      "[OK] id0_id3_0003.mp4: 106 frames\n",
      "[OK] id0_id3_0004.mp4: 66 frames\n",
      "[OK] id0_id3_0005.mp4: 92 frames\n",
      "[OK] id0_id3_0006.mp4: 107 frames\n",
      "[OK] id0_id3_0007.mp4: 96 frames\n",
      "[OK] id0_id3_0008.mp4: 93 frames\n",
      "[OK] id0_id3_0009.mp4: 104 frames\n",
      "[OK] id0_id4_0000.mp4: 94 frames\n",
      "[OK] id0_id4_0001.mp4: 61 frames\n",
      "[OK] id0_id4_0002.mp4: 70 frames\n",
      "[OK] id0_id4_0003.mp4: 106 frames\n",
      "[OK] id0_id4_0004.mp4: 66 frames\n",
      "[OK] id0_id4_0005.mp4: 92 frames\n",
      "[OK] id0_id4_0006.mp4: 107 frames\n",
      "[OK] id0_id4_0009.mp4: 104 frames\n",
      "[OK] id0_id6_0000.mp4: 94 frames\n",
      "[OK] id0_id6_0001.mp4: 61 frames\n",
      "[OK] id0_id6_0002.mp4: 70 frames\n",
      "[OK] id0_id6_0006.mp4: 107 frames\n",
      "[OK] id0_id6_0007.mp4: 96 frames\n",
      "[OK] id0_id6_0009.mp4: 104 frames\n",
      "[OK] id0_id9_0000.mp4: 94 frames\n",
      "[OK] id0_id9_0001.mp4: 61 frames\n",
      "[OK] id0_id9_0002.mp4: 70 frames\n",
      "[OK] id0_id9_0003.mp4: 106 frames\n",
      "[OK] id0_id9_0005.mp4: 92 frames\n",
      "[OK] id0_id9_0006.mp4: 107 frames\n",
      "[OK] id0_id9_0007.mp4: 96 frames\n",
      "[OK] id0_id9_0008.mp4: 93 frames\n",
      "[OK] id0_id9_0009.mp4: 104 frames\n",
      "[OK] id10_id11_0001.mp4: 21 frames\n",
      "[OK] id10_id11_0002.mp4: 65 frames\n",
      "[OK] id10_id11_0003.mp4: 69 frames\n",
      "[OK] id10_id11_0004.mp4: 104 frames\n",
      "[OK] id10_id11_0005.mp4: 64 frames\n",
      "[OK] id10_id11_0006.mp4: 33 frames\n",
      "[OK] id10_id11_0007.mp4: 100 frames\n",
      "[OK] id10_id11_0008.mp4: 76 frames\n",
      "[OK] id10_id12_0001.mp4: 21 frames\n",
      "[OK] id10_id12_0002.mp4: 65 frames\n",
      "[OK] id10_id12_0003.mp4: 69 frames\n",
      "[OK] id10_id12_0004.mp4: 104 frames\n",
      "[OK] id10_id12_0005.mp4: 64 frames\n",
      "[OK] id10_id12_0006.mp4: 33 frames\n",
      "[OK] id10_id12_0007.mp4: 100 frames\n",
      "[OK] id10_id12_0008.mp4: 76 frames\n",
      "[OK] id10_id13_0001.mp4: 21 frames\n",
      "[OK] id10_id13_0002.mp4: 65 frames\n",
      "[OK] id10_id13_0003.mp4: 69 frames\n",
      "[OK] id10_id13_0004.mp4: 104 frames\n",
      "[OK] id10_id13_0005.mp4: 64 frames\n",
      "[OK] id10_id13_0006.mp4: 33 frames\n",
      "[OK] id10_id13_0007.mp4: 100 frames\n",
      "[OK] id10_id13_0008.mp4: 76 frames\n",
      "[OK] id10_id7_0001.mp4: 21 frames\n",
      "[OK] id10_id7_0002.mp4: 65 frames\n",
      "[OK] id10_id7_0003.mp4: 69 frames\n",
      "[OK] id10_id7_0004.mp4: 104 frames\n",
      "[OK] id10_id7_0005.mp4: 64 frames\n",
      "[OK] id10_id7_0006.mp4: 33 frames\n",
      "[OK] id10_id7_0007.mp4: 100 frames\n",
      "[OK] id10_id7_0008.mp4: 76 frames\n",
      "[OK] id10_id7_0009.mp4: 107 frames\n",
      "[OK] id11_id7_0000.mp4: 70 frames\n",
      "[OK] id11_id7_0001.mp4: 89 frames\n",
      "[OK] id11_id7_0002.mp4: 72 frames\n",
      "[OK] id11_id7_0003.mp4: 63 frames\n",
      "[OK] id11_id7_0004.mp4: 68 frames\n",
      "[OK] id11_id7_0005.mp4: 66 frames\n",
      "[OK] id11_id7_0006.mp4: 64 frames\n",
      "[OK] id11_id7_0007.mp4: 64 frames\n",
      "[OK] id11_id7_0008.mp4: 75 frames\n",
      "[OK] id11_id7_0009.mp4: 80 frames\n",
      "[OK] id11_id7_0010.mp4: 66 frames\n",
      "[OK] id12_id10_0001.mp4: 80 frames\n",
      "[OK] id12_id10_0002.mp4: 73 frames\n",
      "[OK] id12_id10_0004.mp4: 68 frames\n",
      "[OK] id12_id10_0005.mp4: 63 frames\n",
      "[OK] id12_id10_0006.mp4: 62 frames\n",
      "[OK] id12_id7_0000.mp4: 76 frames\n",
      "[OK] id12_id7_0001.mp4: 80 frames\n",
      "[OK] id12_id7_0002.mp4: 73 frames\n",
      "[OK] id12_id7_0004.mp4: 68 frames\n",
      "[OK] id12_id7_0005.mp4: 63 frames\n",
      "[OK] id12_id7_0006.mp4: 62 frames\n",
      "[OK] id13_id10_0000.mp4: 66 frames\n",
      "[OK] id13_id10_0001.mp4: 70 frames\n",
      "[OK] id13_id10_0003.mp4: 68 frames\n",
      "[OK] id13_id10_0004.mp4: 71 frames\n",
      "[OK] id13_id10_0005.mp4: 62 frames\n",
      "[OK] id13_id10_0007.mp4: 91 frames\n",
      "[OK] id13_id10_0008.mp4: 92 frames\n",
      "[OK] id13_id10_0011.mp4: 94 frames\n",
      "[OK] id13_id10_0012.mp4: 102 frames\n",
      "[OK] id13_id10_0014.mp4: 78 frames\n",
      "[OK] id13_id10_0015.mp4: 98 frames\n",
      "[OK] id13_id7_0000.mp4: 66 frames\n",
      "[OK] id13_id7_0001.mp4: 70 frames\n",
      "[OK] id13_id7_0002.mp4: 80 frames\n",
      "[OK] id13_id7_0003.mp4: 68 frames\n",
      "[OK] id13_id7_0004.mp4: 71 frames\n",
      "[OK] id13_id7_0005.mp4: 62 frames\n",
      "[OK] id13_id7_0007.mp4: 91 frames\n",
      "[OK] id13_id7_0008.mp4: 92 frames\n",
      "[OK] id13_id7_0009.mp4: 87 frames\n",
      "[OK] id13_id7_0011.mp4: 94 frames\n",
      "[OK] id13_id7_0012.mp4: 102 frames\n",
      "[OK] id13_id7_0015.mp4: 98 frames\n",
      "[OK] id16_id0_0000.mp4: 64 frames\n",
      "[OK] id16_id0_0001.mp4: 64 frames\n",
      "[OK] id16_id0_0002.mp4: 73 frames\n",
      "[OK] id16_id0_0003.mp4: 61 frames\n",
      "[OK] id16_id0_0004.mp4: 82 frames\n",
      "[OK] id16_id0_0005.mp4: 69 frames\n",
      "[OK] id16_id0_0006.mp4: 45 frames\n",
      "[OK] id16_id0_0007.mp4: 57 frames\n",
      "[OK] id16_id0_0009.mp4: 63 frames\n",
      "[OK] id16_id0_0010.mp4: 65 frames\n",
      "[OK] id16_id0_0011.mp4: 55 frames\n",
      "[OK] id16_id0_0012.mp4: 62 frames\n",
      "[OK] id16_id0_0013.mp4: 63 frames\n",
      "[OK] id16_id17_0000.mp4: 64 frames\n",
      "[OK] id16_id17_0001.mp4: 64 frames\n",
      "[OK] id16_id17_0002.mp4: 73 frames\n",
      "[OK] id16_id17_0003.mp4: 61 frames\n",
      "[OK] id16_id17_0004.mp4: 82 frames\n",
      "[OK] id16_id17_0005.mp4: 69 frames\n",
      "[OK] id16_id17_0006.mp4: 45 frames\n",
      "[OK] id16_id17_0007.mp4: 57 frames\n",
      "[OK] id16_id17_0009.mp4: 63 frames\n",
      "[OK] id16_id17_0010.mp4: 65 frames\n",
      "[OK] id16_id17_0011.mp4: 55 frames\n",
      "[OK] id16_id17_0012.mp4: 62 frames\n",
      "[OK] id16_id17_0013.mp4: 63 frames\n",
      "[OK] id16_id1_0000.mp4: 64 frames\n",
      "[OK] id16_id1_0001.mp4: 64 frames\n",
      "[OK] id16_id1_0002.mp4: 73 frames\n",
      "[OK] id16_id1_0003.mp4: 61 frames\n",
      "[OK] id16_id1_0004.mp4: 82 frames\n",
      "[OK] id16_id1_0005.mp4: 69 frames\n",
      "[OK] id16_id1_0006.mp4: 45 frames\n",
      "[OK] id16_id1_0007.mp4: 57 frames\n",
      "[OK] id16_id1_0009.mp4: 63 frames\n",
      "[OK] id16_id1_0010.mp4: 65 frames\n",
      "[OK] id16_id1_0011.mp4: 55 frames\n",
      "[OK] id16_id1_0012.mp4: 62 frames\n",
      "[OK] id16_id1_0013.mp4: 63 frames\n",
      "[OK] id16_id2_0000.mp4: 64 frames\n",
      "[OK] id16_id2_0001.mp4: 64 frames\n",
      "[OK] id16_id2_0002.mp4: 73 frames\n",
      "[OK] id16_id2_0003.mp4: 61 frames\n",
      "[OK] id16_id2_0004.mp4: 82 frames\n",
      "[OK] id16_id2_0005.mp4: 69 frames\n",
      "[OK] id16_id2_0006.mp4: 45 frames\n",
      "[OK] id16_id2_0007.mp4: 57 frames\n",
      "[OK] id16_id2_0009.mp4: 63 frames\n",
      "[OK] id16_id2_0010.mp4: 65 frames\n",
      "[OK] id16_id2_0011.mp4: 55 frames\n",
      "[OK] id16_id2_0012.mp4: 62 frames\n",
      "[OK] id16_id2_0013.mp4: 63 frames\n",
      "[OK] id16_id3_0000.mp4: 64 frames\n",
      "[OK] id16_id3_0001.mp4: 64 frames\n",
      "[OK] id16_id3_0002.mp4: 73 frames\n",
      "[OK] id16_id3_0003.mp4: 61 frames\n",
      "[OK] id16_id3_0004.mp4: 82 frames\n",
      "[OK] id16_id3_0005.mp4: 69 frames\n",
      "[OK] id16_id3_0006.mp4: 45 frames\n",
      "[OK] id16_id3_0007.mp4: 57 frames\n",
      "[OK] id16_id3_0009.mp4: 63 frames\n",
      "[OK] id16_id3_0010.mp4: 65 frames\n",
      "[OK] id16_id3_0011.mp4: 55 frames\n",
      "[OK] id16_id3_0012.mp4: 62 frames\n",
      "[OK] id16_id3_0013.mp4: 63 frames\n",
      "[OK] id16_id6_0000.mp4: 64 frames\n",
      "[OK] id16_id6_0001.mp4: 64 frames\n",
      "[OK] id16_id6_0002.mp4: 73 frames\n",
      "[OK] id16_id6_0003.mp4: 61 frames\n",
      "[OK] id16_id6_0004.mp4: 82 frames\n",
      "[OK] id16_id6_0005.mp4: 69 frames\n",
      "[OK] id16_id6_0006.mp4: 45 frames\n",
      "[OK] id16_id6_0007.mp4: 57 frames\n",
      "[OK] id16_id6_0009.mp4: 63 frames\n",
      "[OK] id16_id6_0010.mp4: 65 frames\n",
      "[OK] id16_id6_0011.mp4: 55 frames\n",
      "[OK] id16_id6_0012.mp4: 62 frames\n",
      "[OK] id16_id6_0013.mp4: 63 frames\n",
      "[OK] id16_id9_0000.mp4: 64 frames\n",
      "[OK] id16_id9_0001.mp4: 64 frames\n",
      "[OK] id16_id9_0002.mp4: 73 frames\n",
      "[OK] id16_id9_0003.mp4: 61 frames\n",
      "[OK] id16_id9_0004.mp4: 82 frames\n",
      "[OK] id16_id9_0005.mp4: 69 frames\n",
      "[OK] id16_id9_0006.mp4: 45 frames\n",
      "[OK] id16_id9_0007.mp4: 57 frames\n",
      "[OK] id16_id9_0009.mp4: 63 frames\n",
      "[OK] id16_id9_0010.mp4: 65 frames\n",
      "[OK] id16_id9_0011.mp4: 55 frames\n",
      "[OK] id16_id9_0012.mp4: 62 frames\n",
      "[OK] id16_id9_0013.mp4: 63 frames\n",
      "[OK] id17_id0_0000.mp4: 61 frames\n",
      "[OK] id17_id0_0001.mp4: 63 frames\n",
      "[OK] id17_id0_0002.mp4: 67 frames\n",
      "[OK] id17_id0_0003.mp4: 50 frames\n",
      "[OK] id17_id0_0004.mp4: 62 frames\n",
      "[OK] id17_id0_0005.mp4: 63 frames\n",
      "[OK] id17_id0_0006.mp4: 41 frames\n",
      "[OK] id17_id0_0007.mp4: 55 frames\n",
      "[OK] id17_id0_0009.mp4: 60 frames\n",
      "[OK] id17_id16_0000.mp4: 61 frames\n",
      "[OK] id17_id16_0001.mp4: 63 frames\n",
      "[OK] id17_id16_0002.mp4: 67 frames\n",
      "[OK] id17_id16_0003.mp4: 50 frames\n",
      "[OK] id17_id16_0004.mp4: 62 frames\n",
      "[OK] id17_id16_0005.mp4: 63 frames\n",
      "[OK] id17_id16_0006.mp4: 41 frames\n",
      "[OK] id17_id16_0007.mp4: 55 frames\n",
      "[OK] id17_id16_0009.mp4: 60 frames\n",
      "[OK] id17_id1_0000.mp4: 61 frames\n",
      "[OK] id17_id1_0001.mp4: 63 frames\n",
      "[OK] id17_id1_0002.mp4: 67 frames\n",
      "[OK] id17_id1_0003.mp4: 50 frames\n",
      "[OK] id17_id1_0004.mp4: 62 frames\n",
      "[OK] id17_id1_0005.mp4: 63 frames\n",
      "[OK] id17_id1_0006.mp4: 41 frames\n",
      "[OK] id17_id1_0007.mp4: 55 frames\n",
      "[OK] id17_id1_0009.mp4: 60 frames\n",
      "[OK] id17_id2_0000.mp4: 61 frames\n",
      "[OK] id17_id2_0001.mp4: 63 frames\n",
      "[OK] id17_id2_0002.mp4: 67 frames\n",
      "[OK] id17_id2_0003.mp4: 50 frames\n",
      "[OK] id17_id2_0004.mp4: 62 frames\n",
      "[OK] id17_id2_0005.mp4: 63 frames\n",
      "[OK] id17_id2_0006.mp4: 41 frames\n",
      "[OK] id17_id2_0007.mp4: 55 frames\n",
      "[OK] id17_id2_0009.mp4: 60 frames\n",
      "[OK] id17_id3_0000.mp4: 61 frames\n",
      "[OK] id17_id3_0001.mp4: 63 frames\n",
      "[OK] id17_id3_0002.mp4: 67 frames\n",
      "[OK] id17_id3_0003.mp4: 50 frames\n",
      "[OK] id17_id3_0004.mp4: 62 frames\n",
      "[OK] id17_id3_0005.mp4: 63 frames\n",
      "[OK] id17_id3_0006.mp4: 41 frames\n",
      "[OK] id17_id3_0007.mp4: 55 frames\n",
      "[OK] id17_id3_0009.mp4: 60 frames\n",
      "[OK] id17_id6_0000.mp4: 61 frames\n",
      "[OK] id17_id6_0001.mp4: 63 frames\n",
      "[OK] id17_id6_0002.mp4: 67 frames\n",
      "[OK] id17_id6_0003.mp4: 50 frames\n",
      "[OK] id17_id6_0004.mp4: 62 frames\n",
      "[OK] id17_id6_0005.mp4: 63 frames\n",
      "[OK] id17_id6_0006.mp4: 41 frames\n",
      "[OK] id17_id6_0007.mp4: 55 frames\n",
      "[OK] id17_id6_0009.mp4: 60 frames\n",
      "[OK] id17_id9_0000.mp4: 61 frames\n",
      "[OK] id17_id9_0001.mp4: 63 frames\n",
      "[OK] id17_id9_0002.mp4: 67 frames\n",
      "[OK] id17_id9_0003.mp4: 50 frames\n",
      "[OK] id17_id9_0004.mp4: 62 frames\n",
      "[OK] id17_id9_0005.mp4: 63 frames\n",
      "[OK] id17_id9_0006.mp4: 41 frames\n",
      "[OK] id17_id9_0007.mp4: 55 frames\n",
      "[OK] id17_id9_0009.mp4: 60 frames\n",
      "[OK] id1_id0_0000.mp4: 73 frames\n",
      "[OK] id1_id0_0001.mp4: 55 frames\n",
      "[OK] id1_id0_0002.mp4: 80 frames\n",
      "[OK] id1_id0_0003.mp4: 91 frames\n",
      "[OK] id1_id0_0004.mp4: 82 frames\n",
      "[OK] id1_id0_0005.mp4: 85 frames\n",
      "[OK] id1_id0_0006.mp4: 101 frames\n",
      "[OK] id1_id0_0007.mp4: 66 frames\n",
      "[OK] id1_id0_0009.mp4: 83 frames\n",
      "[OK] id1_id16_0000.mp4: 73 frames\n",
      "[OK] id1_id16_0001.mp4: 55 frames\n",
      "[OK] id1_id16_0002.mp4: 80 frames\n",
      "[OK] id1_id16_0003.mp4: 91 frames\n",
      "[OK] id1_id16_0004.mp4: 82 frames\n",
      "[OK] id1_id16_0005.mp4: 85 frames\n",
      "[OK] id1_id16_0006.mp4: 101 frames\n",
      "[OK] id1_id16_0007.mp4: 66 frames\n",
      "[OK] id1_id16_0009.mp4: 83 frames\n",
      "[OK] id1_id17_0000.mp4: 73 frames\n",
      "[OK] id1_id17_0001.mp4: 55 frames\n",
      "[OK] id1_id17_0002.mp4: 80 frames\n",
      "[OK] id1_id17_0003.mp4: 91 frames\n",
      "[OK] id1_id17_0004.mp4: 82 frames\n",
      "[OK] id1_id17_0005.mp4: 85 frames\n",
      "[OK] id1_id17_0006.mp4: 101 frames\n",
      "[OK] id1_id17_0007.mp4: 66 frames\n",
      "[OK] id1_id17_0009.mp4: 83 frames\n",
      "[OK] id1_id2_0000.mp4: 73 frames\n",
      "[OK] id1_id2_0001.mp4: 55 frames\n",
      "[OK] id1_id2_0002.mp4: 80 frames\n",
      "[OK] id1_id2_0003.mp4: 91 frames\n",
      "[OK] id1_id2_0004.mp4: 82 frames\n",
      "[OK] id1_id2_0005.mp4: 85 frames\n",
      "[OK] id1_id2_0006.mp4: 101 frames\n",
      "[OK] id1_id2_0007.mp4: 66 frames\n",
      "[OK] id1_id2_0009.mp4: 83 frames\n",
      "[OK] id1_id3_0000.mp4: 73 frames\n",
      "[OK] id1_id3_0001.mp4: 55 frames\n",
      "[OK] id1_id3_0002.mp4: 80 frames\n",
      "[OK] id1_id3_0003.mp4: 91 frames\n",
      "[OK] id1_id3_0004.mp4: 82 frames\n",
      "[OK] id1_id3_0005.mp4: 85 frames\n",
      "[OK] id1_id3_0006.mp4: 101 frames\n",
      "[OK] id1_id3_0007.mp4: 66 frames\n",
      "[OK] id1_id3_0009.mp4: 83 frames\n",
      "[OK] id1_id4_0000.mp4: 73 frames\n",
      "[OK] id1_id4_0001.mp4: 55 frames\n",
      "[OK] id1_id4_0002.mp4: 80 frames\n",
      "[OK] id1_id4_0003.mp4: 91 frames\n",
      "[OK] id1_id4_0004.mp4: 82 frames\n",
      "[OK] id1_id4_0005.mp4: 85 frames\n",
      "[OK] id1_id4_0006.mp4: 101 frames\n",
      "[OK] id1_id4_0007.mp4: 66 frames\n",
      "[OK] id1_id4_0009.mp4: 83 frames\n",
      "[OK] id1_id6_0000.mp4: 73 frames\n",
      "[OK] id1_id6_0001.mp4: 55 frames\n",
      "[OK] id1_id6_0002.mp4: 80 frames\n",
      "[OK] id1_id6_0003.mp4: 91 frames\n",
      "[OK] id1_id6_0004.mp4: 82 frames\n",
      "[OK] id1_id6_0005.mp4: 85 frames\n",
      "[OK] id1_id6_0006.mp4: 101 frames\n",
      "[OK] id1_id6_0007.mp4: 66 frames\n",
      "[OK] id1_id6_0009.mp4: 83 frames\n",
      "[OK] id1_id9_0000.mp4: 73 frames\n",
      "[OK] id1_id9_0001.mp4: 55 frames\n",
      "[OK] id1_id9_0002.mp4: 80 frames\n",
      "[OK] id1_id9_0003.mp4: 91 frames\n",
      "[OK] id1_id9_0004.mp4: 82 frames\n",
      "[OK] id1_id9_0005.mp4: 85 frames\n",
      "[OK] id1_id9_0006.mp4: 101 frames\n",
      "[OK] id1_id9_0007.mp4: 66 frames\n",
      "[OK] id1_id9_0009.mp4: 83 frames\n",
      "[OK] id2_id0_0000.mp4: 89 frames\n",
      "[OK] id2_id0_0001.mp4: 76 frames\n",
      "[OK] id2_id0_0002.mp4: 73 frames\n",
      "[OK] id2_id0_0003.mp4: 77 frames\n",
      "[OK] id2_id0_0004.mp4: 85 frames\n",
      "[OK] id2_id0_0005.mp4: 62 frames\n",
      "[OK] id2_id0_0006.mp4: 76 frames\n",
      "[OK] id2_id0_0007.mp4: 94 frames\n",
      "[OK] id2_id0_0008.mp4: 104 frames\n",
      "[OK] id2_id0_0009.mp4: 65 frames\n",
      "[OK] id2_id16_0000.mp4: 89 frames\n",
      "[OK] id2_id16_0001.mp4: 76 frames\n",
      "[OK] id2_id16_0002.mp4: 73 frames\n",
      "[OK] id2_id16_0003.mp4: 77 frames\n",
      "[OK] id2_id16_0004.mp4: 85 frames\n",
      "[OK] id2_id16_0005.mp4: 62 frames\n",
      "[OK] id2_id16_0006.mp4: 76 frames\n",
      "[OK] id2_id16_0007.mp4: 94 frames\n",
      "[OK] id2_id16_0008.mp4: 104 frames\n",
      "[OK] id2_id16_0009.mp4: 65 frames\n",
      "[OK] id2_id17_0000.mp4: 89 frames\n",
      "[OK] id2_id17_0001.mp4: 76 frames\n",
      "[OK] id2_id17_0002.mp4: 73 frames\n",
      "[OK] id2_id17_0003.mp4: 77 frames\n",
      "[OK] id2_id17_0004.mp4: 85 frames\n",
      "[OK] id2_id17_0005.mp4: 62 frames\n",
      "[OK] id2_id17_0006.mp4: 76 frames\n",
      "[OK] id2_id17_0007.mp4: 94 frames\n",
      "[OK] id2_id17_0008.mp4: 104 frames\n",
      "[OK] id2_id17_0009.mp4: 65 frames\n",
      "[OK] id2_id1_0000.mp4: 89 frames\n",
      "[OK] id2_id1_0001.mp4: 76 frames\n",
      "[OK] id2_id1_0002.mp4: 73 frames\n",
      "[OK] id2_id1_0003.mp4: 77 frames\n",
      "[OK] id2_id1_0004.mp4: 85 frames\n",
      "[OK] id2_id1_0005.mp4: 62 frames\n",
      "[OK] id2_id1_0006.mp4: 76 frames\n",
      "[OK] id2_id1_0007.mp4: 94 frames\n",
      "[OK] id2_id1_0008.mp4: 104 frames\n",
      "[OK] id2_id1_0009.mp4: 65 frames\n",
      "[OK] id2_id3_0000.mp4: 89 frames\n",
      "[OK] id2_id3_0001.mp4: 76 frames\n",
      "[OK] id2_id3_0002.mp4: 73 frames\n",
      "[OK] id2_id3_0003.mp4: 77 frames\n",
      "[OK] id2_id3_0004.mp4: 85 frames\n",
      "[OK] id2_id3_0005.mp4: 62 frames\n",
      "[OK] id2_id3_0006.mp4: 76 frames\n",
      "[OK] id2_id3_0007.mp4: 94 frames\n",
      "[OK] id2_id3_0008.mp4: 104 frames\n",
      "[OK] id2_id3_0009.mp4: 65 frames\n",
      "[OK] id2_id4_0000.mp4: 89 frames\n",
      "[OK] id2_id4_0001.mp4: 76 frames\n",
      "[OK] id2_id4_0002.mp4: 73 frames\n",
      "[OK] id2_id4_0003.mp4: 77 frames\n",
      "[OK] id2_id4_0006.mp4: 76 frames\n",
      "[OK] id2_id4_0007.mp4: 94 frames\n",
      "[OK] id2_id4_0008.mp4: 104 frames\n",
      "[OK] id2_id4_0009.mp4: 65 frames\n",
      "[OK] id2_id6_0001.mp4: 76 frames\n",
      "[OK] id2_id6_0002.mp4: 73 frames\n",
      "[OK] id2_id6_0003.mp4: 77 frames\n",
      "[OK] id2_id6_0004.mp4: 85 frames\n",
      "[OK] id2_id6_0005.mp4: 62 frames\n",
      "[OK] id2_id6_0006.mp4: 76 frames\n",
      "[OK] id2_id6_0007.mp4: 94 frames\n",
      "[OK] id2_id6_0008.mp4: 104 frames\n",
      "[OK] id2_id6_0009.mp4: 65 frames\n",
      "[OK] id2_id9_0000.mp4: 89 frames\n",
      "[OK] id2_id9_0001.mp4: 76 frames\n",
      "[OK] id2_id9_0002.mp4: 73 frames\n",
      "[OK] id2_id9_0003.mp4: 77 frames\n",
      "[OK] id2_id9_0004.mp4: 85 frames\n",
      "[OK] id2_id9_0005.mp4: 62 frames\n",
      "[OK] id2_id9_0006.mp4: 76 frames\n",
      "[OK] id2_id9_0007.mp4: 94 frames\n",
      "[OK] id2_id9_0008.mp4: 104 frames\n",
      "[OK] id2_id9_0009.mp4: 65 frames\n",
      "[OK] id3_id0_0000.mp4: 92 frames\n",
      "[OK] id3_id0_0001.mp4: 76 frames\n",
      "[OK] id3_id0_0002.mp4: 86 frames\n",
      "[OK] id3_id0_0003.mp4: 75 frames\n",
      "[OK] id3_id0_0004.mp4: 100 frames\n",
      "[OK] id3_id0_0005.mp4: 80 frames\n",
      "[OK] id3_id0_0006.mp4: 82 frames\n",
      "[OK] id3_id0_0007.mp4: 73 frames\n",
      "[OK] id3_id0_0008.mp4: 92 frames\n",
      "[OK] id3_id0_0009.mp4: 100 frames\n",
      "[OK] id3_id16_0000.mp4: 92 frames\n",
      "[OK] id3_id16_0001.mp4: 76 frames\n",
      "[OK] id3_id16_0002.mp4: 86 frames\n",
      "[OK] id3_id16_0003.mp4: 75 frames\n",
      "[OK] id3_id16_0004.mp4: 100 frames\n",
      "[OK] id3_id16_0005.mp4: 80 frames\n",
      "[OK] id3_id16_0006.mp4: 82 frames\n",
      "[OK] id3_id16_0008.mp4: 92 frames\n",
      "[OK] id3_id16_0009.mp4: 100 frames\n",
      "[OK] id3_id17_0000.mp4: 92 frames\n",
      "[OK] id3_id17_0001.mp4: 76 frames\n",
      "[OK] id3_id17_0002.mp4: 86 frames\n",
      "[OK] id3_id17_0003.mp4: 75 frames\n",
      "[OK] id3_id17_0004.mp4: 100 frames\n",
      "[OK] id3_id17_0005.mp4: 80 frames\n",
      "[OK] id3_id17_0006.mp4: 82 frames\n",
      "[OK] id3_id17_0007.mp4: 73 frames\n",
      "[OK] id3_id17_0008.mp4: 92 frames\n",
      "[OK] id3_id17_0009.mp4: 100 frames\n",
      "[OK] id3_id1_0001.mp4: 76 frames\n",
      "[OK] id3_id1_0002.mp4: 86 frames\n",
      "[OK] id3_id1_0003.mp4: 75 frames\n",
      "[OK] id3_id1_0004.mp4: 100 frames\n",
      "[OK] id3_id1_0005.mp4: 80 frames\n",
      "[OK] id3_id1_0006.mp4: 82 frames\n",
      "[OK] id3_id1_0007.mp4: 73 frames\n",
      "[OK] id3_id1_0008.mp4: 92 frames\n",
      "[OK] id3_id1_0009.mp4: 100 frames\n",
      "[OK] id3_id2_0000.mp4: 92 frames\n",
      "[OK] id3_id2_0001.mp4: 76 frames\n",
      "[OK] id3_id2_0002.mp4: 86 frames\n",
      "[OK] id3_id2_0003.mp4: 75 frames\n",
      "[OK] id3_id2_0004.mp4: 100 frames\n",
      "[OK] id3_id2_0005.mp4: 80 frames\n",
      "[OK] id3_id2_0006.mp4: 82 frames\n",
      "[OK] id3_id2_0007.mp4: 73 frames\n",
      "[OK] id3_id2_0008.mp4: 92 frames\n",
      "[OK] id3_id2_0009.mp4: 100 frames\n",
      "[OK] id3_id4_0001.mp4: 76 frames\n",
      "[OK] id3_id4_0003.mp4: 75 frames\n",
      "[OK] id3_id4_0004.mp4: 100 frames\n",
      "[OK] id3_id4_0005.mp4: 80 frames\n",
      "[OK] id3_id4_0006.mp4: 82 frames\n",
      "[OK] id3_id4_0008.mp4: 92 frames\n",
      "[OK] id3_id6_0000.mp4: 92 frames\n",
      "[OK] id3_id6_0001.mp4: 76 frames\n",
      "[OK] id3_id6_0002.mp4: 86 frames\n",
      "[OK] id3_id6_0003.mp4: 75 frames\n",
      "[OK] id3_id6_0004.mp4: 100 frames\n",
      "[OK] id3_id6_0005.mp4: 80 frames\n",
      "[OK] id3_id6_0006.mp4: 82 frames\n",
      "[OK] id3_id6_0007.mp4: 73 frames\n",
      "[OK] id3_id6_0008.mp4: 92 frames\n",
      "[OK] id3_id6_0009.mp4: 100 frames\n",
      "[OK] id3_id9_0000.mp4: 92 frames\n",
      "[OK] id3_id9_0001.mp4: 76 frames\n",
      "[OK] id3_id9_0002.mp4: 86 frames\n",
      "[OK] id3_id9_0003.mp4: 75 frames\n",
      "[OK] id3_id9_0004.mp4: 100 frames\n",
      "[OK] id3_id9_0005.mp4: 80 frames\n",
      "[OK] id3_id9_0006.mp4: 82 frames\n",
      "[OK] id3_id9_0007.mp4: 73 frames\n",
      "[OK] id3_id9_0008.mp4: 92 frames\n",
      "[OK] id3_id9_0009.mp4: 100 frames\n",
      "[OK] id4_id0_0000.mp4: 96 frames\n",
      "[OK] id4_id0_0001.mp4: 92 frames\n",
      "[OK] id4_id0_0002.mp4: 99 frames\n",
      "[OK] id4_id0_0003.mp4: 89 frames\n",
      "[OK] id4_id0_0004.mp4: 88 frames\n",
      "[OK] id4_id0_0005.mp4: 94 frames\n",
      "[OK] id4_id0_0006.mp4: 61 frames\n",
      "[OK] id4_id0_0007.mp4: 90 frames\n",
      "[OK] id4_id0_0008.mp4: 66 frames\n",
      "[OK] id4_id0_0009.mp4: 97 frames\n",
      "[OK] id4_id1_0000.mp4: 96 frames\n",
      "[OK] id4_id1_0001.mp4: 92 frames\n",
      "[OK] id4_id1_0002.mp4: 99 frames\n",
      "[OK] id4_id1_0003.mp4: 89 frames\n",
      "[OK] id4_id1_0004.mp4: 88 frames\n",
      "[OK] id4_id1_0005.mp4: 94 frames\n",
      "[OK] id4_id1_0006.mp4: 61 frames\n",
      "[OK] id4_id1_0007.mp4: 90 frames\n",
      "[OK] id4_id1_0008.mp4: 66 frames\n",
      "[OK] id4_id1_0009.mp4: 97 frames\n",
      "[OK] id4_id2_0000.mp4: 96 frames\n",
      "[OK] id4_id2_0001.mp4: 92 frames\n",
      "[OK] id4_id2_0002.mp4: 99 frames\n",
      "[OK] id4_id2_0003.mp4: 89 frames\n",
      "[OK] id4_id2_0004.mp4: 88 frames\n",
      "[OK] id4_id2_0005.mp4: 94 frames\n",
      "[OK] id4_id2_0006.mp4: 61 frames\n",
      "[OK] id4_id2_0007.mp4: 90 frames\n",
      "[OK] id4_id2_0008.mp4: 66 frames\n",
      "[OK] id4_id2_0009.mp4: 97 frames\n",
      "[OK] id4_id3_0000.mp4: 96 frames\n",
      "[OK] id4_id3_0001.mp4: 92 frames\n",
      "[OK] id4_id3_0002.mp4: 99 frames\n",
      "[OK] id4_id3_0003.mp4: 89 frames\n",
      "[OK] id4_id3_0004.mp4: 88 frames\n",
      "[OK] id4_id3_0005.mp4: 94 frames\n",
      "[OK] id4_id3_0006.mp4: 61 frames\n",
      "[OK] id4_id3_0007.mp4: 90 frames\n",
      "[OK] id4_id3_0008.mp4: 66 frames\n",
      "[OK] id4_id3_0009.mp4: 97 frames\n",
      "[OK] id4_id6_0000.mp4: 96 frames\n",
      "[OK] id4_id6_0001.mp4: 92 frames\n",
      "[OK] id4_id6_0002.mp4: 99 frames\n",
      "[OK] id4_id6_0003.mp4: 89 frames\n",
      "[OK] id4_id6_0004.mp4: 88 frames\n",
      "[OK] id4_id6_0005.mp4: 94 frames\n",
      "[OK] id4_id6_0006.mp4: 61 frames\n",
      "[OK] id4_id6_0007.mp4: 90 frames\n",
      "[OK] id4_id6_0008.mp4: 66 frames\n",
      "[OK] id4_id6_0009.mp4: 97 frames\n",
      "[OK] id4_id9_0000.mp4: 96 frames\n",
      "[OK] id4_id9_0001.mp4: 92 frames\n",
      "[OK] id4_id9_0002.mp4: 99 frames\n",
      "[OK] id4_id9_0003.mp4: 89 frames\n",
      "[OK] id4_id9_0004.mp4: 88 frames\n",
      "[OK] id4_id9_0005.mp4: 94 frames\n",
      "[OK] id4_id9_0006.mp4: 61 frames\n",
      "[OK] id4_id9_0007.mp4: 90 frames\n",
      "[OK] id4_id9_0008.mp4: 66 frames\n",
      "[OK] id6_id0_0000.mp4: 126 frames\n",
      "[OK] id6_id0_0001.mp4: 65 frames\n",
      "[OK] id6_id0_0002.mp4: 123 frames\n",
      "[OK] id6_id0_0003.mp4: 75 frames\n",
      "[OK] id6_id0_0004.mp4: 92 frames\n",
      "[OK] id6_id0_0005.mp4: 75 frames\n",
      "[OK] id6_id0_0006.mp4: 59 frames\n",
      "[OK] id6_id0_0007.mp4: 112 frames\n",
      "[OK] id6_id0_0008.mp4: 77 frames\n",
      "[OK] id6_id0_0009.mp4: 106 frames\n",
      "[OK] id6_id16_0000.mp4: 126 frames\n",
      "[OK] id6_id16_0001.mp4: 65 frames\n",
      "[OK] id6_id16_0002.mp4: 123 frames\n",
      "[OK] id6_id16_0003.mp4: 75 frames\n",
      "[OK] id6_id16_0004.mp4: 92 frames\n",
      "[OK] id6_id16_0005.mp4: 75 frames\n",
      "[OK] id6_id16_0006.mp4: 59 frames\n",
      "[OK] id6_id16_0007.mp4: 112 frames\n",
      "[OK] id6_id16_0008.mp4: 77 frames\n",
      "[OK] id6_id16_0009.mp4: 106 frames\n",
      "[OK] id6_id17_0000.mp4: 126 frames\n",
      "[OK] id6_id17_0001.mp4: 65 frames\n",
      "[OK] id6_id17_0002.mp4: 123 frames\n",
      "[OK] id6_id17_0003.mp4: 75 frames\n",
      "[OK] id6_id17_0004.mp4: 92 frames\n",
      "[OK] id6_id17_0005.mp4: 75 frames\n",
      "[OK] id6_id17_0006.mp4: 59 frames\n",
      "[OK] id6_id17_0007.mp4: 112 frames\n",
      "[OK] id6_id17_0008.mp4: 77 frames\n",
      "[OK] id6_id17_0009.mp4: 106 frames\n",
      "[OK] id6_id1_0000.mp4: 126 frames\n",
      "[OK] id6_id1_0001.mp4: 65 frames\n",
      "[OK] id6_id1_0002.mp4: 123 frames\n",
      "[OK] id6_id1_0003.mp4: 75 frames\n",
      "[OK] id6_id1_0004.mp4: 92 frames\n",
      "[OK] id6_id1_0005.mp4: 75 frames\n",
      "[OK] id6_id1_0006.mp4: 59 frames\n",
      "[OK] id6_id1_0007.mp4: 112 frames\n",
      "[OK] id6_id1_0008.mp4: 77 frames\n",
      "[OK] id6_id1_0009.mp4: 106 frames\n",
      "[OK] id6_id2_0000.mp4: 126 frames\n",
      "[OK] id6_id2_0001.mp4: 65 frames\n",
      "[OK] id6_id2_0002.mp4: 123 frames\n",
      "[OK] id6_id2_0003.mp4: 75 frames\n",
      "[OK] id6_id2_0004.mp4: 92 frames\n",
      "[OK] id6_id2_0005.mp4: 75 frames\n",
      "[OK] id6_id2_0006.mp4: 59 frames\n",
      "[OK] id6_id2_0007.mp4: 112 frames\n",
      "[OK] id6_id2_0008.mp4: 77 frames\n",
      "[OK] id6_id2_0009.mp4: 106 frames\n",
      "[OK] id6_id3_0000.mp4: 126 frames\n",
      "[OK] id6_id3_0001.mp4: 65 frames\n",
      "[OK] id6_id3_0002.mp4: 123 frames\n",
      "[OK] id6_id3_0003.mp4: 75 frames\n",
      "[OK] id6_id3_0004.mp4: 92 frames\n",
      "[OK] id6_id3_0005.mp4: 75 frames\n",
      "[OK] id6_id3_0006.mp4: 59 frames\n",
      "[OK] id6_id3_0007.mp4: 112 frames\n",
      "[OK] id6_id3_0008.mp4: 77 frames\n",
      "[OK] id6_id3_0009.mp4: 106 frames\n",
      "[OK] id6_id4_0001.mp4: 65 frames\n",
      "[OK] id6_id4_0004.mp4: 92 frames\n",
      "[OK] id6_id4_0005.mp4: 75 frames\n",
      "[OK] id6_id4_0006.mp4: 59 frames\n",
      "[OK] id6_id4_0007.mp4: 112 frames\n",
      "[OK] id6_id4_0008.mp4: 77 frames\n",
      "[OK] id6_id4_0009.mp4: 106 frames\n",
      "[OK] id6_id9_0000.mp4: 126 frames\n",
      "[OK] id6_id9_0001.mp4: 65 frames\n",
      "[OK] id6_id9_0002.mp4: 123 frames\n",
      "[OK] id6_id9_0003.mp4: 75 frames\n",
      "[OK] id6_id9_0004.mp4: 92 frames\n",
      "[OK] id6_id9_0005.mp4: 75 frames\n",
      "[OK] id6_id9_0006.mp4: 59 frames\n",
      "[OK] id6_id9_0007.mp4: 112 frames\n",
      "[OK] id6_id9_0008.mp4: 77 frames\n",
      "[OK] id6_id9_0009.mp4: 106 frames\n",
      "[OK] id7_id10_0001.mp4: 59 frames\n",
      "[OK] id7_id10_0002.mp4: 54 frames\n",
      "[OK] id7_id10_0003.mp4: 111 frames\n",
      "[OK] id7_id10_0004.mp4: 148 frames\n",
      "[OK] id7_id10_0005.mp4: 33 frames\n",
      "[OK] id7_id10_0007.mp4: 83 frames\n",
      "[OK] id7_id10_0009.mp4: 92 frames\n",
      "[OK] id7_id11_0000.mp4: 56 frames\n",
      "[OK] id7_id11_0001.mp4: 59 frames\n",
      "[OK] id7_id11_0002.mp4: 54 frames\n",
      "[OK] id7_id11_0004.mp4: 148 frames\n",
      "[OK] id7_id11_0005.mp4: 33 frames\n",
      "[OK] id7_id11_0006.mp4: 43 frames\n",
      "[OK] id7_id11_0007.mp4: 83 frames\n",
      "[OK] id7_id11_0009.mp4: 92 frames\n",
      "[OK] id7_id12_0000.mp4: 56 frames\n",
      "[OK] id7_id12_0002.mp4: 54 frames\n",
      "[OK] id7_id12_0004.mp4: 148 frames\n",
      "[OK] id7_id12_0005.mp4: 33 frames\n",
      "[OK] id7_id12_0007.mp4: 83 frames\n",
      "[OK] id7_id12_0009.mp4: 92 frames\n",
      "[OK] id7_id13_0000.mp4: 56 frames\n",
      "[OK] id7_id13_0001.mp4: 59 frames\n",
      "[OK] id7_id13_0002.mp4: 54 frames\n",
      "[OK] id7_id13_0004.mp4: 148 frames\n",
      "[OK] id7_id13_0005.mp4: 33 frames\n",
      "[OK] id7_id13_0006.mp4: 43 frames\n",
      "[OK] id7_id13_0007.mp4: 83 frames\n",
      "[OK] id7_id13_0009.mp4: 92 frames\n",
      "[OK] id8_id0_0002.mp4: 105 frames\n",
      "[OK] id8_id0_0003.mp4: 80 frames\n",
      "[OK] id8_id0_0007.mp4: 78 frames\n",
      "[OK] id8_id0_0008.mp4: 114 frames\n",
      "[OK] id8_id1_0002.mp4: 105 frames\n",
      "[OK] id8_id1_0003.mp4: 80 frames\n",
      "[OK] id8_id1_0007.mp4: 78 frames\n",
      "[OK] id8_id1_0008.mp4: 114 frames\n",
      "[OK] id8_id2_0002.mp4: 105 frames\n",
      "[OK] id8_id2_0003.mp4: 80 frames\n",
      "[OK] id8_id2_0007.mp4: 78 frames\n",
      "[OK] id8_id2_0008.mp4: 114 frames\n",
      "[OK] id8_id3_0002.mp4: 105 frames\n",
      "[OK] id8_id3_0003.mp4: 80 frames\n",
      "[OK] id8_id3_0007.mp4: 78 frames\n",
      "[OK] id8_id3_0008.mp4: 114 frames\n",
      "[OK] id8_id4_0007.mp4: 78 frames\n",
      "[OK] id8_id5_0002.mp4: 105 frames\n",
      "[OK] id8_id5_0003.mp4: 80 frames\n",
      "[OK] id8_id5_0007.mp4: 78 frames\n",
      "[OK] id8_id5_0008.mp4: 114 frames\n",
      "[OK] id8_id6_0002.mp4: 105 frames\n",
      "[OK] id8_id6_0003.mp4: 80 frames\n",
      "[OK] id8_id6_0007.mp4: 78 frames\n",
      "[OK] id8_id6_0008.mp4: 114 frames\n",
      "[OK] id8_id7_0002.mp4: 105 frames\n",
      "[OK] id8_id7_0003.mp4: 80 frames\n",
      "[OK] id8_id7_0007.mp4: 78 frames\n",
      "[OK] id8_id7_0008.mp4: 114 frames\n",
      "[OK] id8_id9_0002.mp4: 105 frames\n",
      "[OK] id8_id9_0003.mp4: 80 frames\n",
      "[OK] id8_id9_0007.mp4: 78 frames\n",
      "[OK] id8_id9_0008.mp4: 114 frames\n",
      "[OK] id9_id0_0000.mp4: 90 frames\n",
      "[OK] id9_id0_0001.mp4: 88 frames\n",
      "[OK] id9_id0_0003.mp4: 99 frames\n",
      "[OK] id9_id0_0004.mp4: 73 frames\n",
      "[OK] id9_id0_0005.mp4: 84 frames\n",
      "[OK] id9_id0_0006.mp4: 81 frames\n",
      "[OK] id9_id0_0007.mp4: 92 frames\n",
      "[OK] id9_id0_0008.mp4: 93 frames\n",
      "[OK] id9_id0_0009.mp4: 63 frames\n",
      "[OK] id9_id16_0000.mp4: 90 frames\n",
      "[OK] id9_id16_0001.mp4: 88 frames\n",
      "[OK] id9_id16_0002.mp4: 73 frames\n",
      "[OK] id9_id16_0003.mp4: 99 frames\n",
      "[OK] id9_id16_0004.mp4: 73 frames\n",
      "[OK] id9_id16_0005.mp4: 84 frames\n",
      "[OK] id9_id16_0006.mp4: 81 frames\n",
      "[OK] id9_id16_0007.mp4: 92 frames\n",
      "[OK] id9_id16_0008.mp4: 93 frames\n",
      "[OK] id9_id16_0009.mp4: 63 frames\n",
      "[OK] id9_id17_0000.mp4: 90 frames\n",
      "[OK] id9_id17_0001.mp4: 88 frames\n",
      "[OK] id9_id17_0002.mp4: 73 frames\n",
      "[OK] id9_id17_0003.mp4: 99 frames\n",
      "[OK] id9_id17_0004.mp4: 73 frames\n",
      "[OK] id9_id17_0005.mp4: 84 frames\n",
      "[OK] id9_id17_0006.mp4: 81 frames\n",
      "[OK] id9_id17_0007.mp4: 92 frames\n",
      "[OK] id9_id17_0008.mp4: 93 frames\n",
      "[OK] id9_id17_0009.mp4: 63 frames\n",
      "[OK] id9_id1_0000.mp4: 90 frames\n",
      "[OK] id9_id1_0001.mp4: 88 frames\n",
      "[OK] id9_id1_0002.mp4: 73 frames\n",
      "[OK] id9_id1_0004.mp4: 73 frames\n",
      "[OK] id9_id1_0005.mp4: 84 frames\n",
      "[OK] id9_id1_0006.mp4: 81 frames\n",
      "[OK] id9_id1_0008.mp4: 93 frames\n",
      "[OK] id9_id1_0009.mp4: 63 frames\n",
      "[OK] id9_id2_0000.mp4: 90 frames\n",
      "[OK] id9_id2_0001.mp4: 88 frames\n",
      "[OK] id9_id2_0002.mp4: 73 frames\n",
      "[OK] id9_id2_0004.mp4: 73 frames\n",
      "[OK] id9_id2_0005.mp4: 84 frames\n",
      "[OK] id9_id2_0006.mp4: 81 frames\n",
      "[OK] id9_id2_0007.mp4: 92 frames\n",
      "[OK] id9_id2_0008.mp4: 93 frames\n",
      "[OK] id9_id2_0009.mp4: 63 frames\n",
      "[OK] id9_id3_0000.mp4: 90 frames\n",
      "[OK] id9_id3_0001.mp4: 88 frames\n",
      "[OK] id9_id3_0002.mp4: 73 frames\n",
      "[OK] id9_id3_0004.mp4: 73 frames\n",
      "[OK] id9_id3_0005.mp4: 84 frames\n",
      "[OK] id9_id3_0006.mp4: 81 frames\n",
      "[OK] id9_id3_0007.mp4: 92 frames\n",
      "[OK] id9_id3_0008.mp4: 93 frames\n",
      "[OK] id9_id3_0009.mp4: 63 frames\n",
      "[OK] id9_id4_0000.mp4: 90 frames\n",
      "[OK] id9_id4_0001.mp4: 88 frames\n",
      "[OK] id9_id4_0002.mp4: 73 frames\n",
      "[OK] id9_id4_0004.mp4: 73 frames\n",
      "[OK] id9_id4_0005.mp4: 84 frames\n",
      "[OK] id9_id4_0006.mp4: 81 frames\n",
      "[OK] id9_id4_0007.mp4: 92 frames\n",
      "[OK] id9_id4_0008.mp4: 93 frames\n",
      "[OK] id9_id4_0009.mp4: 63 frames\n",
      "[OK] id9_id6_0000.mp4: 90 frames\n",
      "[OK] id9_id6_0001.mp4: 88 frames\n",
      "[OK] id9_id6_0002.mp4: 73 frames\n",
      "[OK] id9_id6_0004.mp4: 73 frames\n",
      "[OK] id9_id6_0005.mp4: 84 frames\n",
      "[OK] id9_id6_0006.mp4: 81 frames\n",
      "[OK] id9_id6_0007.mp4: 92 frames\n",
      "[OK] id9_id6_0008.mp4: 93 frames\n",
      "[OK] id9_id6_0009.mp4: 63 frames\n",
      "[OK] id0_0000.mp4: 94 frames\n",
      "[OK] id0_0001.mp4: 61 frames\n",
      "[OK] id0_0002.mp4: 70 frames\n",
      "[OK] id0_0003.mp4: 106 frames\n",
      "[OK] id0_0004.mp4: 66 frames\n",
      "[OK] id0_0005.mp4: 92 frames\n",
      "[OK] id0_0006.mp4: 107 frames\n",
      "[OK] id0_0007.mp4: 96 frames\n",
      "[OK] id0_0008.mp4: 93 frames\n",
      "[OK] id0_0009.mp4: 104 frames\n",
      "[OK] id10_0000.mp4: 48 frames\n",
      "[OK] id10_0001.mp4: 29 frames\n",
      "[OK] id10_0002.mp4: 65 frames\n",
      "[OK] id10_0003.mp4: 71 frames\n",
      "[OK] id10_0004.mp4: 104 frames\n",
      "[OK] id10_0005.mp4: 64 frames\n",
      "[OK] id10_0006.mp4: 35 frames\n",
      "[OK] id10_0007.mp4: 101 frames\n",
      "[OK] id10_0008.mp4: 76 frames\n",
      "[OK] id10_0009.mp4: 107 frames\n",
      "[OK] id11_0000.mp4: 75 frames\n",
      "[OK] id11_0001.mp4: 89 frames\n",
      "[OK] id11_0002.mp4: 72 frames\n",
      "[OK] id11_0003.mp4: 63 frames\n",
      "[OK] id11_0004.mp4: 70 frames\n",
      "[OK] id11_0005.mp4: 66 frames\n",
      "[OK] id11_0006.mp4: 64 frames\n",
      "[OK] id11_0007.mp4: 64 frames\n",
      "[OK] id11_0008.mp4: 75 frames\n",
      "[OK] id11_0009.mp4: 80 frames\n",
      "[OK] id11_0010.mp4: 66 frames\n",
      "[OK] id12_0000.mp4: 76 frames\n",
      "[OK] id12_0001.mp4: 80 frames\n",
      "[OK] id12_0002.mp4: 73 frames\n",
      "[OK] id12_0003.mp4: 76 frames\n",
      "[OK] id12_0004.mp4: 68 frames\n",
      "[OK] id12_0005.mp4: 63 frames\n",
      "[OK] id12_0006.mp4: 62 frames\n",
      "[OK] id13_0000.mp4: 66 frames\n",
      "[OK] id13_0001.mp4: 70 frames\n",
      "[OK] id13_0002.mp4: 80 frames\n",
      "[OK] id13_0003.mp4: 68 frames\n",
      "[OK] id13_0004.mp4: 71 frames\n",
      "[OK] id13_0005.mp4: 62 frames\n",
      "[OK] id13_0006.mp4: 67 frames\n",
      "[OK] id13_0007.mp4: 91 frames\n",
      "[OK] id13_0008.mp4: 92 frames\n",
      "[OK] id13_0009.mp4: 87 frames\n",
      "[OK] id13_0010.mp4: 67 frames\n",
      "[OK] id13_0011.mp4: 94 frames\n",
      "[OK] id13_0012.mp4: 102 frames\n",
      "[OK] id13_0013.mp4: 92 frames\n",
      "[OK] id13_0014.mp4: 78 frames\n",
      "[OK] id13_0015.mp4: 98 frames\n",
      "[OK] id16_0000.mp4: 64 frames\n",
      "[OK] id16_0001.mp4: 64 frames\n",
      "[OK] id16_0002.mp4: 73 frames\n",
      "[OK] id16_0003.mp4: 61 frames\n",
      "[OK] id16_0004.mp4: 82 frames\n",
      "[OK] id16_0005.mp4: 69 frames\n",
      "[OK] id16_0006.mp4: 45 frames\n",
      "[OK] id16_0007.mp4: 57 frames\n",
      "[OK] id16_0008.mp4: 68 frames\n",
      "[OK] id16_0009.mp4: 63 frames\n",
      "[OK] id16_0010.mp4: 65 frames\n",
      "[OK] id16_0011.mp4: 55 frames\n",
      "[OK] id16_0012.mp4: 62 frames\n",
      "[OK] id16_0013.mp4: 63 frames\n",
      "[OK] id17_0000.mp4: 61 frames\n",
      "[OK] id17_0001.mp4: 63 frames\n",
      "[OK] id17_0002.mp4: 67 frames\n",
      "[OK] id17_0003.mp4: 50 frames\n",
      "[OK] id17_0004.mp4: 62 frames\n",
      "[OK] id17_0005.mp4: 63 frames\n",
      "[OK] id17_0006.mp4: 41 frames\n",
      "[OK] id17_0007.mp4: 55 frames\n",
      "[OK] id17_0008.mp4: 54 frames\n",
      "[OK] id17_0009.mp4: 60 frames\n",
      "[OK] id1_0000.mp4: 75 frames\n",
      "[OK] id1_0001.mp4: 56 frames\n",
      "[OK] id1_0002.mp4: 80 frames\n",
      "[OK] id1_0003.mp4: 91 frames\n",
      "[OK] id1_0004.mp4: 82 frames\n",
      "[OK] id1_0005.mp4: 85 frames\n",
      "[OK] id1_0006.mp4: 101 frames\n",
      "[OK] id1_0007.mp4: 66 frames\n",
      "[OK] id1_0008.mp4: 104 frames\n",
      "[OK] id1_0009.mp4: 83 frames\n",
      "[OK] id2_0000.mp4: 89 frames\n",
      "[OK] id2_0001.mp4: 76 frames\n",
      "[OK] id2_0002.mp4: 73 frames\n",
      "[OK] id2_0003.mp4: 77 frames\n",
      "[OK] id2_0004.mp4: 85 frames\n",
      "[OK] id2_0005.mp4: 62 frames\n",
      "[OK] id2_0006.mp4: 76 frames\n",
      "[OK] id2_0007.mp4: 94 frames\n",
      "[OK] id2_0008.mp4: 104 frames\n",
      "[OK] id2_0009.mp4: 65 frames\n",
      "[OK] id3_0000.mp4: 92 frames\n",
      "[OK] id3_0001.mp4: 76 frames\n",
      "[OK] id3_0002.mp4: 86 frames\n",
      "[OK] id3_0003.mp4: 75 frames\n",
      "[OK] id3_0004.mp4: 100 frames\n",
      "[OK] id3_0005.mp4: 80 frames\n",
      "[OK] id3_0006.mp4: 82 frames\n",
      "[OK] id3_0007.mp4: 73 frames\n",
      "[OK] id3_0008.mp4: 92 frames\n",
      "[OK] id3_0009.mp4: 100 frames\n",
      "[OK] id4_0000.mp4: 96 frames\n",
      "[OK] id4_0001.mp4: 92 frames\n",
      "[OK] id4_0002.mp4: 99 frames\n",
      "[OK] id4_0003.mp4: 89 frames\n",
      "[OK] id4_0004.mp4: 88 frames\n",
      "[OK] id4_0005.mp4: 94 frames\n",
      "[OK] id4_0006.mp4: 61 frames\n",
      "[OK] id4_0007.mp4: 90 frames\n",
      "[OK] id4_0008.mp4: 66 frames\n",
      "[OK] id4_0009.mp4: 97 frames\n",
      "[OK] id6_0000.mp4: 126 frames\n",
      "[OK] id6_0001.mp4: 65 frames\n",
      "[OK] id6_0002.mp4: 123 frames\n",
      "[OK] id6_0003.mp4: 75 frames\n",
      "[OK] id6_0004.mp4: 92 frames\n",
      "[OK] id6_0005.mp4: 75 frames\n",
      "[OK] id6_0006.mp4: 59 frames\n",
      "[OK] id6_0007.mp4: 112 frames\n",
      "[OK] id6_0008.mp4: 77 frames\n",
      "[OK] id6_0009.mp4: 106 frames\n",
      "[OK] id7_0000.mp4: 56 frames\n",
      "[OK] id7_0001.mp4: 59 frames\n",
      "[OK] id7_0002.mp4: 54 frames\n",
      "[OK] id7_0003.mp4: 111 frames\n",
      "[OK] id7_0004.mp4: 148 frames\n",
      "[OK] id7_0005.mp4: 33 frames\n",
      "[OK] id7_0006.mp4: 43 frames\n",
      "[OK] id7_0007.mp4: 83 frames\n",
      "[OK] id7_0008.mp4: 59 frames\n",
      "[OK] id7_0009.mp4: 92 frames\n",
      "[OK] id8_0000.mp4: 96 frames\n",
      "[OK] id8_0001.mp4: 92 frames\n",
      "[OK] id8_0002.mp4: 105 frames\n",
      "[OK] id8_0003.mp4: 81 frames\n",
      "[OK] id8_0004.mp4: 114 frames\n",
      "[OK] id8_0005.mp4: 43 frames\n",
      "[OK] id8_0006.mp4: 103 frames\n",
      "[OK] id8_0007.mp4: 78 frames\n",
      "[OK] id8_0008.mp4: 114 frames\n",
      "[OK] id8_0009.mp4: 76 frames\n",
      "[OK] id9_0000.mp4: 90 frames\n",
      "[OK] id9_0001.mp4: 88 frames\n",
      "[OK] id9_0002.mp4: 73 frames\n",
      "[OK] id9_0003.mp4: 99 frames\n",
      "[OK] id9_0004.mp4: 73 frames\n",
      "[OK] id9_0005.mp4: 84 frames\n",
      "[OK] id9_0006.mp4: 81 frames\n",
      "[OK] id9_0007.mp4: 92 frames\n",
      "[OK] id9_0008.mp4: 93 frames\n",
      "[OK] id9_0009.mp4: 63 frames\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "VIDEOS_DIR = Path(r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\")\n",
    "OUT_FRAMES_FAKE = FRAMES_DIR / \"fake\"\n",
    "OUT_FRAMES_REAL = FRAMES_DIR / \"real\"\n",
    "OUT_FRAMES_FAKE.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FRAMES_REAL.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_frames_from_video(video_path: Path, out_dir: Path, max_frames: int = MAX_FRAMES_PER_VIDEO, every_n: int = 5):\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    idx, saved = 0, 0\n",
    "    while True and saved < max_frames:\n",
    "        ret = cap.grab()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % every_n == 0:\n",
    "            ret, frame = cap.retrieve()\n",
    "            if not ret:\n",
    "                break\n",
    "            out_path = out_dir / f\"{video_path.stem}_f{idx:06d}.jpg\"\n",
    "            cv2.imwrite(str(out_path), frame)\n",
    "            saved += 1\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    print(f\"[OK] {video_path.name}: {saved} frames\")\n",
    "\n",
    "# D√©commentez pour extraire si vous avez des vid√©os locales:\n",
    "for label in [\"fake\",\"real\"]:\n",
    "    vdir = VIDEOS_DIR / label\n",
    "    if not vdir.exists(): \n",
    "        continue\n",
    "    for mp in vdir.glob(\"*.mp4\"):\n",
    "        out_dir = OUT_FRAMES_FAKE if label==\"fake\" else OUT_FRAMES_REAL\n",
    "        extract_frames_from_video(mp, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586cb10f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658e343f",
   "metadata": {},
   "source": [
    "##  Inversion GAN ‚Üí Vecteurs latents `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93df665",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'folders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation: on peut choisir d'apprendre sur `folders` (images pr√™tes) \u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ou sur `FRAMES_DIR/fake|real` si vous avez extrait des frames depuis vid√©os.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m DATA_SOURCES \u001b[38;5;241m=\u001b[39m \u001b[43mfolders\u001b[49m  \u001b[38;5;66;03m# ou: [str(OUT_FRAMES_FAKE), str(OUT_FRAMES_REAL)]\u001b[39;00m\n\u001b[0;32m     49\u001b[0m X, y \u001b[38;5;241m=\u001b[39m build_latent_dataset(DATA_SOURCES, cache_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m X\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape, np\u001b[38;5;241m.\u001b[39mbincount(y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'folders' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚ö†Ô∏è √Ä BRANCHER : Remplacez `gan_invert(img)` par votre impl√©mentation.\n",
    "# Par d√©faut, on renvoie un vecteur latent al√©atoire (d√©mo).\n",
    "# Int√©gration typique : pSp/e4e/ReStyle/Encoder StyleGAN ‚Üí np.ndarray de taille LATENT_DIM.\n",
    "\n",
    "def gan_invert(pil_img: Image.Image) -> np.ndarray:\n",
    "    # TODO: Remplacez par votre code d'inversion (retour: np.ndarray shape (LATENT_DIM,))\n",
    "    # Exemple d'API attendue :\n",
    "    #   z = your_encoder.encode(pil_img)  # (LATENT_DIM,)\n",
    "    #   return np.asarray(z, dtype=np.float32)\n",
    "    return np.random.randn(LATENT_DIM).astype(np.float32)\n",
    "\n",
    "def build_latent_dataset(folders_or_frames: list, cache_prefix: str = \"dataset\"):\n",
    "    \"\"\"Calcule/charge X(latents) et y(labels) √† partir de dossiers images.\n",
    "       Sauvegarde X.npy, y.npy pour r√©utiliser rapidement.\"\"\"\n",
    "    X_cache = LATENTS_DIR / f\"{cache_prefix}_X.npy\"\n",
    "    y_cache = LATENTS_DIR / f\"{cache_prefix}_y.npy\"\n",
    "    if X_cache.exists() and y_cache.exists():\n",
    "        X = np.load(X_cache)\n",
    "        y = np.load(y_cache)\n",
    "        print(f\"[OK] Charg√© cache: {X.shape}, labels: {y.shape}\")\n",
    "        return X, y\n",
    "\n",
    "    paths, labels = list_images_in_folders(folders_or_frames)\n",
    "    print(f\"[INFO] Images trouv√©es: {len(paths)}\")\n",
    "    X_list = []\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            z = gan_invert(img)\n",
    "            if z.ndim != 1:\n",
    "                z = z.reshape(-1)\n",
    "            X_list.append(z)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skip {p}: {e}\")\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  ... {i}/{len(paths)}\")\n",
    "\n",
    "    X = np.vstack(X_list).astype(np.float32)\n",
    "    y = np.asarray(labels[:len(X_list)], dtype=np.int64)\n",
    "\n",
    "    np.save(X_cache, X)\n",
    "    np.save(y_cache, y)\n",
    "    print(f\"[OK] Sauvegard√©: {X_cache.name}, {y_cache.name} - Shapes: {X.shape}, {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "# Exemple d'utilisation: on peut choisir d'apprendre sur `folders` (images pr√™tes) \n",
    "# ou sur `FRAMES_DIR/fake|real` si vous avez extrait des frames depuis vid√©os.\n",
    "DATA_SOURCES = folders  # ou: [str(OUT_FRAMES_FAKE), str(OUT_FRAMES_REAL)]\n",
    "X, y = build_latent_dataset(DATA_SOURCES, cache_prefix=\"raw\")\n",
    "X.shape, y.shape, np.bincount(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db21336",
   "metadata": {},
   "source": [
    "## üìà Analyse de l‚Äôespace latent (PCA / t‚ÄëSNE / UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71200e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# UMAP est optionnel\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "    print(\"[INFO] UMAP non disponible (installez `umap-learn` pour l'activer).\")\n",
    "\n",
    "def plot_2d(emb2d, labels, title, out_file):\n",
    "    fig = plt.figure()\n",
    "    xs, ys = emb2d[:,0], emb2d[:,1]\n",
    "    plt.scatter(xs, ys, s=12, alpha=0.8, c=labels)\n",
    "    plt.title(title)\n",
    "    out_path = PLOTS_DIR / out_file\n",
    "    save_plot(fig, out_path, title=title)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=min(50, X.shape[1]))\n",
    "Xp = pca.fit_transform(X)\n",
    "print(\"[PCA] Explained var (10 premi√®res):\", pca.explained_variance_ratio_[:10])\n",
    "plot_2d(Xp[:,:2], y, \"PCA (2D)\", \"pca_2d.png\")\n",
    "\n",
    "# t-SNE\n",
    "Xt = TSNE(n_components=2, random_state=SEED, init=\"random\", learning_rate=\"auto\").fit_transform(X)\n",
    "plot_2d(Xt, y, \"t-SNE (2D)\", \"tsne_2d.png\")\n",
    "\n",
    "# UMAP (si dispo)\n",
    "if HAS_UMAP:\n",
    "    Xu = umap.UMAP(n_components=2, random_state=SEED).fit_transform(X)\n",
    "    plot_2d(Xu, y, \"UMAP (2D)\", \"umap_2d.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fc128",
   "metadata": {},
   "source": [
    "## üìä Statistiques & Distances (intra/inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fac5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def class_stats(X, y, cls):\n",
    "    Xc = X[y==cls]\n",
    "    mu = Xc.mean(axis=0)\n",
    "    var = Xc.var(axis=0)\n",
    "    return Xc, mu, var\n",
    "\n",
    "X_fake, mu_fake, var_fake = class_stats(X, y, 0)\n",
    "X_real, mu_real, var_real = class_stats(X, y, 1)\n",
    "\n",
    "print(\"[FAKE] n=\", len(X_fake), \" mean|var dims:\", mu_fake.shape, var_fake.shape)\n",
    "print(\"[REAL] n=\", len(X_real), \" mean|var dims:\", mu_real.shape, var_real.shape)\n",
    "\n",
    "# Distances intra-classes (moyennes)\n",
    "intra_fake = pairwise_distances(X_fake).mean() if len(X_fake)>1 else float(\"nan\")\n",
    "intra_real = pairwise_distances(X_real).mean() if len(X_real)>1 else float(\"nan\")\n",
    "# Distance inter-classes (entre barycentres)\n",
    "inter_centroids = np.linalg.norm(mu_fake - mu_real)\n",
    "\n",
    "print(f\"Distance intra FAKE: {intra_fake:.4f}\")\n",
    "print(f\"Distance intra REAL: {intra_real:.4f}\")\n",
    "print(f\"Distance entre centro√Ødes FAKE/REAL: {inter_centroids:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04117ee9",
   "metadata": {},
   "source": [
    "## ü§ñ Entra√Ænement ML (SVM / RandomForest / MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bafe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "import joblib\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=SEED, stratify=y)\n",
    "\n",
    "models = {\n",
    "    \"svm_linear\": Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(kernel=\"linear\", probability=True, random_state=SEED))]),\n",
    "    \"svm_rbf\":    Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=SEED))]),\n",
    "    \"rf\":         RandomForestClassifier(n_estimators=300, random_state=SEED),\n",
    "    \"mlp\":        Pipeline([(\"scaler\", StandardScaler()), (\"clf\", MLPClassifier(hidden_layer_sizes=(256,128), max_iter=200, random_state=SEED))]),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "reports = {}\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "        print(f\"[CV] {name}: acc={scores.mean():.3f}¬±{scores.std():.3f}\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        report = classification_report(y_test, y_pred, target_names=[\"FAKE\",\"REEL\"], output_dict=True)\n",
    "        cm = confusion_matrix(y_test, y_pred).tolist()\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else float(\"nan\")\n",
    "\n",
    "        reports[name] = {\"cv_acc_mean\": float(scores.mean()), \"cv_acc_std\": float(scores.std()),\n",
    "                         \"report\": report, \"confusion_matrix\": cm, \"roc_auc\": float(auc)}\n",
    "\n",
    "        # Sauvegarde mod√®le\n",
    "        out_model = MODELS_DIR / f\"{name}.joblib\"\n",
    "        joblib.dump(model, out_model)\n",
    "        print(f\"[OK] Mod√®le sauvegard√© ‚Üí {out_model}\")\n",
    "\n",
    "        # ROC plot (si proba dispo)\n",
    "        if y_proba is not None:\n",
    "            fig = plt.figure()\n",
    "            RocCurveDisplay.from_predictions(y_test, y_proba)\n",
    "            save_plot(fig, PLOTS_DIR / f\"roc_{name}.png\", title=f\"ROC {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {name} a √©chou√©: {e}\")\n",
    "\n",
    "# Sauvegarde des m√©triques\n",
    "with open(MODELS_DIR / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reports, f, indent=2)\n",
    "print(\"[OK] M√©triques sauvegard√©es ‚Üí\", MODELS_DIR / \"metrics.json\")\n",
    "\n",
    "reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194450b",
   "metadata": {},
   "source": [
    "## üéØ Pr√©diction (FAKE vs R√âEL) sur images/vid√©o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b31820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_on_images(image_paths: list, model_path: Path) -> Dict[str, float]:\n",
    "    \"\"\"Charge un mod√®le .joblib et renvoie proba R√©el (1) par image.\"\"\"\n",
    "    import joblib\n",
    "    results = {}\n",
    "    model = joblib.load(model_path)\n",
    "    for p in image_paths:\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            z = gan_invert(img)\n",
    "            if z.ndim != 1:\n",
    "                z = z.reshape(-1)\n",
    "            z = z.reshape(1, -1)\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                proba = float(model.predict_proba(z)[0,1])\n",
    "            else:\n",
    "                # fallback: decision_function -> approx via sigmoid\n",
    "                dec = float(model.decision_function(z)[0])\n",
    "                proba = 1/(1+np.exp(-dec))\n",
    "            results[str(p)] = proba\n",
    "        except Exception as e:\n",
    "            results[str(p)] = f\"ERROR: {e}\"\n",
    "    return results\n",
    "\n",
    "# Exemple d'utilisation:\n",
    "# sample_imgs = [list(Path(folders[0]).glob('*.jpg'))[0], list(Path(folders[1]).glob('*.jpg'))[0]]\n",
    "# preds = predict_on_images(sample_imgs, MODELS_DIR / \"svm_linear.joblib\")\n",
    "# preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943d07f",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Notes & Conseils\n",
    "\n",
    "- **GAN Inversion** : Remplacez `gan_invert(pil_img)` par votre encodeur (pSp/e4e/ReStyle‚Ä¶).\n",
    "- **Normalisation** : Conservez la m√™me pr√©‚Äëproc image (taille, centrage, normalisation) que celle attendue par votre encodeur.\n",
    "- **Latent dimension** : Ajustez `LATENT_DIM` si votre espace latent est `W`, `W+`, `S`, etc.\n",
    "- **√âquilibrage** : Si le dataset est d√©s√©quilibr√©, explorez `class_weight='balanced'` (SVM) ou r√©√©chantillonnage.\n",
    "- **Sauvegardes** : `LATENTS_DIR/*.npy` + `MODELS_DIR/*.joblib` + graphiques dans `PLOTS_DIR/`.\n",
    "- **G√©n√©ralisation** : Validez sur vid√©os/images tenues **hors** du jeu d‚Äôentra√Ænement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ccdee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "CUDA dispo : True\n",
      "Nom GPU : NVIDIA GeForce GTX 1650 Ti with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA dispo :\", torch.cuda.is_available())\n",
    "print(\"Nom GPU :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Aucun GPU d√©tect√©\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b21cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Dossier pour latents\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m LATENTS_DIR \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./latents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m LATENTS_DIR\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# üîß 1) Charger ton mod√®le e4e/pSp\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML + r√©duction dimension\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from argparse import Namespace\n",
    "\n",
    "###############################################\n",
    "# 1) CONFIGURATION\n",
    "###############################################\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "FAKE_DIR = r'C:/Users/EliteLaptop/Desktop/kawtar/GAN_inversion/artifacts/frames/fake'\n",
    "REAL_DIR = r'C:/Users/EliteLaptop/Desktop/kawtar/GAN_inversion/artifacts/frames/real'\n",
    "CKPT_PATH = r'C:/Users/EliteLaptop/Desktop/kawtar/GAN_inversion/encoder4editing/pretrained_models/e4e_ffhq_encode.pt'\n",
    "\n",
    "###############################################\n",
    "# 2) MODEL e4e\n",
    "###############################################\n",
    "\n",
    "from encoder4editing.models.psp import pSp\n",
    "\n",
    "print(\"[INFO] Loading model...\")\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "opts = ckpt['opts']\n",
    "if isinstance(opts, dict):\n",
    "    opts = Namespace(**opts)\n",
    "opts.checkpoint_path = CKPT_PATH\n",
    "\n",
    "model = pSp(opts).to(device).eval()\n",
    "print(\"[OK] Model loaded.\")\n",
    "\n",
    "###############################################\n",
    "# 3) FACE ALIGNMENT (OBLIGATOIRE pour e4e)\n",
    "###############################################\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "mtcnn = MTCNN(\n",
    "    image_size=256,\n",
    "    margin=0,\n",
    "    post_process=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def align_face(pil_img):\n",
    "    \"\"\"Retourne une image align√©e FFHQ-style pour e4e\"\"\"\n",
    "    try:\n",
    "        aligned = mtcnn(pil_img)\n",
    "        if aligned is None:\n",
    "            print(\"[WARN] No face detected.\")\n",
    "            return None\n",
    "        aligned = aligned.permute(1,2,0).byte().cpu().numpy()\n",
    "        return Image.fromarray(aligned)\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] Face alignment failed:\", e)\n",
    "        return None\n",
    "\n",
    "###############################################\n",
    "# 4) GAN INVERSION\n",
    "###############################################\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "def gan_invert(pil_img):\n",
    "    \"\"\"Retourne latent vector via e4e (latent W+)\"\"\"\n",
    "    try:\n",
    "        aligned = align_face(pil_img)\n",
    "        if aligned is None:\n",
    "            return None\n",
    "\n",
    "        img_tensor = transform(aligned).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, latents = model(img_tensor, return_latents=True)\n",
    "\n",
    "        latent = latents.squeeze().detach().cpu().numpy().astype(np.float32)\n",
    "        return latent\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] GAN inversion failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "###############################################\n",
    "# 5) Construire dataset latent (X, y)\n",
    "###############################################\n",
    "\n",
    "def load_latents(fake_dir, real_dir):\n",
    "    X, y = [], []\n",
    "\n",
    "    print(\"[INFO] Processing REAL images...\")\n",
    "    for fname in os.listdir(real_dir):\n",
    "        try:\n",
    "            img = Image.open(os.path.join(real_dir, fname))\n",
    "            latent = gan_invert(img)\n",
    "            if latent is not None:\n",
    "                X.append(latent)\n",
    "                y.append(0)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(\"[INFO] Processing FAKE images...\")\n",
    "    for fname in os.listdir(fake_dir):\n",
    "        try:\n",
    "            img = Image.open(os.path.join(fake_dir, fname))\n",
    "            latent = gan_invert(img)\n",
    "            if latent is not None:\n",
    "                X.append(latent)\n",
    "                y.append(1)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(\"[OK] Latent dataset built:\", X.shape, y.shape)\n",
    "    return X, y\n",
    "\n",
    "###############################################\n",
    "# 6) Charger ou cr√©er latents\n",
    "###############################################\n",
    "\n",
    "LATENT_PATH = \"latents.npy\"\n",
    "LABEL_PATH = \"labels.npy\"\n",
    "\n",
    "if os.path.exists(LATENT_PATH) and os.path.exists(LABEL_PATH):\n",
    "    X = np.load(LATENT_PATH)\n",
    "    y = np.load(LABEL_PATH)\n",
    "else:\n",
    "    X, y = load_latents(FAKE_DIR, REAL_DIR)\n",
    "    np.save(LATENT_PATH, X)\n",
    "    np.save(LABEL_PATH, y)\n",
    "\n",
    "###############################################\n",
    "# 7) Standardisation\n",
    "###############################################\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "###############################################\n",
    "# 8) PCA plotting\n",
    "###############################################\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=\"coolwarm\", s=5)\n",
    "plt.title(\"PCA - Latent Space\")\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "# 9) t-SNE\n",
    "###############################################\n",
    "\n",
    "X_pca50 = PCA(n_components=50).fit_transform(X_scaled)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_pca50)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y, cmap=\"coolwarm\", s=5)\n",
    "plt.title(\"t-SNE - Latent Space\")\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "# 10) KMeans clustering\n",
    "###############################################\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap=\"viridis\", s=5)\n",
    "plt.title(\"KMeans clustering\")\n",
    "plt.show()\n",
    "\n",
    "###############################################\n",
    "# 11) ML MODELS\n",
    "###############################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "pred = svm.predict(X_test)\n",
    "print(\"SVM ACC:\", accuracy_score(y_test, pred))\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(512,256,64), max_iter=300)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=300)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "###############################################\n",
    "# 12) PREDICTION\n",
    "###############################################\n",
    "\n",
    "def predict_image(path, model=svm):\n",
    "    img = Image.open(path)\n",
    "    latent = gan_invert(img)\n",
    "    if latent is None:\n",
    "        return \"ERROR: no face detected\"\n",
    "    latent = scaler.transform([latent])\n",
    "    return \"FAKE\" if model.predict(latent)[0] == 1 else \"REAL\"\n",
    "\n",
    "print(\"\\nPipeline complet ex√©cut√© ‚úî\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1ef47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Erreur chargement e4e : No module named 'models'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using encoder: ResNet50\n",
      "Found REAL: 790\n",
      "Found FAKE: 3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding REAL:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 332/790 [00:09<00:13, 33.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 137\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     img \u001b[38;5;241m=\u001b[39m safe_imread(p)\n\u001b[1;32m--> 137\u001b[0m     X_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mencoder_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    138\u001b[0m     y_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[10], line 93\u001b[0m, in \u001b[0;36mload_resnet_fallback.<locals>.encode\u001b[1;34m(img_bgr)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(img_bgr):\n\u001b[0;32m     92\u001b[0m     img_rgb \u001b[38;5;241m=\u001b[39m img_bgr[:,:,::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 93\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_resnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_rgb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model(t)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\PIL\\Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2348\u001b[0m         )\n\u001b[0;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2354\u001b[0m         )\n\u001b[1;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# NOTEBOOK UNIQUE : Deepfake detection pipeline\n",
    "# ==========================================================\n",
    "# Requirements:\n",
    "# pip install torch torchvision timm scikit-learn xgboost umap-learn matplotlib tqdm opencv-python\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# sklearn & others\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Tes dossiers contenant des frames D√âJ√Ä extraites\n",
    "path_real = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\real\"\n",
    "path_fake = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\extracted_frames\\fake\"\n",
    "\n",
    "# checkpoint e4e si disponible\n",
    "ckpt_e4e = os.path.join('encoder4editing','pretrained_models','e4e_ffhq_encode.pt')\n",
    "\n",
    "# dossier de sortie\n",
    "output_dir = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\results_gan_pipeline\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "use_gan_inversion = True     # True = essayer e4e, sinon fallback ResNet50\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CHARGEMENT DU MODELE : e4e OU RESNET50\n",
    "# ----------------------------------------------------------\n",
    "e4e_model = None\n",
    "resnet_model = None\n",
    "\n",
    "transform_resnet = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# tentative e4e\n",
    "def try_load_e4e(ckpt_path):\n",
    "    try:\n",
    "        from models.psp import pSp\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        opts = ckpt['opts']\n",
    "        opts['checkpoint_path'] = ckpt_path\n",
    "        opts = type('Options', (), opts)()\n",
    "        net = pSp(opts).to(device).eval()\n",
    "\n",
    "        def encode(img_bgr):\n",
    "            img_rgb = img_bgr[:,:,::-1]\n",
    "            tf = T.Compose([T.ToPILImage(), T.Resize((256,256)), T.ToTensor()])\n",
    "            t = tf(img_rgb).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = net.encoder(t)\n",
    "                return out.cpu().numpy().reshape(-1)\n",
    "        print(\"e4e loaded successfully.\")\n",
    "        return encode\n",
    "    except Exception as e:\n",
    "        print(\"Erreur chargement e4e :\", e)\n",
    "        return None\n",
    "\n",
    "def load_resnet_fallback():\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.to(device).eval()\n",
    "    def encode(img_bgr):\n",
    "        img_rgb = img_bgr[:,:,::-1]\n",
    "        t = transform_resnet(img_rgb).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            return model(t).cpu().numpy().reshape(-1)\n",
    "    return encode\n",
    "\n",
    "if use_gan_inversion and os.path.exists(ckpt_e4e):\n",
    "    e4e_model = try_load_e4e(ckpt_e4e)\n",
    "\n",
    "encoder_fn = e4e_model if e4e_model is not None else load_resnet_fallback()\n",
    "print(\"Using encoder:\", \"e4e\" if e4e_model else \"ResNet50\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1) LECTURE DES IMAGES (supporte les sous-dossiers)\n",
    "# ----------------------------------------------------------\n",
    "def list_images(folder):\n",
    "    exts = ('.jpg','.jpeg','.png','.bmp')\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                all_files.append(os.path.join(root, f))\n",
    "    return all_files\n",
    "\n",
    "imgs_real = list_images(path_real)\n",
    "imgs_fake = list_images(path_fake)\n",
    "\n",
    "print(\"Found REAL:\", len(imgs_real))\n",
    "print(\"Found FAKE:\", len(imgs_fake))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2) EXTRACTION DES FEATURES / LATENTS\n",
    "# ----------------------------------------------------------\n",
    "def safe_imread(p):\n",
    "    img = cv2.imread(p)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Unable to read image \" + p)\n",
    "    return img\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for p in tqdm(imgs_real, desc=\"Encoding REAL\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(0)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "for p in tqdm(imgs_fake, desc=\"Encoding FAKE\"):\n",
    "    try:\n",
    "        img = safe_imread(p)\n",
    "        X_list.append(encoder_fn(img))\n",
    "        y_list.append(1)\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", p, e)\n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "print(\"Latents shape:\", X.shape)\n",
    "\n",
    "np.save(os.path.join(output_dir, \"latent_vectors.npy\"), X)\n",
    "np.save(os.path.join(output_dir, \"labels.npy\"), y)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3) ANALYSE DE L‚ÄôESPACE LATENT\n",
    "# ----------------------------------------------------------\n",
    "results_summary = {}\n",
    "\n",
    "# PCA\n",
    "if X.shape[0] >= 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    plt.scatter(X_pca[:,0], X_pca[:,1], c=y)\n",
    "    plt.title(\"PCA 2D\")\n",
    "    plt.savefig(os.path.join(output_dir, \"pca.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# t-SNE\n",
    "if X.shape[0] >= 5:\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y)\n",
    "    plt.title(\"t-SNE\")\n",
    "    plt.savefig(os.path.join(output_dir, \"tsne.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# UMAP\n",
    "if X.shape[0] >= 5:\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X)\n",
    "    plt.scatter(X_umap[:,0], X_umap[:,1], c=y)\n",
    "    plt.title(\"UMAP\")\n",
    "    plt.savefig(os.path.join(output_dir, \"umap.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# distances intra/inter\n",
    "if len(np.unique(y)) == 2:\n",
    "    X_real = X[y==0]\n",
    "    X_fake = X[y==1]\n",
    "\n",
    "    dist_real = np.mean(cdist(X_real, X_real)) if len(X_real)>=2 else None\n",
    "    dist_fake = np.mean(cdist(X_fake, X_fake)) if len(X_fake)>=2 else None\n",
    "    dist_inter = np.mean(cdist(X_real, X_fake)) if len(X_real)>=1 and len(X_fake)>=1 else None\n",
    "\n",
    "    results_summary.update({\n",
    "        'intra_real': dist_real,\n",
    "        'intra_fake': dist_fake,\n",
    "        'inter': dist_inter\n",
    "    })\n",
    "\n",
    "with open(os.path.join(output_dir, \"latent_analysis.json\"), \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4) MACHINE LEARNING\n",
    "# ----------------------------------------------------------\n",
    "if X.shape[0] < 10:\n",
    "    print(\"Pas assez d‚Äô√©chantillons pour ML !\")\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    models_results = {}\n",
    "\n",
    "    # SVM\n",
    "    svm = SVC(kernel='rbf')\n",
    "    svm.fit(X_train, y_train)\n",
    "    pred = svm.predict(X_test)\n",
    "    models_results[\"svm\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(svm, os.path.join(output_dir, \"model_svm.joblib\"))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred = rf.predict(X_test)\n",
    "    models_results[\"rf\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(rf, os.path.join(output_dir, \"model_rf.joblib\"))\n",
    "\n",
    "    # MLP\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(512,256), max_iter=500)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    pred = mlp.predict(X_test)\n",
    "    models_results[\"mlp\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(mlp, os.path.join(output_dir, \"model_mlp.joblib\"))\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    pred = xgb.predict(X_test)\n",
    "    models_results[\"xgb\"] = float(accuracy_score(y_test, pred))\n",
    "    joblib.dump(xgb, os.path.join(output_dir, \"model_xgb.joblib\"))\n",
    "\n",
    "    joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "\n",
    "    with open(os.path.join(output_dir, \"results_models.json\"), \"w\") as f:\n",
    "        json.dump(models_results, f, indent=2)\n",
    "\n",
    "    print(\"\\nAccuracy des mod√®les :\")\n",
    "    for m,a in models_results.items():\n",
    "        print(f\" - {m}: {a:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# FIN\n",
    "# ----------------------------------------------------------\n",
    "print(\"\\nPipeline termin√© ! R√©sultats dans :\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
