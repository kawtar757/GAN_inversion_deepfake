{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13351fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649e4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (4.8.0.76)\n",
      "Requirement already satisfied: einops in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\EliteLaptop/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:04<00:00, 11.0MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [11:49<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.5877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [11:58<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.4840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [11:28<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.4756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [10:23<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [10:34<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.4707\n",
      "Output: tensor([[0.8440]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1Ô∏è‚É£ Install Libraries\n",
    "# =========================\n",
    "!pip install torch torchvision opencv-python einops tqdm\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Imports\n",
    "# =========================\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Dataset Paths (Windows)\n",
    "# =========================\n",
    "BASE_PATH = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\"\n",
    "\n",
    "REAL_PATH = os.path.join(BASE_PATH, \"real\")\n",
    "FAKE_PATH = os.path.join(BASE_PATH, \"fake\")\n",
    "\n",
    "DATASET = {\n",
    "    \"Real\": REAL_PATH,\n",
    "    \"Fake\": FAKE_PATH\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"Real\": 0,\n",
    "    \"Fake\": 1\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Dataset Class\n",
    "# =========================\n",
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, paths, labels, frames=16):\n",
    "        self.samples = []\n",
    "        self.frames = frames\n",
    "\n",
    "        for k, p in paths.items():\n",
    "            for vid in os.listdir(p):\n",
    "                if vid.endswith(\".mp4\"):\n",
    "                    self.samples.append((os.path.join(p, vid), labels[k]))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def extract_frames(self, video):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Impossible d'ouvrir:\", video)\n",
    "            return None\n",
    "\n",
    "        frames = []\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total == 0:\n",
    "            print(\"‚ùå Vid√©o vide:\", video)\n",
    "            cap.release()\n",
    "            return None\n",
    "\n",
    "        idxs = torch.linspace(0, total - 1, self.frames).long().tolist()\n",
    "        for i in range(total):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in idxs:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            return None\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            video, label = self.samples[idx]\n",
    "            frames = self.extract_frames(video)\n",
    "            if frames is not None:\n",
    "                return frames, torch.tensor(label)\n",
    "            idx = (idx + 1) % len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Neural Oscillator Layer\n",
    "# =========================\n",
    "class NeuralOscillator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(torch.randn(dim))\n",
    "        self.phase = nn.Parameter(torch.randn(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.size(1), device=x.device).float()\n",
    "        osc = torch.sin(self.freq * t.unsqueeze(-1) + self.phase)\n",
    "        return x * osc\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Quantum Attention Layer\n",
    "# =========================\n",
    "class QuantumAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.phase = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phase = self.phase(x)\n",
    "        amp = torch.abs(torch.sum(x * torch.exp(1j * phase), dim=1))\n",
    "        return amp.real\n",
    "\n",
    "# =========================\n",
    "# 7Ô∏è‚É£ Full Model: QINON\n",
    "# =========================\n",
    "class QINON(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()  # 512 features per frame\n",
    "\n",
    "        self.osc = NeuralOscillator(512)\n",
    "        self.attn = QuantumAttention(512)\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        feats = self.backbone(x)\n",
    "        feats = rearrange(feats, '(b t) d -> b t d', b=b)\n",
    "        feats = self.osc(feats)\n",
    "        feats = self.attn(feats)\n",
    "        out = self.fc(feats)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# =========================\n",
    "# 8Ô∏è‚É£ Dataset + DataLoader\n",
    "# =========================\n",
    "dataset = DeepFakeDataset(DATASET, LABELS, frames=16)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# =========================\n",
    "# 9Ô∏è‚É£ Initialize Model\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = QINON().to(device)\n",
    "\n",
    "# =========================\n",
    "# üîü Optimizer + Loss\n",
    "# =========================\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Training Loop\n",
    "# =========================\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Test single batch\n",
    "# =========================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = next(iter(loader))\n",
    "    x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
    "    out = model(x)\n",
    "    print(\"Output:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5af7066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (4.8.0.76)\n",
      "Requirement already satisfied: einops in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\elitelaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [27:05<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 1.2616 | Accuracy: 0.7335 | AUC: 0.4641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [25:50<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.6234 | Accuracy: 0.7838 | AUC: 0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [26:15<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.5314 | Accuracy: 0.8080 | AUC: 0.5109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [25:44<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.4982 | Accuracy: 0.8195 | AUC: 0.5610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 953/953 [24:38<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.5074 | Accuracy: 0.8195 | AUC: 0.5583\n",
      "Output probabilities: [[0.6556499]]\n",
      "Test Accuracy: 1.0000 | Test AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1Ô∏è‚É£ Install Libraries\n",
    "# =========================\n",
    "!pip install torch torchvision opencv-python einops tqdm scikit-learn\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Imports\n",
    "# =========================\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Dataset Paths (Windows)\n",
    "# =========================\n",
    "BASE_PATH = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\"\n",
    "\n",
    "REAL_PATH = os.path.join(BASE_PATH, \"real\")\n",
    "FAKE_PATH = os.path.join(BASE_PATH, \"fake\")\n",
    "\n",
    "DATASET = {\n",
    "    \"Real\": REAL_PATH,\n",
    "    \"Fake\": FAKE_PATH\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"Real\": 0,\n",
    "    \"Fake\": 1\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Dataset Class\n",
    "# =========================\n",
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, paths, labels, frames=50):\n",
    "        self.samples = []\n",
    "        self.frames = frames\n",
    "\n",
    "        for k, p in paths.items():\n",
    "            for vid in os.listdir(p):\n",
    "                if vid.endswith(\".mp4\"):\n",
    "                    self.samples.append((os.path.join(p, vid), labels[k]))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def extract_frames(self, video):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        if not cap.isOpened():\n",
    "            print(\"‚ùå Impossible d'ouvrir:\", video)\n",
    "            return None\n",
    "\n",
    "        frames = []\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total == 0:\n",
    "            print(\"‚ùå Vid√©o vide:\", video)\n",
    "            cap.release()\n",
    "            return None\n",
    "\n",
    "        idxs = torch.linspace(0, total - 1, self.frames).long().tolist()\n",
    "        for i in range(total):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in idxs:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            return None\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            video, label = self.samples[idx]\n",
    "            frames = self.extract_frames(video)\n",
    "            if frames is not None:\n",
    "                return frames, torch.tensor(label)\n",
    "            idx = (idx + 1) % len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Neural Oscillator Layer\n",
    "# =========================\n",
    "class NeuralOscillator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(torch.randn(dim))\n",
    "        self.phase = nn.Parameter(torch.randn(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.arange(x.size(1), device=x.device).float()\n",
    "        osc = torch.sin(self.freq * t.unsqueeze(-1) + self.phase)\n",
    "        return x * osc\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Quantum Attention Layer\n",
    "# =========================\n",
    "class QuantumAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.phase = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phase = self.phase(x)\n",
    "        amp = torch.abs(torch.sum(x * torch.exp(1j * phase), dim=1))\n",
    "        return amp.real\n",
    "\n",
    "# =========================\n",
    "# 7Ô∏è‚É£ Full Model: QINON\n",
    "# =========================\n",
    "class QINON(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()  # 512 features per frame\n",
    "\n",
    "        self.osc = NeuralOscillator(512)\n",
    "        self.attn = QuantumAttention(512)\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        feats = self.backbone(x)\n",
    "        feats = rearrange(feats, '(b t) d -> b t d', b=b)\n",
    "        feats = self.osc(feats)\n",
    "        feats = self.attn(feats)\n",
    "        out = self.fc(feats)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# =========================\n",
    "# 8Ô∏è‚É£ Dataset + DataLoader\n",
    "# =========================\n",
    "dataset = DeepFakeDataset(DATASET, LABELS, frames=100)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# =========================\n",
    "# 9Ô∏è‚É£ Initialize Model\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = QINON().to(device)\n",
    "\n",
    "# =========================\n",
    "# üîü Optimizer + Loss\n",
    "# =========================\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Training Loop avec Accuracy & AUC\n",
    "# =========================\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for x, y in tqdm(loader):\n",
    "        x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    binary_preds = [1 if p >= 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, binary_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss/len(loader):.4f} | Accuracy: {acc:.4f} | AUC: {auc:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Test single batch avec Accuracy & AUC\n",
    "# =========================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = next(iter(loader))\n",
    "    x, y = x.to(device), y.to(device).float().unsqueeze(1)\n",
    "    out = model(x)\n",
    "\n",
    "    binary_out = [1 if p >= 0.5 else 0 for p in out.cpu().numpy()]\n",
    "    acc = accuracy_score(y.cpu().numpy(), binary_out)\n",
    "    try:\n",
    "        auc = roc_auc_score(y.cpu().numpy(), out.cpu().numpy())\n",
    "    except:\n",
    "        auc = 0.0\n",
    "\n",
    "    print(\"Output probabilities:\", out.cpu().numpy())\n",
    "    print(f\"Test Accuracy: {acc:.4f} | Test AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cf35e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [34:39<00:00,  1.78s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.6416 | Val Acc: 0.4897 | Val AUC: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [33:26<00:00,  1.72s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5] Loss: 0.4397 | Val Acc: 0.5479 | Val AUC: 0.6074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [31:10<00:00,  1.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5] Loss: 0.3318 | Val Acc: 0.5479 | Val AUC: 0.6713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [26:58<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5] Loss: 0.2993 | Val Acc: 0.6233 | Val AUC: 0.6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [31:10<00:00,  1.60s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5] Loss: 0.2631 | Val Acc: 0.5582 | Val AUC: 0.5885\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Quantum-Inspired Neural Oscillator Network (QINON)\n",
    "# DeepFake Detection ‚Äì Stable & Improved Version\n",
    "# =========================================================\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ Imports\n",
    "# =========================\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Paths (Windows)\n",
    "# =========================\n",
    "BASE_PATH = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\"\n",
    "\n",
    "DATASET_PATHS = {\n",
    "    \"Real\": os.path.join(BASE_PATH, \"real\"),\n",
    "    \"Fake\": os.path.join(BASE_PATH, \"fake\")\n",
    "}\n",
    "\n",
    "LABELS = {\"Real\": 0, \"Fake\": 1}\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Dataset\n",
    "# =========================\n",
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, samples, frames=32):\n",
    "        self.samples = samples\n",
    "        self.frames = frames\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def extract_frames(self, video):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total <= 0:\n",
    "            return None\n",
    "\n",
    "        idxs = torch.linspace(0, total - 1, self.frames).long().tolist()\n",
    "        frames = []\n",
    "\n",
    "        for i in range(total):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in idxs:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "        return torch.stack(frames) if len(frames) > 0 else None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            path, label = self.samples[idx]\n",
    "            frames = self.extract_frames(path)\n",
    "            if frames is not None:\n",
    "                return frames, torch.tensor(label, dtype=torch.float32)\n",
    "            idx = (idx + 1) % len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Neural Oscillator (Stable)\n",
    "# =========================\n",
    "class NeuralOscillator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(torch.randn(dim) * 0.1)\n",
    "        self.phase = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = torch.linspace(0, 1, x.size(1), device=x.device)\n",
    "        osc = torch.sin(2 * torch.pi * self.freq * t.unsqueeze(-1) + self.phase)\n",
    "        return x * osc\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Quantum-Inspired Attention (Real & Stable)\n",
    "# =========================\n",
    "class QuantumAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.phase = nn.Linear(dim, dim)\n",
    "        self.amplitude = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phase = torch.tanh(self.phase(x))\n",
    "        amp = torch.softmax(self.amplitude(x), dim=1)\n",
    "        return torch.sum(x * amp * phase, dim=1)\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ QINON Model\n",
    "# =========================\n",
    "class QINON(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.backbone.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self.osc = NeuralOscillator(512)\n",
    "        self.attn = QuantumAttention(512)\n",
    "        self.fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        feats = self.backbone(x)\n",
    "        feats = rearrange(feats, '(b t) d -> b t d', b=b)\n",
    "        feats = self.osc(feats)\n",
    "        feats = self.attn(feats)\n",
    "        return self.fc(feats)  # logits\n",
    "\n",
    "# =========================\n",
    "# 7Ô∏è‚É£ Prepare Data\n",
    "# =========================\n",
    "samples = []\n",
    "for cls, path in DATASET_PATHS.items():\n",
    "    for v in os.listdir(path):\n",
    "        if v.endswith(\".mp4\"):\n",
    "            samples.append((os.path.join(path, v), LABELS[cls]))\n",
    "\n",
    "train_s, val_s = train_test_split(\n",
    "    samples, test_size=0.2, stratify=[l for _, l in samples]\n",
    ")\n",
    "\n",
    "train_ds = DeepFakeDataset(train_s)\n",
    "val_ds = DeepFakeDataset(val_s)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# =========================\n",
    "# 8Ô∏è‚É£ Training Setup\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = QINON().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# =========================\n",
    "# 9Ô∏è‚É£ Training Loop\n",
    "# =========================\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(device), y.to(device).unsqueeze(1)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_preds.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, [p >= 0.5 for p in all_preds])\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Loss: {train_loss/len(train_loader):.4f} \"\n",
    "          f\"| Val Acc: {acc:.4f} | Val AUC: {auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3cbdba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [29:58<00:00,  1.54s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8] Loss: 0.5995 | Val Acc: 0.3459 | Val AUC: 0.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [29:06<00:00,  1.50s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8] Loss: 0.3722 | Val Acc: 0.2260 | Val AUC: 0.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [35:11<00:00,  1.81s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8] Loss: 0.3067 | Val Acc: 0.2740 | Val AUC: 0.2592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [30:07<00:00,  1.55s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8] Loss: 0.2789 | Val Acc: 0.3185 | Val AUC: 0.1606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1167/1167 [32:00<00:00,  1.65s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8] Loss: 0.2438 | Val Acc: 0.2808 | Val AUC: 0.2091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 811/1167 [1:37:16<42:41,  7.20s/it]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 204\u001b[0m\n\u001b[0;32m    201\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    202\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    205\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    206\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 71\u001b[0m, in \u001b[0;36mDeepFakeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\n\u001b[1;32m---> 71\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m frames, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mDeepFakeDataset.extract_frames\u001b[1;34m(self, video)\u001b[0m\n\u001b[0;32m     54\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n\u001b[1;32m---> 57\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# QINON + Temporal FFT\n",
    "# Quantum-Inspired Neural Oscillator Network\n",
    "# DeepFake Detection (Improved AUC Version)\n",
    "# =========================================================\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ Imports\n",
    "# =========================\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Paths (Windows)\n",
    "# =========================\n",
    "BASE_PATH = r\"C:\\Users\\EliteLaptop\\Desktop\\kawtar\\GAN_inversion\\raw\"\n",
    "\n",
    "DATASET_PATHS = {\n",
    "    \"Real\": os.path.join(BASE_PATH, \"real\"),\n",
    "    \"Fake\": os.path.join(BASE_PATH, \"fake\")\n",
    "}\n",
    "\n",
    "LABELS = {\"Real\": 0, \"Fake\": 1}\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Dataset\n",
    "# =========================\n",
    "class DeepFakeDataset(Dataset):\n",
    "    def __init__(self, samples, frames=32):\n",
    "        self.samples = samples\n",
    "        self.frames = frames\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def extract_frames(self, video):\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return None\n",
    "\n",
    "        idxs = torch.linspace(0, total - 1, self.frames).long().tolist()\n",
    "        frames = []\n",
    "\n",
    "        for i in range(total):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if i in idxs:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = self.transform(frame)\n",
    "                frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "        return torch.stack(frames) if len(frames) > 0 else None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            path, label = self.samples[idx]\n",
    "            frames = self.extract_frames(path)\n",
    "            if frames is not None:\n",
    "                return frames, torch.tensor(label, dtype=torch.float32)\n",
    "            idx = (idx + 1) % len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ Neural Oscillator\n",
    "# =========================\n",
    "class NeuralOscillator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(torch.randn(dim) * 0.1)\n",
    "        self.phase = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D]\n",
    "        t = torch.linspace(0, 1, x.size(1), device=x.device)\n",
    "        osc = torch.sin(2 * torch.pi * self.freq * t.unsqueeze(-1) + self.phase)\n",
    "        return x * osc\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ Quantum-Inspired Attention\n",
    "# =========================\n",
    "class QuantumAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.phase = nn.Linear(dim, dim)\n",
    "        self.amplitude = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phase = torch.tanh(self.phase(x))\n",
    "        amp = torch.softmax(self.amplitude(x), dim=1)\n",
    "        return torch.sum(x * amp * phase, dim=1)\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Temporal FFT Branch\n",
    "# =========================\n",
    "class TemporalFFT(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T, D]\n",
    "        fft = torch.fft.rfft(x, dim=1)\n",
    "        mag = torch.abs(fft).mean(dim=1)\n",
    "        return self.fc(mag)\n",
    "\n",
    "# =========================\n",
    "# 7Ô∏è‚É£ QINON + FFT Model\n",
    "# =========================\n",
    "class QINON(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in self.backbone.layer4.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        self.osc = NeuralOscillator(512)\n",
    "        self.attn = QuantumAttention(512)\n",
    "        self.fft = TemporalFFT(512)\n",
    "\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c, h, w = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        feats = self.backbone(x)\n",
    "        feats = rearrange(feats, '(b t) d -> b t d', b=b)\n",
    "\n",
    "        osc_feats = self.osc(feats)\n",
    "        attn_feats = self.attn(osc_feats)   # [B, 512]\n",
    "        fft_feats = self.fft(feats)         # [B, 512]\n",
    "\n",
    "        fused = torch.cat([attn_feats, fft_feats], dim=1)\n",
    "        return self.fc(fused)  # logits\n",
    "\n",
    "# =========================\n",
    "# 8Ô∏è‚É£ Prepare Data\n",
    "# =========================\n",
    "samples = []\n",
    "for cls, path in DATASET_PATHS.items():\n",
    "    for v in os.listdir(path):\n",
    "        if v.endswith(\".mp4\"):\n",
    "            samples.append((os.path.join(path, v), LABELS[cls]))\n",
    "\n",
    "train_s, val_s = train_test_split(\n",
    "    samples,\n",
    "    test_size=0.2,\n",
    "    stratify=[l for _, l in samples]\n",
    ")\n",
    "\n",
    "train_ds = DeepFakeDataset(train_s)\n",
    "val_ds = DeepFakeDataset(val_s)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# =========================\n",
    "# 9Ô∏è‚É£ Training Setup\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = QINON().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# üîü Training Loop\n",
    "# =========================\n",
    "EPOCHS = 8\n",
    "best_auc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_preds.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(y.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, [p >= 0.5 for p in all_preds])\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    scheduler.step(auc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Loss: {train_loss/len(train_loader):.4f} \"\n",
    "          f\"| Val Acc: {acc:.4f} | Val AUC: {auc:.4f}\")\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        torch.save(model.state_dict(), \"best_qinon_fft.pth\")\n",
    "\n",
    "print(f\"\\n‚úÖ Best Validation AUC: {best_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
