{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b26c82",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline de détection Deepfake vs Réel \n",
    "Ce notebook implémente l'architecture demandée :\n",
    "- Extraction de frames depuis des vidéos\n",
    "- (Optionnel) GAN-inversion → vecteurs latents `z` (placeholder / hook)\n",
    "- Extraction de vecteurs latents par image (fallback: caractéristiques CNN)\n",
    "- Analyse de l'espace latent : PCA / t-SNE / UMAP, moyenne/variance, distances, clustering\n",
    "- Entraînement de classifieurs : SVM, MLP, Random Forest\n",
    "- Évaluation et export du modèle\n",
    "\n",
    "**Structure attendue des données** (à adapter) :\n",
    "```\n",
    "data/\n",
    "  videos/\n",
    "    real_001.mp4\n",
    "    fake_001.mp4\n",
    "  labels.csv   # colonnes: filename,label  (label in {real,fake})\n",
    "```\n",
    "> NOTE : La partie *GAN inversion* est délicate (dépend du GAN, checkpoints, et GPU). Le notebook fournit un *hook* où vous pouvez insérer votre méthode d'inversion (p.ex. e4e, pSp, Restyle, etc.). Si vous ne l'avez pas, le notebook utilisera un extracteur CNN (ResNet) comme approximation des latents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ffa6555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed. Restart the kernel if necessary.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Installer les dépendances nécessaires (exécuté une seule fois)\n",
    "# UMAP et moviepy peuvent être optionnels selon vos besoins.\n",
    "import sys\n",
    "import subprocess\n",
    "def pip_install(packages):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + packages)\n",
    "\n",
    "packages = [\n",
    "    \"tqdm\", \"opencv-python\", \"scikit-learn\", \"matplotlib\", \"pandas\", \"joblib\",\n",
    "    \"torch\", \"torchvision\", \"moviepy\", \"umap-learn\"\n",
    "]\n",
    "pip_install(packages)\n",
    "print(\"Packages installed. Restart the kernel if necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1341dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports et configuration\n",
    "import os, math, io, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import joblib\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "# Fix plotting defaults (one plot per cell later)\n",
    "%matplotlib inline\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e0ff4e",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Extraction des frames depuis les vidéos\n",
    "Fonctions pour extraire frames, sauver en répertoires par vidéo, et construire un DataFrame des images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b215285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = Path('data')\n",
    "VIDEOS_DIR = DATA_DIR / 'videos'\n",
    "FRAMES_DIR = DATA_DIR / 'frames'  # output\n",
    "FRAMES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_frames_from_video(video_path, out_dir, fps=1, max_frames=None, resize=(256,256)):\n",
    "    \"\"\"Extract frames at `fps` frames per second. Save as JPEGs in out_dir/video_stem/\"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f'Cannot open {video_path}')\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    step = max(1, int(round(video_fps / fps)))\n",
    "    frame_idx = 0\n",
    "    saved = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_idx % step == 0:\n",
    "            if resize is not None:\n",
    "                frame = cv2.resize(frame, resize)\n",
    "            out_path = out_dir / f\"frame_{saved:05d}.jpg\"\n",
    "            cv2.imwrite(str(out_path), frame)\n",
    "            saved += 1\n",
    "            if max_frames and saved >= max_frames:\n",
    "                break\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "def extract_all_frames(labels_csv='data/labels.csv', fps=1, max_frames_per_video=None, resize=(256,256)):\n",
    "    labels = pd.read_csv(labels_csv)\n",
    "    rows = []\n",
    "    for _, row in tqdm(labels.iterrows(), total=len(labels)):\n",
    "        fname = row['filename']\n",
    "        label = row['label']\n",
    "        video_path = VIDEOS_DIR / fname\n",
    "        if not video_path.exists():\n",
    "            print('Missing', video_path); continue\n",
    "        out_dir = FRAMES_DIR / Path(fname).stem\n",
    "        n = extract_frames_from_video(video_path, out_dir, fps=fps, max_frames=max_frames_per_video, resize=resize)\n",
    "        for i in range(n):\n",
    "            rows.append({'video': fname, 'frame_path': str(out_dir / f'frame_{i:05d}.jpg'), 'label': label})\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# df_frames = extract_all_frames('data/labels.csv', fps=1, max_frames_per_video=30, resize=(256,256))\n",
    "# df_frames.to_csv('data/frames_index.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5331de6",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Extraction des vecteurs latents (hook)\n",
    "- **Option A (GAN inversion)** : insérez ici votre fonction d'inversion GAN qui prend une image (PIL/np.array) et retourne un vecteur latent `z` (1D numpy array).\n",
    "- **Option B (fallback CNN features)** : si vous ne disposez pas d'un inversion pipeline, on utilise un extracteur CNN (ResNet50 pretrained) et on prend la couche `avgpool` comme vecteur d'embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfa182fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\EliteLaptop\\miniconda3\\envs\\torch_gpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\EliteLaptop/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:09<00:00, 10.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# --- Prepare CNN fallback extractor (ResNet50) ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "# Remove final classifier to get features\n",
    "feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def cnn_get_latent(image_bgr):\n",
    "    \"\"\"Input: image as BGR numpy array (OpenCV). Output: 1D numpy latent vector.\"\"\"\n",
    "    img = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat = feature_extractor(x)  # shape (1,2048,1,1)\n",
    "        feat = feat.reshape(feat.shape[0], -1).cpu().numpy().squeeze()\n",
    "    return feat\n",
    "\n",
    "# --- GAN inversion placeholder ---\n",
    "def gan_inversion_get_latent(image_bgr):\n",
    "    \"\"\"PLACEHOLDER: Replace with your GAN inversion method.\n",
    "    Should return a 1D numpy array latent vector z.\n",
    "    Example signature kept identical to cnn_get_latent for compatibility.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Provide your GAN inversion function here.\")\n",
    "\n",
    "# Wrapper to choose method\n",
    "def get_latent(image_path, method='cnn'):\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if method == 'cnn':\n",
    "        return cnn_get_latent(img)\n",
    "    elif method == 'gan':\n",
    "        return gan_inversion_get_latent(img)\n",
    "    else:\n",
    "        raise ValueError('Unknown method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45b4abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Batch extraction of latents and caching\n",
    "def build_latents(df_frames, method='cnn', cache_path='data/latents.npz', batch_size=256, overwrite=False):\n",
    "    cache_path = Path(cache_path)\n",
    "    if cache_path.exists() and not overwrite:\n",
    "        print('Loading cached latents...')\n",
    "        data = np.load(cache_path, allow_pickle=True)\n",
    "        latents = data['latents']\n",
    "        paths = data['paths'].tolist()\n",
    "        labels = data['labels'].tolist()\n",
    "        return pd.DataFrame({'frame_path': paths, 'latent_idx': list(range(len(paths))), 'label': labels}), latents\n",
    "    latents = []\n",
    "    paths = []\n",
    "    labels = []\n",
    "    for i, row in tqdm(df_frames.iterrows(), total=len(df_frames)):\n",
    "        p = row['frame_path']\n",
    "        try:\n",
    "            z = get_latent(p, method=method)\n",
    "        except Exception as e:\n",
    "            print('Error extracting', p, e); continue\n",
    "        latents.append(z.astype(np.float32))\n",
    "        paths.append(p)\n",
    "        labels.append(row['label'])\n",
    "    latents = np.stack(latents, axis=0)\n",
    "    np.savez_compressed(cache_path, latents=latents, paths=np.array(paths), labels=np.array(labels))\n",
    "    df_idx = pd.DataFrame({'frame_path': paths, 'latent_idx': list(range(len(paths))), 'label': labels})\n",
    "    return df_idx, latents\n",
    "\n",
    "# Example:\n",
    "# df_frames = pd.read_csv('data/frames_index.csv')\n",
    "# df_idx, latents = build_latents(df_frames, method='cnn', cache_path='data/latents.npz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f76a8c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Analyse de l'espace latent\n",
    "PCA / t-SNE / UMAP, statistiques (moyenne / variance), distances intra/inter-clusters, clustering simple (KMeans) et visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ad5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def analyze_latents(latents, labels, n_components_pca=50, tsne_perplexity=30, umap_n_neighbors=15):\n",
    "    results = {}\n",
    "    scaler = StandardScaler()\n",
    "    Zs = scaler.fit_transform(latents)\n",
    "    # PCA\n",
    "    pca = PCA(n_components=min(n_components_pca, Zs.shape[1]))\n",
    "    Z_pca = pca.fit_transform(Zs)\n",
    "    results['pca'] = {'model': pca, 'embedding': Z_pca}\n",
    "    # t-SNE (on PCA-reduced for speed)\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=tsne_perplexity, init='random', random_state=42)\n",
    "    Z_tsne = tsne.fit_transform(Z_pca[:, :min(50, Z_pca.shape[1])])\n",
    "    results['tsne'] = {'model': tsne, 'embedding': Z_tsne}\n",
    "    # UMAP\n",
    "    um = umap.UMAP(n_components=2, n_neighbors=umap_n_neighbors, random_state=42)\n",
    "    Z_umap = um.fit_transform(Z_pca[:, :min(50, Z_pca.shape[1])])\n",
    "    results['umap'] = {'model': um, 'embedding': Z_umap}\n",
    "    # Stats\n",
    "    unique_labels = np.unique(labels)\n",
    "    stats = {}\n",
    "    for ul in unique_labels:\n",
    "        idx = np.where(labels == ul)[0]\n",
    "        stats[ul] = {'mean': latents[idx].mean(axis=0), 'var': latents[idx].var(axis=0), 'count': len(idx)}\n",
    "    results['stats'] = stats\n",
    "    # Distances intra/inter\n",
    "    dists = {}\n",
    "    for a in unique_labels:\n",
    "        for b in unique_labels:\n",
    "            idxa = np.where(labels == a)[0]\n",
    "            idxb = np.where(labels == b)[0]\n",
    "            if len(idxa)==0 or len(idxb)==0:\n",
    "                d = np.nan\n",
    "            else:\n",
    "                d = cdist(latents[idxa], latents[idxb]).mean()\n",
    "            dists[f'{a}_to_{b}'] = d\n",
    "    results['distances'] = dists\n",
    "    # Clustering\n",
    "    kmeans = KMeans(n_clusters=min(8, len(unique_labels)*2), random_state=42)\n",
    "    clusters = kmeans.fit_predict(Z_pca[:, :min(50, Z_pca.shape[1])])\n",
    "    results['kmeans'] = {'model': kmeans, 'clusters': clusters}\n",
    "    return results\n",
    "\n",
    "def plot_embedding(embedding, labels, title='Embedding'):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    labels_unique = np.unique(labels)\n",
    "    for lab in labels_unique:\n",
    "        idx = np.where(labels == lab)[0]\n",
    "        plt.scatter(embedding[idx,0], embedding[idx,1], label=str(lab), s=8)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('dim1'); plt.ylabel('dim2')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage after obtaining latents:\n",
    "# results = analyze_latents(latents, df_idx['label'].values)\n",
    "# plot_embedding(results['umap']['embedding'], df_idx['label'].values, title='UMAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcaf0e",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Entraînement des classifieurs (SVM, MLP, RandomForest)\n",
    "Fonctions pour entraîner, valider via cross-validation et sauvegarder les modèles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f9439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate(latents, labels, method_name='SVM', cv=5, random_state=42):\n",
    "    X = latents.copy()\n",
    "    y = np.array([1 if lab=='fake' else 0 for lab in labels])\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(X)\n",
    "    models = {}\n",
    "    if method_name == 'SVM':\n",
    "        clf = SVC(kernel='rbf', probability=True, random_state=random_state)\n",
    "    elif method_name == 'RF':\n",
    "        clf = RandomForestClassifier(n_estimators=200, random_state=random_state)\n",
    "    elif method_name == 'MLP':\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(512,256), max_iter=200, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError('Unknown method')\n",
    "    # Cross-validation simple\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    y_true_all=[]; y_pred_all=[]; y_prob_all=[]\n",
    "    for train_idx, test_idx in skf.split(Xs, y):\n",
    "        clf.fit(Xs[train_idx], y[train_idx])\n",
    "        y_pred = clf.predict(Xs[test_idx])\n",
    "        y_prob = clf.predict_proba(Xs[test_idx])[:,1] if hasattr(clf, 'predict_proba') else None\n",
    "        y_true_all.extend(y[test_idx].tolist())\n",
    "        y_pred_all.extend(y_pred.tolist())\n",
    "        if y_prob is not None:\n",
    "            y_prob_all.extend(y_prob.tolist())\n",
    "    report = classification_report(y_true_all, y_pred_all, output_dict=True)\n",
    "    acc = accuracy_score(y_true_all, y_pred_all)\n",
    "    roc = roc_auc_score(y_true_all, y_prob_all) if len(y_prob_all)>0 else None\n",
    "    models['clf'] = clf\n",
    "    models['scaler'] = scaler\n",
    "    models['report'] = report\n",
    "    models['accuracy'] = acc\n",
    "    models['roc_auc'] = roc\n",
    "    return models\n",
    "\n",
    "# Train all three and compare\n",
    "def train_compare_all(latents, labels):\n",
    "    outcomes = {}\n",
    "    for name in ['SVM', 'RF', 'MLP']:\n",
    "        print('Training', name)\n",
    "        m = train_and_evaluate(latents, labels, method_name=name)\n",
    "        outcomes[name] = m\n",
    "        print(name, 'acc=', m['accuracy'], 'roc_auc=', m['roc_auc'])\n",
    "    return outcomes\n",
    "\n",
    "# Save & load utility\n",
    "def save_model(obj, path='models/model.joblib'):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(obj, path)\n",
    "def load_model(path='models/model.joblib'):\n",
    "    return joblib.load(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e453ced",
   "metadata": {},
   "source": [
    "\n",
    "### Agrégation par vidéo\n",
    "Les latents sont par-frame. Pour classifier une vidéo, on agrégera (moyenne, médiane, ou vote majoritaire) les prédictions de ses frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7873e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_video_from_frames(df_idx, latents, trained_model, agg='mean'):\n",
    "    # df_idx: DataFrame with frame_path, latent_idx, label, video (optional)\n",
    "    # trained_model: dict with 'clf' and 'scaler'\n",
    "    clf = trained_model['clf']\n",
    "    scaler = trained_model['scaler']\n",
    "    df_idx = df_idx.copy()\n",
    "    X = latents\n",
    "    Xs = scaler.transform(X)\n",
    "    probs = clf.predict_proba(Xs)[:,1] if hasattr(clf, 'predict_proba') else clf.decision_function(Xs)\n",
    "    df_idx['prob_fake'] = probs\n",
    "    # Need a video column: try to parse from frame_path\n",
    "    def video_from_path(p): \n",
    "        parts = Path(p).parent.name\n",
    "        return parts\n",
    "    df_idx['video'] = df_idx['frame_path'].apply(video_from_path)\n",
    "    agg_rows = []\n",
    "    for vid, g in df_idx.groupby('video'):\n",
    "        if agg == 'mean':\n",
    "            score = g['prob_fake'].mean()\n",
    "        elif agg == 'median':\n",
    "            score = g['prob_fake'].median()\n",
    "        elif agg == 'vote':\n",
    "            score = (g['prob_fake']>0.5).mean()\n",
    "        else:\n",
    "            score = g['prob_fake'].mean()\n",
    "        label = g['label'].iloc[0]\n",
    "        pred = 'fake' if score>0.5 else 'real'\n",
    "        agg_rows.append({'video': vid, 'score': score, 'label': label, 'pred': pred})\n",
    "    return pd.DataFrame(agg_rows)\n",
    "\n",
    "# Example usage:\n",
    "# outcomes = train_compare_all(latents, df_idx['label'].values)\n",
    "# df_video_preds = predict_video_from_frames(df_idx, latents, outcomes['RF'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f29390",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Notes finales et recommandations\n",
    "- **GAN inversion** : remplacez la fonction `gan_inversion_get_latent` par votre pipeline (e4e/psp/Restyle...). Les sorties doivent être vecteurs 1D comparables (même taille) pour toutes les images.\n",
    "- **GPU** : l'inversion GAN et l'extraction CNN utilisent le GPU si disponible. Pour de gros jeux, utilisez traitement par lots et sauvegardez les latents.\n",
    "- **Contrôle qualité** : vérifiez l'uniformité des latents (normes, outliers) et inspectez des projections (UMAP/t-SNE) pour anomalités.\n",
    "- **Améliorations** : tests augmentés (flip/crop), architectures d'ensembles, time-series models (LSTM) sur séquences de latents pour capturer temporalité.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
